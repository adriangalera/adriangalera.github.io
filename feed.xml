<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
    xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:webfeeds="http://webfeeds.org/rss/1.0">
    <channel>
        <title>Adrian Galera blog</title>
        <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
        <link>http://localhost:4000/</link>
        <description>My personal blog page</description>
        <pubDate>Tue, 18 Jun 2019 23:12:41 +0200</pubDate>
        <webfeeds:icon>http://localhost:4000/assets/img/avatars/me-100x100.png</webfeeds:icon>
        
        <item>
            
            <title>Grafana, nginx reverse-proxy and Docker</title>
            <link>http://localhost:4000/grafana-nginx-reverse-proxy-docker/</link>
            <guid isPermaLink="false">/grafana-nginx-reverse-proxy-docker/</guid>
            <description>&lt;p&gt;We want to start monitoring our AWS resources using Cloudwatch. The API is awesome, however its visualisation tool sucks, that's why we choose Grafana to present the data.&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Currently, we're using ECS to deploy our services and Grafana is available as a Docker image therefore this tool is a natural fit! We checked the documentation and everything looks fine: &lt;a href=&quot;https://grafana.com/docs/installation/docker/&quot;&gt;https://grafana.com/docs/installation/docker&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The problem comes when we tried to deploy Grafana docker image using our governance tool. This tool expects a &lt;em&gt;/health-check &lt;/em&gt;endpoint to detect the status of our applications. Here we have two approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Try to modify the Grafana code to add this logic inside&lt;/li&gt;
&lt;li&gt;Add something to the Docker container to answer this endpoint.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For obvious reasons we chose the second version. Initially we were thinking on some kind of script. However we realised that the requests to Grafana need to be proxied. This is even mentioned in their documentation! &lt;a href=&quot;https://grafana.com/docs/installation/behind_proxy/&quot;&gt;https://grafana.com/docs/installation/behind_proxy/&lt;/a&gt; .&lt;/p&gt;
&lt;h5&gt;The solution&lt;/h5&gt;
&lt;p&gt;Now that we have discarded the scripting, we chose nginx to implement the reverse proxy and we delegate the health-check endpoint to a static content under webroot of nginx.&lt;/p&gt;
&lt;p&gt;Hereunder is the Dockerfile, which is self-explanatory:&lt;/p&gt;
&lt;pre&gt;FROM grafana/grafana
EXPOSE 8080 8080
COPY health-check /health-check
COPY start-nginx-grafana.sh /start-nginx-grafana.sh
USER root
RUN apt-get update &amp;amp;&amp;amp; apt-get install -y nginx
RUN chown -R grafana:grafana /etc/nginx/nginx.conf /var/log/nginx /var/lib/nginx /start-nginx-grafana.sh
RUN chmod +x /start-nginx-grafana.sh
USER grafana
RUN cp /health-check/nginx.conf /etc/nginx/nginx.conf
ENTRYPOINT [ &quot;/start-nginx-grafana.sh&quot; ]&lt;/pre&gt;
&lt;p&gt;The tricky part we found was that installing nginx required sudo permissions, however this could be easily achieved changing to the user root in the Dockerfile. Grafana service runs as grafana user, so, some permissions of files and folders of the nginx services need to be changed to the grafana user.&lt;/p&gt;
&lt;p&gt;The following snippet shows the nginx.conf file:&lt;/p&gt;
&lt;pre&gt;worker_processes  1;
pid /var/lib/nginx/nginx.pid;


events {
    worker_connections  1024;
}

http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;
    server {
        listen       8080;
        server_name  localhost;

        location /health-check {
            default_type  &quot;application/json&quot;;
            root   /health-check;
            index  health-check.json;
        }

        location / {
            proxy_pass http://localhost:3000/;
        }

        #location /api {
        #    return 403;
        #}
    }
}&lt;/pre&gt;
&lt;p&gt;This configuration enables the health-check endpoint to be compatible with our governance tool. Precisely nginx returns the file health-check.json in response to this endpoint. nginx proxies any other request to the Grafana instance running inside the container. The presence of the nginx reverse proxy enables the user to implement more features, like blocking the Grafana API, etc...&lt;/p&gt;
&lt;p&gt;You can check the code to provide Grafana,nginx and Docker here: &lt;a href=&quot;https://bitbucket.org/agaleraegea/nginx-grafana-docker/src/master/&quot;&gt;https://bitbucket.org/agaleraegea/nginx-grafana-docker/src/master/&lt;/a&gt;&lt;/p&gt;
</description>
            <pubDate>Sat, 15 Jun 2019 00:00:00 +0200</pubDate>
        </item>
        
        <item>
            
            <title>Parse huge local json file</title>
            <link>http://localhost:4000/parse-huge-local-json-file/</link>
            <guid isPermaLink="false">/parse-huge-local-json-file/</guid>
            <description>&lt;p&gt;I need to provide a UI to analyse the results of a long-running process that generates huge JSON file whose size is in GB order.&lt;/p&gt;
&lt;p&gt;In the article I will talk about the solution implemented using the HTML5 FileReader API and the mighty &lt;a href=&quot;#using-oboe-js&quot;&gt;oboe.js&lt;/a&gt; library&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;TL;DR: &lt;a href=&quot;#code&quot;&gt;The code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanks to &lt;a href=&quot;https://www.html5rocks.com/en/tutorials/file/dndfiles/&quot;&gt;HTML5 FileReader API&lt;/a&gt; , files can be read locally in the browser without any need for servers. Even better, files can be read in &lt;a href=&quot;https://gist.github.com/alediaferia/cfb3a7503039f9278381&quot;&gt;chunks&lt;/a&gt; in order to keep the memory footprint as low as desired.&lt;/p&gt;
&lt;p&gt;If you search in Google about how to parse huge JSON files, eventually the streaming techniques will appear. In the XML world there are two different techniques for parsing files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SAX: Read the XML as events, keeping a little memory footprint&lt;/li&gt;
&lt;li&gt;DOM: Read the whole XML in memory allowing easy manipulation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Working with JSON the DOM technique is the most used. For instance&lt;/p&gt;
&lt;pre class=&quot;lang:default decode:true &quot;&gt;JSON.parse&lt;/pre&gt;
&lt;p&gt;loads the whole string in memory before parsing the JSON. What will happen if the string is really big? The browser will explode.&lt;/p&gt;
&lt;p&gt;We need to apply the SAX loading technique to read the big JSON file. In order to achieve that we can use &lt;a href=&quot;http://oboejs.com/&quot;&gt;Oboejs&lt;/a&gt; library:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Oboe.js is an &lt;a href=&quot;http://oboejs.com/LICENCE&quot;&gt;open source&lt;/a&gt; Javascript library for loading JSON using streaming, combining the convenience of DOM with the speed and fluidity of SAX.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h5 id=&quot;using-oboe-js&quot;&gt;Using oboe.js&lt;/h5&gt;
&lt;p&gt;Reading the documentation it is not clear if one can use the FileReader API with oboe-js. It clearly says you can pass an URL or a NodeJs stream to its initializer method:&lt;/p&gt;
&lt;p&gt;[caption id=&quot;attachment_207&quot; align=&quot;alignnone&quot; width=&quot;1346&quot;]&lt;img class=&quot;size-full wp-image-207&quot; src=&quot;http://localhost/wp-content/uploads/2019/03/Screen-Shot-2019-03-13-at-19.07.34.png&quot; alt=&quot;OboeJS URL configuration&quot; width=&quot;1346&quot; height=&quot;466&quot; /&gt; OboeJS URL configuration[/caption]&lt;/p&gt;
&lt;p&gt;[caption id=&quot;attachment_208&quot; align=&quot;alignnone&quot; width=&quot;1374&quot;]&lt;img class=&quot;wp-image-208 size-full&quot; src=&quot;http://localhost/wp-content/uploads/2019/03/Screen-Shot-2019-03-13-at-19.07.47.png&quot; alt=&quot;Oboe-js Stream configuration&quot; width=&quot;1374&quot; height=&quot;150&quot; /&gt; Oboe-js Stream configuration[/caption]&lt;/p&gt;
&lt;p&gt;Searching over the internet I have found this Gitlab &lt;a href=&quot;https://github.com/jimhigson/oboe.js/issues/112&quot;&gt;issue&lt;/a&gt; where it's author is asking for some solution to not using an URL nor NodeJs stream.&lt;/p&gt;
&lt;p&gt;So, finally there's a way to combine the power of the FileReader API and the streaming capabilities of oboe-js&lt;/p&gt;
&lt;h5 id=&quot;code&quot;&gt;The code&lt;/h5&gt;
&lt;p&gt;Since the UI we are building is built in React, I have made this project as a plug-and-play React component:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://bitbucket.org/agaleraegea/parse-huge-json&quot;&gt;https://bitbucket.org/agaleraegea/parse-huge-json&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;P.S: The plug-and-play worked like a charm!&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
</description>
            <pubDate>Wed, 13 Mar 2019 00:00:00 +0100</pubDate>
        </item>
        
        <item>
            
            <title>Relational model in DynamoDB</title>
            <link>http://localhost:4000/relational-model-dynamodb/</link>
            <guid isPermaLink="false">/relational-model-dynamodb/</guid>
            <description>&lt;p&gt;For a new project that I am starting, I have the requirement to implement a highly relational model in AWS. The most important condition is to keep costs as low as possible, since this is a personal project and I do not want to get poor.&lt;/p&gt;
&lt;p&gt;Therefore I will persist the model in DynamoDB configured to use the minimum resources as possible.&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;The application consist on three entities: User,Map and Points.&lt;/p&gt;
&lt;p&gt;Users can create multiple maps that contain several points. The following UML schema explain the relationships:&lt;/p&gt;
&lt;p&gt;[caption id=&quot;attachment_176&quot; align=&quot;alignnone&quot; width=&quot;602&quot;]&lt;img class=&quot;size-full wp-image-176&quot; src=&quot;http://localhost/wp-content/uploads/2019/01/estuve-model.png&quot; alt=&quot;Relational model UML&quot; width=&quot;602&quot; height=&quot;114&quot; /&gt; Relational model UML[/caption]&lt;/p&gt;
&lt;p&gt;DynamoDB is a key-value store with support for range key. Thanks to that I am able to implement the following queries:&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;CRUD User,Map,Point&lt;/em&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Add a map for one user&lt;/em&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Add a point in a map&lt;/em&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Get points from a map&lt;/em&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Remove map from user&lt;/em&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Remove point from user&lt;/em&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Get maps for a user&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;The DynamoDB model&lt;/h2&gt;
&lt;h5&gt;Users table&lt;/h5&gt;
&lt;p&gt;The user table is straightforward, the only key is a unique identifier for the user.&lt;/p&gt;
&lt;pre class=&quot;&quot;&gt;{
    &quot;TableName&quot;: USER_TABLE,
    &quot;KeySchema&quot;: [
        {
            &quot;AttributeName&quot;: &quot;user_id&quot;,
            &quot;KeyType&quot;: &quot;HASH&quot;
        }
    ],
    &quot;AttributeDefinitions&quot;: [
        {
            &quot;AttributeName&quot;: &quot;user_id&quot;,
            &quot;AttributeType&quot;: &quot;S&quot;
        }
    ]
}
&lt;/pre&gt;
&lt;p&gt;There are additional attributes that keep track of the number of points and maps stored for that user:&lt;/p&gt;
&lt;pre class=&quot;&quot;&gt;record = {
    'user_id': {
        'S': str(obj.user_id)
    },
    'num_points': {
        'N': str(obj.num_points)
    },
    'num_maps': {
        'N': str(obj.num_maps)
    }
}&lt;/pre&gt;
&lt;h5&gt;Maps table&lt;/h5&gt;
&lt;p&gt;The map table is a little bit more complex, because it has to keep relations between users and maps. Therefore, I use the range key to save the unique identifier of the map:&lt;/p&gt;
&lt;pre class=&quot;&quot;&gt;{
    &quot;TableName&quot;: MAPS_TABLE,
    &quot;KeySchema&quot;: [
        {
            &quot;AttributeName&quot;: &quot;user_id&quot;,
            &quot;KeyType&quot;: &quot;HASH&quot;
        },
{
            &quot;AttributeName&quot;: &quot;map_id&quot;,
            &quot;KeyType&quot;: &quot;RANGE&quot;
        },
    ],
    &quot;AttributeDefinitions&quot;: [
        {
            &quot;AttributeName&quot;: &quot;map_id&quot;,
            &quot;AttributeType&quot;: &quot;S&quot;
        },
{
            &quot;AttributeName&quot;: &quot;user_id&quot;,
            &quot;AttributeType&quot;: &quot;S&quot;
        }
    ]
}&lt;/pre&gt;
&lt;p&gt;There are additional attributes associated to the map (self-explanatory):&lt;/p&gt;
&lt;pre class=&quot;&quot;&gt;{
    'user_id': {
        'S': str(obj.user_id)
    },
    'map_id': {
        'S': str(obj.map_id)
    },
    'name': {
        'S': str(obj.name)
    },
    'description': {
        'S': str(obj.description)
    },
    'num_points': {
        'N': str(obj.num_points)
    }
}&lt;/pre&gt;
&lt;h5&gt;Points table&lt;/h5&gt;
&lt;p&gt;This is most complex table. The keys are similar to the maps, the range key is used to store the unique identifier of the map:&lt;/p&gt;
&lt;pre class=&quot;&quot;&gt;{
    &quot;TableName&quot;: POINTS_TABLE,
    &quot;KeySchema&quot;: [
        {
            &quot;AttributeName&quot;: &quot;map_id&quot;,
            &quot;KeyType&quot;: &quot;HASH&quot;
        },
        {
            &quot;AttributeName&quot;: &quot;point_id&quot;,
            &quot;KeyType&quot;: &quot;RANGE&quot;
        },
    ],
    &quot;AttributeDefinitions&quot;: [
        {
            &quot;AttributeName&quot;: &quot;map_id&quot;,
            &quot;AttributeType&quot;: &quot;S&quot;
        },
        {
            &quot;AttributeName&quot;: &quot;point_id&quot;,
            &quot;AttributeType&quot;: &quot;S&quot;
        },
    ]
}&lt;/pre&gt;
&lt;p&gt;And the additional parameters:&lt;/p&gt;
&lt;pre class=&quot;&quot;&gt;{
    'point_id': {
        'S': obj.point_id
    },
    'map_id': {
        'S': str(obj.map_id)
    },
    'lat': {
        'S': str(obj.lat)
    },
    'lon': {
        'S': str(obj.lon)
    },
    'date': {
        'N': str(obj.epoch)
    },
    'name': {
        'S': str(obj.name)
    },
}&lt;/pre&gt;
&lt;p&gt;The challenge with this model is to be able to delete a map with a large number of points. It is counter-intuitive, because one might think that removing only the points with the primary key of the map will make the work but ...&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;THIS WILL NOT WORK!&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Due to the way DynamoDB is implemented, this is not possible (&lt;a href=&quot;https://stackoverflow.com/questions/34259358/dynamodb-delete-all-items-having-same-hash-key&quot;&gt;https://stackoverflow.com/questions/34259358/dynamodb-delete-all-items-having-same-hash-key&lt;/a&gt;). In that kind of tables, you need to provide the primary key and the range key in order to delete an item.&lt;/p&gt;
&lt;p&gt;Since the number of items can be large, it could take a lot of capacity to delete a the points. I do not want to consume that capacity, so I will let DynamoDB throttle the deletes to adapt to the capacity.&lt;/p&gt;
&lt;p&gt;The project is serverless (Lambda) based and trying to delete a large number of points will result in timeouts when DynamoDB throttle the deletes. There are two possible solutions here: increase the write capacity of the table (increase cost) or increase the Lambda timeout (increase cost).&lt;/p&gt;
&lt;p&gt;After thinking a little bit, the valid solution I choose is to launch an ECS Task with the logic to delete the large number of maps:&lt;/p&gt;
&lt;pre class=&quot;&quot;&gt;client.run_task(
    cluster=&quot;arn:aws:ecs:eu-west-1:***:cluster/remove-points-cluster&quot;,
    taskDefinition=&quot;arn:aws:ecs:eu-west-1:***:task-definition/remove-points&quot;,
    overrides={
        &quot;containerOverrides&quot;: [
            {
                &quot;name&quot;: &quot;remove-points&quot;,
                &quot;environment&quot;: [
                    {
                        &quot;name&quot;: &quot;MAP_ID&quot;,
                        &quot;value&quot;: map_id
                    }
                ]
            }
        ]
    },
    launchType=&quot;FARGATE&quot;,
    networkConfiguration={
        &quot;awsvpcConfiguration&quot;: {
            &quot;subnets&quot;: [&quot;1&quot;, &quot;2&quot;],
            &quot;assignPublicIp&quot;: &quot;ENABLED&quot;
        }
    }
)&lt;/pre&gt;
&lt;p&gt;The best part of this ECS Task is that only took 5 minutes to use the same code base and Dockerize the logic of removing the points!&lt;/p&gt;
&lt;p&gt;Now the long-running task of delete a large number of points is done in ECS, where the pricing model is pay per use. Since this is a feature that is not going to happen a lot, it's perfectly fine.&lt;/p&gt;
</description>
            <pubDate>Mon, 28 Jan 2019 00:00:00 +0100</pubDate>
        </item>
        
        <item>
            
            <title>Mocking external API with wiremock</title>
            <link>http://localhost:4000/mocking-external-apis-wiremock/</link>
            <guid isPermaLink="false">/mocking-external-apis-wiremock/</guid>
            <description>&lt;p class=&quot;p1&quot;&gt;What happens when you are developing a component that heavily rely on an external API you do not control? Or even worst, that still does not exist. How could you test your component without connecting the external dependency? When we don't have control over the API that we need to integrate, we need a tool like a &quot;mock server&quot;.&lt;/p&gt;
&lt;p class=&quot;p1&quot;&gt;This article will discover and provide a bootstrap project for wiremock. More info: &lt;a href=&quot;http://wiremock.org/&quot;&gt;wiremock.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Quoting from their website:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;WireMock is a simulator for HTTP-based APIs. Some might consider it a &lt;strong&gt;service virtualization&lt;/strong&gt; tool or a &lt;strong&gt;mock server&lt;/strong&gt;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;At its core is a Java software that receives HTTP requests with some mapped requests to responses&lt;/p&gt;
&lt;p&gt;TL;DR: &lt;a href=&quot;https://bitbucket.org/agaleraegea/docker-compose-wiremock/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://bitbucket.org/agaleraegea/docker-compose-wiremock/&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Configuring wiremock&lt;/h3&gt;
&lt;p&gt;Configuring wiremock only consists on defining the requests to be mocked and the response that should be answered on the presence of the mocked request.&lt;/p&gt;
&lt;h3&gt;Docker&lt;/h3&gt;
&lt;p&gt;One nice way of integrate wiremock with your current testing environment is using it inside docker. There's this project &lt;a href=&quot;https://github.com/rodolpheche/wiremock-docker&quot;&gt;https://github.com/rodolpheche/wiremock-docker&lt;/a&gt; that provides the wiremock service to docker.&lt;/p&gt;
&lt;p&gt;In order to configure it, you must create the following folder structure:&lt;/p&gt;
&lt;pre&gt;.
.
├── Dockerfile
└── stubs
    ├── __files
    │   └── response.json
    └── mappings
        └── request.json
&lt;/pre&gt;
&lt;p&gt;The mappings folder contains all the mocked requests definitions and __files contains the response JSON for the mocked requests as shown before.&lt;/p&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;p&gt;Let's say we have an external API developed by another team in the company under the host externalapi.com and is not yet finished. The call that our service needs to perform is externalapi.com/v1/resource/resource1 and will respond:&lt;/p&gt;
&lt;pre lang=&quot;json&quot; class=&quot;attributes&quot; title=&quot;External API response&quot;&gt;{
    &quot;hello&quot;:&quot;world&quot;
}
&lt;/pre&gt;
&lt;p&gt;Let's configure wiremock, so we can start working on our service in parallel with the other team.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Configure the request mapping
&lt;pre lang=&quot;json&quot; class=&quot;attributes&quot; title=&quot;request.json&quot;&gt;{
    &quot;request&quot;:{
        &quot;method&quot;:&quot;GET&quot;,
        &quot;urlPathPattern&quot;:&quot;/v1/resource/([a-zA-Z0-9-\\_]*)&quot;
    },
    &quot;response&quot;:{
        &quot;status&quot;:200,
        &quot;bodyFileName&quot;:&quot;response.json&quot;,
        &quot;headers&quot;:{
            &quot;Content-Type&quot;:&quot;application/json&quot;
        }
    }
}
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Configure the response
&lt;pre lang=&quot;json&quot; class=&quot;attributes&quot; title=&quot;response.json&quot;&gt;{
    &quot;hello&quot;:&quot;world&quot;
}&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Test it
&lt;pre language=&quot;bash&quot; class=&quot;attributes&quot; title=&quot;Testing wiremock&quot;&gt;docker-compose up --build -d
curl http://localhost:7070/v1/resource/resource1
{
&quot;hello&quot; : &quot;world&quot;
}
&lt;/pre&gt;
&lt;p&gt;Yay! It worked!&amp;lt;/li&amp;gt;
&amp;lt;/ol&amp;gt;
&lt;p&gt;The only missing point is configure the actual component to point to the mocked server. For example with &lt;a href=&quot;https://github.com/Netflix/ribbon&quot;&gt;ribbon&lt;/a&gt;:&lt;/p&gt;
&lt;div&gt;
&lt;div&gt;
&lt;pre&gt;externalservice.ribbon.listOfServers=http://localhost:7070&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;
</description>
            <pubDate>Tue, 27 Nov 2018 00:00:00 +0100</pubDate>
        </item>
        
        <item>
            
            <title>Raspberry Pi for QA</title>
            <link>http://localhost:4000/raspberry-pi-qa/</link>
            <guid isPermaLink="false">/raspberry-pi-qa/</guid>
            <description>&lt;p&gt;On a previous company the QA team needed to perform some scenarios that were difficult to reproduce, for instance force a buffering event. Their setup &lt;span style=&quot;font-size: 1rem;&quot;&gt;require deep technical understanding of Linux. In order to ease its world, I decided to create a neat service running on a Raspberry Pi. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;h2&gt;The problems:&lt;/h2&gt;
&lt;p&gt;Concretely, the scenarios that the QA needed to cover were:&lt;/p&gt;
&lt;h3&gt;&lt;a href=&quot;#network-performance&quot;&gt;&lt;em&gt;Restrict the network performance&lt;/em&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The test that the QA team were performing, included provoking buffering issues in media players. They found issues on how this could be achieved: in Chrome it could be achieved through Developer Tools, but what about mobile devices, or even worst: some weird embedded media players?&lt;/p&gt;
&lt;h3&gt;&lt;a href=&quot;#geoblock&quot;&gt;&lt;em&gt;Access Geo-blocked content&lt;/em&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Since the company had customers world-wide and some of them used Geo-blocked content, they need to access those contents through the use of VPNs. This required spending some time configuration the VPN clients on their side&lt;/p&gt;
&lt;h3&gt;&lt;a href=&quot;#ipblock&quot;&gt;&lt;em&gt;IP Blocking&lt;/em&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;There were some scenarios (I cannot remember right now) that required to block the connection to some IP. This is really easy to do on a UNIX machine with iptables, but good luck doing that on Windows.&lt;/p&gt;
&lt;h3&gt;&lt;a href=&quot;#dnsspoof&quot;&gt;&lt;em&gt;Modify DNS records&lt;/em&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;It were some scenario where they needed to change the DNS records. For instance: www.google.com -&amp;gt; 192.168.1.100. I don't remember the rationale to this requirement :(&lt;/p&gt;
&lt;h3&gt;&lt;a href=&quot;#httpssniffer&quot;&gt;&lt;i&gt;Analyse&lt;/i&gt;&lt;em&gt; HTTPS traffic&lt;/em&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In the majority of the environments the connections with the company server's were made with HTTPS. This caused a little bit of headache while analysing the HTTP traffic.&lt;/p&gt;
&lt;h2&gt;The solutions ...&lt;/h2&gt;
&lt;p&gt;Since most of the scenarios require some networking tweaks, the obvious decision was Linux, even better: Raspberry Pi. The project consist in a NodeJS Express application that executed scripts on the RaspberryPi.&lt;/p&gt;
&lt;p&gt;The raspberry PI have two network interfaces: the LAN and the WiFi. The configuration is setup in the interfaces file:&lt;/p&gt;
&lt;pre class=&quot;attributes&quot; title=&quot;Network interfaces&quot;&gt;
auto lo
iface lo inet loopback
auto eth0
auto wlan0
#static ethernet conf
iface eth0 inet static
address 192.168.1.100
netmask 255.255.255.0
gateway	192.168.1.1
dns-nameservers 8.8.4.4
iface wlan0 inet static
address 192.168.150.1
netmask 255.255.255.0
&lt;/pre&gt;
&lt;p&gt;In the WiFi interface hostapd service is configured in order to serve a WiFi connection. This will configure the traffic outgoing traffic from wlan0 to eth0.&lt;/p&gt;
&lt;h3&gt;Playing with tc and ifb&lt;/h3&gt;
&lt;p&gt;The network performance restrictions can be applied using the tc command and the ifb module on Linux. This module redirects the traffic from one real network interface to a virtual one. When the traffic passes through the ifb0 interface the token bucket (&lt;a href=&quot;https://en.wikipedia.org/wiki/Token_bucket#Hierarchical_token_bucket&quot;&gt;htb&lt;/a&gt;) applies the network configuration&lt;/p&gt;
&lt;pre lang=&quot;bash&quot; class=&quot;attributes&quot; title=&quot;Traffic shaping script&quot; id=&quot;network-performance&quot;&gt;
#Enable ifb module and setup the ifb0 interface
modprobe ifb numifbs=1 &amp;amp;&amp;amp; ip link set dev ifb0 up
#Create a device that redirects all the traffic
#from eth0 to ifb0
tc qdisc add dev eth0 handle ffff: ingress &amp;amp;&amp;amp; \
tc filter add dev eth0 parent ffff: protocol ip u32 \
match u32 0 0 action mirred egress redirect dev ifb0
#Modify the token bucket configuration
tc qdisc add dev ifb0 root handle 1: htb default 10 &amp;amp;&amp;amp; \
tc class add dev ifb0 parent 1: classid 1:1 htb rate 1mb &amp;amp;&amp;amp; \
tc class add dev ifb0 parent 1:1 classid 1:10 htb rate 1mb
&lt;/pre&gt;
&lt;h3&gt;OpenVPN and Iptables&lt;/h3&gt;
&lt;p&gt;In order to be able to avoid the Geo-blocking of contents, the company provide us a commercial VPN. This consisted on a series of OpenVPN configuration scripts for multiple countries. Therefore, the app only needs to call openvpn:&lt;/p&gt;
&lt;pre lang=&quot;bash&quot; class=&quot;attributes&quot; title=&quot;openvpn&quot; id=&quot;geoblock&quot;&gt;
openvpn ALBANIA-TCP.ovpn
&lt;/pre&gt;
&lt;p&gt;It's really easy to block an IP on a Linux box, the app only needs to call the iptables, for example:&lt;/p&gt;
&lt;pre lang=&quot;bash&quot; class=&quot;attributes&quot; title=&quot;iptables&quot; id=&quot;ipblock&quot;&gt;
iptables -I FORWARD -s 192.168.150.0/24 -d 8.8.8.8  -j DROP
&lt;/pre&gt;
&lt;p&gt;This snippet blocks the outgoing connections to 8.8.8.8 that goes out from the WiFi network&lt;/p&gt;
&lt;h3&gt;Networking stuff ...&lt;/h3&gt;
&lt;p id=&quot;dnsspoof&quot;&gt;Regarding DNS Spoofing, it's a little bit trickier, however the &lt;strong&gt;dnsmasq&lt;/strong&gt; service is very useful in that situation.&lt;/p&gt;
&lt;p&gt;[caption id=&quot;attachment_127&quot; align=&quot;alignnone&quot; width=&quot;578&quot;]&lt;img class=&quot;wp-image-127 size-full&quot; src=&quot;http://localhost/wp-content/uploads/2018/09/dnsmasq.png&quot; alt=&quot;DNS Masq Schema&quot; width=&quot;578&quot; height=&quot;301&quot; /&gt; DNS Masq schema[/caption]&lt;/p&gt;
&lt;p&gt;The clients connected to the WiFi will resolve the DNS queries thanks to the dnsmasq client listening to incoming connections. This service is able to perform custom DNS resolutions based on a file that works like a /etc/hosts file.&lt;/p&gt;
&lt;pre lang=&quot;bash&quot; class=&quot;attributes&quot; title=&quot;dnsmasq configuration&quot; id=&quot;dnsspoof&quot;&gt;
192.168.56.1   ubuntu.tecmint.lan
192.168.56.10  centos.tecmint.lan
&lt;/pre&gt;
&lt;p id=&quot;httpssniffer&quot;&gt;Regarding the HTTPS Sniffer, the implemented solution implies having the SSL certificates from the company installed in a reverse proxy that terminates the SSL connection and forwards the HTTP traffic to an internal server that stores the decrypted requests on a cache.&lt;/p&gt;
&lt;p&gt;[caption id=&quot;attachment_129&quot; align=&quot;alignnone&quot; width=&quot;551&quot;]&lt;img class=&quot;size-full wp-image-129&quot; src=&quot;http://localhost/wp-content/uploads/2018/09/httpsproxy.png&quot; alt=&quot;HTTPS Proxy Setup&quot; width=&quot;551&quot; height=&quot;201&quot; /&gt; HTTPS Proxy Setup[/caption]&lt;/p&gt;
&lt;p&gt;This cache can be queried from the GUI to inspect the requests. There's an additional implementation that allows to run pcap capture software directly on the network interface to inspect the packets from the UI.&lt;/p&gt;
&lt;p&gt;This environment requires a little setup, that is the configuration of the proxy on the computers of the QA team.&lt;/p&gt;
&lt;p&gt;Finally, the QA team were able to save a lot of time setting up their scenarios. With this app it's only a matter of clicking buttons instead of executing weird scripts&lt;/p&gt;
&lt;h2&gt;The code ...&lt;/h2&gt;
&lt;p&gt;Don't blame too much on the code quality: the whole project was implemented few years ago and as a side project on one or two weekends&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://bitbucket.org/agaleraegea/rpitester&quot;&gt;https://bitbucket.org/agaleraegea/rpitester&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here's a video of me presenting that project ;)&lt;/p&gt;
&lt;p&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/aUaEF87pdms?rel=0&amp;amp;showinfo=0&quot; width=&quot;560&quot; height=&quot;315&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;allowfullscreen&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;
</description>
            <pubDate>Fri, 21 Sep 2018 00:00:00 +0200</pubDate>
        </item>
        
        <item>
            
            <title>Creating a timeseries database with DynamoDB</title>
            <link>http://localhost:4000/creating-timeseries-database-dynamodb/</link>
            <guid isPermaLink="false">/creating-timeseries-database-dynamodb/</guid>
            <description>&lt;p&gt;Working with sensor data, we need to store timestamped data in a way that was easily checked and show it in graphs. This post explains the process we follow to end up implementing a timeseries database on DynamoDB.&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;h1&gt;Requirements&lt;/h1&gt;
&lt;p&gt;The problem was pretty straightforward at the beginning: store temporal data that came from multiple sensors where one sensor could have more than one metric. For instance: one battery sensor might monitor the battery level as well as the battery voltage. The sensor data data flows continuously at an undetermined interval, it can be 1 minute, 5 minutes, ... Generally speaking the data can be represented as:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;METRIC NAME - TIMESTAMP - VALUE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In order to support the defined usage scenarios we need to provide a tool able to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Perform temporal queries: give me the metric between 2017/05/21 00:00:00 and 2017/08/21 00:30&lt;/li&gt;
  &lt;li&gt;Support multiple granularity: second, minute, hour, …&lt;/li&gt;
  &lt;li&gt;Support TTL for cold data: expire cold data&lt;/li&gt;
  &lt;li&gt;Perform in a cost-efficient manner&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;Available technologies&lt;/h1&gt;
&lt;p&gt;When one thinks about database at first the relational ones appear as the first option. The problem with relational databases such as MySQL, PostgreSQL, .. is that we the database becomes very unusable when the size of the tables grows. And in this case the data  will grow a lot with the usage.&lt;/p&gt;
&lt;p&gt;Furthermore, when the data is so big the indexes start to generate headaches making the queries take a lot of time.&lt;/p&gt;
&lt;p&gt;Finding these drawbacks in the traditional relational databases, we shift towards NoSQL databases.&lt;/p&gt;
&lt;p&gt;The first one that came into our mind (because we had some previous experience with it) was whisper &lt;a href=&quot;https://github.com/graphite-project/whisper&quot;&gt;https://github.com/graphite-project/whisper&lt;/a&gt;. This database is a small component of the graphite project, basically is a wrapper to write the temporal data to a file performing multiple roll-up aggregations on the fly. This looked promising, however, when we heavy loaded it performed very poorly.&lt;/p&gt;
&lt;p&gt;Since the platform we were building it was AWS based, we decided to analyse what Amazon can provide us and finally found &lt;strong&gt;DynamoDB&lt;/strong&gt;!&lt;/p&gt;
&lt;h1&gt;DynamoDB at the rescue!&lt;/h1&gt;
&lt;p&gt;DynamoDB is a key-value database that supports the configuration of item TTL and its costs are predictable because are in function of the required capacity.&lt;/p&gt;
&lt;p&gt;One might ask: &lt;em&gt;how can I store the presented model in a key-value database?&lt;/em&gt; The magic comes with the composite key feature: &lt;a href=&quot;https://aws.amazon.com/es/blogs/database/choosing-the-right-dynamodb-partition-key/&quot;&gt;https://aws.amazon.com/es/blogs/database/choosing-the-right-dynamodb-partition-key/&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Knowing DynamoDB&lt;/h2&gt;
&lt;p&gt;Quoting AWS documentation:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;This type of key is composed of two attributes. The first attribute is the &lt;em&gt;partition key&lt;/em&gt;, and the second attribute is the&lt;em&gt; sort key&lt;/em&gt;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Therefore, we can use the metric name as the partition key and the timestamp and the sort key and the sensor value as the DynamoDB object:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/img/posts/dynamodbts/dynamo-db-key.png&quot;&gt; &lt;img src=&quot;/assets/img/posts/dynamodbts/dynamo-db-key.png&quot; alt=&quot;DynamoDB Key Schema&quot; /&gt; &lt;/a&gt;
&lt;em&gt;DynamoDB Key Schema&lt;/em&gt;&lt;/p&gt;

&lt;h2&gt;Rollup aggregations?&lt;/h2&gt;
&lt;p&gt;Now we can store the timestamped data in DynamoDB, however, what happens with the multiple granularity requirement? DynamoDB has a nice feature called DynamoDB Streams.&lt;/p&gt;
&lt;p&gt;DynamoDB Streams sends the events generated in the database (new records, edit, delete, ...) to a an AWS Lambda. Hence, we can perform the aggregations for the multiple granularities as soon as a new data value arrives. In order to perform the storage of the multiple aggregations, we can define one table for each aggregation.&lt;/p&gt;
&lt;h2&gt;The implementation: DynamoDB Timeseries Database&lt;/h2&gt;
&lt;p&gt;Finally, in order to complete the setup we have used a serverless approach in order to allocate the cost of the project to the required capacity.&lt;/p&gt;
&lt;p&gt;The final structure of the implemented solution looked like this:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/img/posts/dynamodbts/dynamo-db-database-2.png&quot;&gt;&lt;img src=&quot;/assets/img/posts/dynamodbts/dynamo-db-database-2.png&quot; alt=&quot;Components of DynamoDB TimeseriesDB&quot; /&gt;&lt;/a&gt;
&lt;em&gt;Components of DynamoDB TimeseriesDB&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;1) The service that uses the DynamoDB Timeseries database is a serverless application with an API Gateway calling an Lambda function. One of the steps of the business logic is communicate with the DynamoDB Insert Lambda.&lt;/p&gt;
&lt;p&gt;2) The insertion mechanism of the database is by invoking an AWS Lambda function. This function inserts the timestamped data into the lower granularity table.&lt;/p&gt;
&lt;p&gt;3) When the Insert function inserts the data into the lower granularity table, DynamoDB Streams invokes the AWS Lambda involved in performing the roll-up aggregations.&lt;/p&gt;
&lt;p&gt;4) The Aggregate function has the logic implemented on how to perform multiple aggregations (average, sum, count, ...).&lt;/p&gt;
&lt;p&gt;5) Each time serie can be configured to have different aggregations, TTL, timezone to perform temporal aggregation, etc... There are an additional lambda in order to configure the timeserie parameters.&lt;/p&gt;
&lt;p&gt;6) Once the aggregation is performed, the data is stored into the appropriate DynamoDB table according to the granularity: minute, hour, day, month, year, ...&lt;/p&gt;
&lt;p&gt;With this solution we achieve a solution where we can fine tune the capacity of the database.&lt;/p&gt;
&lt;h1&gt;Production time!&lt;/h1&gt;
&lt;p&gt;When we put this system into production, some issues arises with the capacity of the DynamoDB. When the incoming data flows faster than the reserved capacity in DynamoDB, the lambda functions became very slow and resulting in a high number of timeout errors.&lt;/p&gt;
&lt;p&gt;The reasons for this is that when DynamoDB is running at capacity, it throttle requests, making the client adjust its speed to the reserved capacity. This works good for a traditional client, however it breaks with the serverless approach, because the Lambda functions were taking too much time.&lt;/p&gt;
&lt;p&gt;In order to fix situation, we add another component to the system: an AWS Kinesis Stream &lt;a href=&quot;https://aws.amazon.com/kinesis&quot;&gt;https://aws.amazon.com/kinesis&lt;/a&gt;. Instead of writing directly to the DynamoDB table, the service Lambda function now writes the data into a Kinesis stream.&lt;/p&gt;
&lt;p&gt;In the other side of the stream, we place a Kinesis consumer that is able to consume items from the stream in batches of items. Additionally, we are able to control the insertion speed of items in DynamoDB by sleeping some time between batch consumption. Since this consumer needs to be 24/7, it runs on a traditional EC2 instance.&lt;/p&gt;
&lt;p&gt;Now the scheme looks like this:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/img/posts/dynamodbts/dynamo-db-kinesis-database.png&quot;&gt;&lt;img src=&quot;/assets/img/posts/dynamodbts/dynamo-db-kinesis-database.png&quot; alt=&quot;Adding Kinesis into the TimeseriesDB&quot; /&gt;&lt;/a&gt;
&lt;em&gt;Adding Kinesis into the TimeseriesDB&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The additional point (7) shows the Kinesis stream where the items are being inserted by the Service Lambda function and consumed by the DynamoDB Timeseries DB Kinesis Consumer. The configured batch size (B) and sleep time (T) allows the consumer to buffer the insertion of data up to the reserved DynamoDB capacity.&lt;/p&gt;
&lt;h1&gt;Show me the code&lt;/h1&gt;
&lt;p&gt;An open sourced version of the production code can be found here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/adriangalera/dynamodb-timeseries&quot;&gt;https://github.com/adriangalera/dynamodb-timeseries&lt;/a&gt;&lt;/p&gt;
</description>
            <pubDate>Fri, 21 Sep 2018 00:00:00 +0200</pubDate>
        </item>
        
    </channel>
</rss>