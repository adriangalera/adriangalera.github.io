[
   
      {
        "title"    : "Grafana, nginx reverse-proxy and Docker",
        "category" : "",
        "tags"     : " grafana, docker, devops, nginx",
        "url"      : "/grafana-nginx-reverse-proxy-docker/",
        "date"     : "June 15, 2019",
        "excerpt"  : "We want to start monitoring our AWS resources using Cloudwatch. The API is awesome, however its visualisation tool sucks, that&#39;s why we choose Grafana to present the data.\n\nCurrently, we&#39;re using ECS to deploy our servi...",
        "content"  : "We want to start monitoring our AWS resources using Cloudwatch. The API is awesome, however its visualisation tool sucks, that&#39;s why we choose Grafana to present the data.\n\nCurrently, we&#39;re using ECS to deploy our services and Grafana is available as a Docker image therefore this tool is a natural fit! We checked the documentation and everything looks fine: https://grafana.com/docs/installation/docker\nThe problem comes when we tried to deploy Grafana docker image using our governance tool. This tool expects a /health-check endpoint to detect the status of our applications. Here we have two approaches:\n\nTry to modify the Grafana code to add this logic inside\nAdd something to the Docker container to answer this endpoint.\n\nFor obvious reasons we chose the second version. Initially we were thinking on some kind of script. However we realised that the requests to Grafana need to be proxied. This is even mentioned in their documentation! https://grafana.com/docs/installation/behind_proxy/ .\nThe solution\nNow that we have discarded the scripting, we chose nginx to implement the reverse proxy and we delegate the health-check endpoint to a static content under webroot of nginx.\nHereunder is the Dockerfile, which is self-explanatory:\nFROM grafana/grafana\nEXPOSE 8080 8080\nCOPY health-check /health-check\nCOPY start-nginx-grafana.sh /start-nginx-grafana.sh\nUSER root\nRUN apt-get update &amp;amp;&amp;amp; apt-get install -y nginx\nRUN chown -R grafana:grafana /etc/nginx/nginx.conf /var/log/nginx /var/lib/nginx /start-nginx-grafana.sh\nRUN chmod +x /start-nginx-grafana.sh\nUSER grafana\nRUN cp /health-check/nginx.conf /etc/nginx/nginx.conf\nENTRYPOINT [ &quot;/start-nginx-grafana.sh&quot; ]\nThe tricky part we found was that installing nginx required sudo permissions, however this could be easily achieved changing to the user root in the Dockerfile. Grafana service runs as grafana user, so, some permissions of files and folders of the nginx services need to be changed to the grafana user.\nThe following snippet shows the nginx.conf file:\nworker_processes  1;\npid /var/lib/nginx/nginx.pid;\n\n\nevents {\n    worker_connections  1024;\n}\n\nhttp {\n    include       mime.types;\n    default_type  application/octet-stream;\n    sendfile        on;\n    keepalive_timeout  65;\n    server {\n        listen       8080;\n        server_name  localhost;\n\n        location /health-check {\n            default_type  &quot;application/json&quot;;\n            root   /health-check;\n            index  health-check.json;\n        }\n\n        location / {\n            proxy_pass http://localhost:3000/;\n        }\n\n        #location /api {\n        #    return 403;\n        #}\n    }\n}\nThis configuration enables the health-check endpoint to be compatible with our governance tool. Precisely nginx returns the file health-check.json in response to this endpoint. nginx proxies any other request to the Grafana instance running inside the container. The presence of the nginx reverse proxy enables the user to implement more features, like blocking the Grafana API, etc...\nYou can check the code to provide Grafana,nginx and Docker here: https://bitbucket.org/agaleraegea/nginx-grafana-docker/src/master/\n"
      } ,
   
      {
        "title"    : "Parse huge local json file",
        "category" : "",
        "tags"     : " frontend, react, json, oboejs",
        "url"      : "/parse-huge-local-json-file/",
        "date"     : "March 13, 2019",
        "excerpt"  : "I need to provide a UI to analyse the results of a long-running process that generates huge JSON file whose size is in GB order.\nIn the article I will talk about the solution implemented using the HTML5 FileReader API and the mighty &lt;a h...",
        "content"  : "I need to provide a UI to analyse the results of a long-running process that generates huge JSON file whose size is in GB order.\nIn the article I will talk about the solution implemented using the HTML5 FileReader API and the mighty oboe.js library\n\nTL;DR: The code\nThanks to HTML5 FileReader API , files can be read locally in the browser without any need for servers. Even better, files can be read in chunks in order to keep the memory footprint as low as desired.\nIf you search in Google about how to parse huge JSON files, eventually the streaming techniques will appear. In the XML world there are two different techniques for parsing files:\n\nSAX: Read the XML as events, keeping a little memory footprint\nDOM: Read the whole XML in memory allowing easy manipulation\n\nWorking with JSON the DOM technique is the most used. For instance\nJSON.parse\nloads the whole string in memory before parsing the JSON. What will happen if the string is really big? The browser will explode.\nWe need to apply the SAX loading technique to read the big JSON file. In order to achieve that we can use Oboejs library:\nOboe.js is an open source Javascript library for loading JSON using streaming, combining the convenience of DOM with the speed and fluidity of SAX.\nUsing oboe.js\nReading the documentation it is not clear if one can use the FileReader API with oboe-js. It clearly says you can pass an URL or a NodeJs stream to its initializer method:\n[caption id=&quot;attachment_207&quot; align=&quot;alignnone&quot; width=&quot;1346&quot;] OboeJS URL configuration[/caption]\n[caption id=&quot;attachment_208&quot; align=&quot;alignnone&quot; width=&quot;1374&quot;] Oboe-js Stream configuration[/caption]\nSearching over the internet I have found this Gitlab issue where it&#39;s author is asking for some solution to not using an URL nor NodeJs stream.\nSo, finally there&#39;s a way to combine the power of the FileReader API and the streaming capabilities of oboe-js\nThe code\nSince the UI we are building is built in React, I have made this project as a plug-and-play React component:\nhttps://bitbucket.org/agaleraegea/parse-huge-json\nP.S: The plug-and-play worked like a charm!\n&amp;nbsp;\n"
      } ,
   
      {
        "title"    : "Relational model in DynamoDB",
        "category" : "",
        "tags"     : " aws, cloud, dynamodb, relational, persistence, ecs",
        "url"      : "/relational-model-dynamodb/",
        "date"     : "January 28, 2019",
        "excerpt"  : "For a new project that I am starting, I have the requirement to implement a highly relational model in AWS. The most important condition is to keep costs as low as possible, since this is a personal project and I do not want to get poor.\n&lt;p...",
        "content"  : "For a new project that I am starting, I have the requirement to implement a highly relational model in AWS. The most important condition is to keep costs as low as possible, since this is a personal project and I do not want to get poor.\nTherefore I will persist the model in DynamoDB configured to use the minimum resources as possible.\n\nThe application consist on three entities: User,Map and Points.\nUsers can create multiple maps that contain several points. The following UML schema explain the relationships:\n[caption id=&quot;attachment_176&quot; align=&quot;alignnone&quot; width=&quot;602&quot;] Relational model UML[/caption]\nDynamoDB is a key-value store with support for range key. Thanks to that I am able to implement the following queries:\nCRUD User,Map,Point\nAdd a map for one user\nAdd a point in a map\nGet points from a map\nRemove map from user\nRemove point from user\nGet maps for a user\nThe DynamoDB model\nUsers table\nThe user table is straightforward, the only key is a unique identifier for the user.\n{\n    &quot;TableName&quot;: USER_TABLE,\n    &quot;KeySchema&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;user_id&quot;,\n            &quot;KeyType&quot;: &quot;HASH&quot;\n        }\n    ],\n    &quot;AttributeDefinitions&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;user_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        }\n    ]\n}\n\nThere are additional attributes that keep track of the number of points and maps stored for that user:\nrecord = {\n    &#39;user_id&#39;: {\n        &#39;S&#39;: str(obj.user_id)\n    },\n    &#39;num_points&#39;: {\n        &#39;N&#39;: str(obj.num_points)\n    },\n    &#39;num_maps&#39;: {\n        &#39;N&#39;: str(obj.num_maps)\n    }\n}\nMaps table\nThe map table is a little bit more complex, because it has to keep relations between users and maps. Therefore, I use the range key to save the unique identifier of the map:\n{\n    &quot;TableName&quot;: MAPS_TABLE,\n    &quot;KeySchema&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;user_id&quot;,\n            &quot;KeyType&quot;: &quot;HASH&quot;\n        },\n{\n            &quot;AttributeName&quot;: &quot;map_id&quot;,\n            &quot;KeyType&quot;: &quot;RANGE&quot;\n        },\n    ],\n    &quot;AttributeDefinitions&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;map_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        },\n{\n            &quot;AttributeName&quot;: &quot;user_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        }\n    ]\n}\nThere are additional attributes associated to the map (self-explanatory):\n{\n    &#39;user_id&#39;: {\n        &#39;S&#39;: str(obj.user_id)\n    },\n    &#39;map_id&#39;: {\n        &#39;S&#39;: str(obj.map_id)\n    },\n    &#39;name&#39;: {\n        &#39;S&#39;: str(obj.name)\n    },\n    &#39;description&#39;: {\n        &#39;S&#39;: str(obj.description)\n    },\n    &#39;num_points&#39;: {\n        &#39;N&#39;: str(obj.num_points)\n    }\n}\nPoints table\nThis is most complex table. The keys are similar to the maps, the range key is used to store the unique identifier of the map:\n{\n    &quot;TableName&quot;: POINTS_TABLE,\n    &quot;KeySchema&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;map_id&quot;,\n            &quot;KeyType&quot;: &quot;HASH&quot;\n        },\n        {\n            &quot;AttributeName&quot;: &quot;point_id&quot;,\n            &quot;KeyType&quot;: &quot;RANGE&quot;\n        },\n    ],\n    &quot;AttributeDefinitions&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;map_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        },\n        {\n            &quot;AttributeName&quot;: &quot;point_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        },\n    ]\n}\nAnd the additional parameters:\n{\n    &#39;point_id&#39;: {\n        &#39;S&#39;: obj.point_id\n    },\n    &#39;map_id&#39;: {\n        &#39;S&#39;: str(obj.map_id)\n    },\n    &#39;lat&#39;: {\n        &#39;S&#39;: str(obj.lat)\n    },\n    &#39;lon&#39;: {\n        &#39;S&#39;: str(obj.lon)\n    },\n    &#39;date&#39;: {\n        &#39;N&#39;: str(obj.epoch)\n    },\n    &#39;name&#39;: {\n        &#39;S&#39;: str(obj.name)\n    },\n}\nThe challenge with this model is to be able to delete a map with a large number of points. It is counter-intuitive, because one might think that removing only the points with the primary key of the map will make the work but ...\nTHIS WILL NOT WORK!\nDue to the way DynamoDB is implemented, this is not possible (https://stackoverflow.com/questions/34259358/dynamodb-delete-all-items-having-same-hash-key). In that kind of tables, you need to provide the primary key and the range key in order to delete an item.\nSince the number of items can be large, it could take a lot of capacity to delete a the points. I do not want to consume that capacity, so I will let DynamoDB throttle the deletes to adapt to the capacity.\nThe project is serverless (Lambda) based and trying to delete a large number of points will result in timeouts when DynamoDB throttle the deletes. There are two possible solutions here: increase the write capacity of the table (increase cost) or increase the Lambda timeout (increase cost).\nAfter thinking a little bit, the valid solution I choose is to launch an ECS Task with the logic to delete the large number of maps:\nclient.run_task(\n    cluster=&quot;arn:aws:ecs:eu-west-1:***:cluster/remove-points-cluster&quot;,\n    taskDefinition=&quot;arn:aws:ecs:eu-west-1:***:task-definition/remove-points&quot;,\n    overrides={\n        &quot;containerOverrides&quot;: [\n            {\n                &quot;name&quot;: &quot;remove-points&quot;,\n                &quot;environment&quot;: [\n                    {\n                        &quot;name&quot;: &quot;MAP_ID&quot;,\n                        &quot;value&quot;: map_id\n                    }\n                ]\n            }\n        ]\n    },\n    launchType=&quot;FARGATE&quot;,\n    networkConfiguration={\n        &quot;awsvpcConfiguration&quot;: {\n            &quot;subnets&quot;: [&quot;1&quot;, &quot;2&quot;],\n            &quot;assignPublicIp&quot;: &quot;ENABLED&quot;\n        }\n    }\n)\nThe best part of this ECS Task is that only took 5 minutes to use the same code base and Dockerize the logic of removing the points!\nNow the long-running task of delete a large number of points is done in ECS, where the pricing model is pay per use. Since this is a feature that is not going to happen a lot, it&#39;s perfectly fine.\n"
      } ,
   
      {
        "title"    : "Mocking external API with wiremock",
        "category" : "",
        "tags"     : " java, testing, e2e",
        "url"      : "/mocking-external-apis-wiremock/",
        "date"     : "November 27, 2018",
        "excerpt"  : "What happens when you are developing a component that heavily rely on an external API you do not control? Or even worst, that still does not exist. How could you test your component without connecting the external dependency? When we...",
        "content"  : "What happens when you are developing a component that heavily rely on an external API you do not control? Or even worst, that still does not exist. How could you test your component without connecting the external dependency? When we don&#39;t have control over the API that we need to integrate, we need a tool like a &quot;mock server&quot;.\nThis article will discover and provide a bootstrap project for wiremock. More info: wiremock.org\n\nQuoting from their website:\nWireMock is a simulator for HTTP-based APIs. Some might consider it a service virtualization tool or a mock server.\nAt its core is a Java software that receives HTTP requests with some mapped requests to responses\nTL;DR: https://bitbucket.org/agaleraegea/docker-compose-wiremock/\nConfiguring wiremock\nConfiguring wiremock only consists on defining the requests to be mocked and the response that should be answered on the presence of the mocked request.\nDocker\nOne nice way of integrate wiremock with your current testing environment is using it inside docker. There&#39;s this project https://github.com/rodolpheche/wiremock-docker that provides the wiremock service to docker.\nIn order to configure it, you must create the following folder structure:\n.\n.\n├── Dockerfile\n└── stubs\n    ├── __files\n    │   └── response.json\n    └── mappings\n        └── request.json\n\nThe mappings folder contains all the mocked requests definitions and __files contains the response JSON for the mocked requests as shown before.\nExample\nLet&#39;s say we have an external API developed by another team in the company under the host externalapi.com and is not yet finished. The call that our service needs to perform is externalapi.com/v1/resource/resource1 and will respond:\n{\n    &quot;hello&quot;:&quot;world&quot;\n}\n\nLet&#39;s configure wiremock, so we can start working on our service in parallel with the other team.\n\nConfigure the request mapping\n{\n    &quot;request&quot;:{\n        &quot;method&quot;:&quot;GET&quot;,\n        &quot;urlPathPattern&quot;:&quot;/v1/resource/([a-zA-Z0-9-\\\\_]*)&quot;\n    },\n    &quot;response&quot;:{\n        &quot;status&quot;:200,\n        &quot;bodyFileName&quot;:&quot;response.json&quot;,\n        &quot;headers&quot;:{\n            &quot;Content-Type&quot;:&quot;application/json&quot;\n        }\n    }\n}\n\n\nConfigure the response\n{\n    &quot;hello&quot;:&quot;world&quot;\n}\n\nTest it\ndocker-compose up --build -d\ncurl http://localhost:7070/v1/resource/resource1\n{\n&quot;hello&quot; : &quot;world&quot;\n}\n\nYay! It worked!&amp;lt;/li&amp;gt;\n&amp;lt;/ol&amp;gt;\nThe only missing point is configure the actual component to point to the mocked server. For example with ribbon:\n\n\nexternalservice.ribbon.listOfServers=http://localhost:7070\n\n\n\n"
      } ,
   
      {
        "title"    : "Raspberry Pi for QA",
        "category" : "",
        "tags"     : " linux, nodejs, raspberry-pi",
        "url"      : "/raspberry-pi-qa/",
        "date"     : "September 21, 2018",
        "excerpt"  : "On a previous company the QA team needed to perform some scenarios that were difficult to reproduce, for instance force a buffering event. Their setup require deep technical understanding of Linux. In order to eas...",
        "content"  : "On a previous company the QA team needed to perform some scenarios that were difficult to reproduce, for instance force a buffering event. Their setup require deep technical understanding of Linux. In order to ease its world, I decided to create a neat service running on a Raspberry Pi. \n\nThe problems:\nConcretely, the scenarios that the QA needed to cover were:\nRestrict the network performance\nThe test that the QA team were performing, included provoking buffering issues in media players. They found issues on how this could be achieved: in Chrome it could be achieved through Developer Tools, but what about mobile devices, or even worst: some weird embedded media players?\nAccess Geo-blocked content\nSince the company had customers world-wide and some of them used Geo-blocked content, they need to access those contents through the use of VPNs. This required spending some time configuration the VPN clients on their side\nIP Blocking\nThere were some scenarios (I cannot remember right now) that required to block the connection to some IP. This is really easy to do on a UNIX machine with iptables, but good luck doing that on Windows.\nModify DNS records\nIt were some scenario where they needed to change the DNS records. For instance: www.google.com -&amp;gt; 192.168.1.100. I don&#39;t remember the rationale to this requirement :(\nAnalyse HTTPS traffic\nIn the majority of the environments the connections with the company server&#39;s were made with HTTPS. This caused a little bit of headache while analysing the HTTP traffic.\nThe solutions ...\nSince most of the scenarios require some networking tweaks, the obvious decision was Linux, even better: Raspberry Pi. The project consist in a NodeJS Express application that executed scripts on the RaspberryPi.\nThe raspberry PI have two network interfaces: the LAN and the WiFi. The configuration is setup in the interfaces file:\n\nauto lo\niface lo inet loopback\nauto eth0\nauto wlan0\n#static ethernet conf\niface eth0 inet static\naddress 192.168.1.100\nnetmask 255.255.255.0\ngateway\t192.168.1.1\ndns-nameservers 8.8.4.4\niface wlan0 inet static\naddress 192.168.150.1\nnetmask 255.255.255.0\n\nIn the WiFi interface hostapd service is configured in order to serve a WiFi connection. This will configure the traffic outgoing traffic from wlan0 to eth0.\nPlaying with tc and ifb\nThe network performance restrictions can be applied using the tc command and the ifb module on Linux. This module redirects the traffic from one real network interface to a virtual one. When the traffic passes through the ifb0 interface the token bucket (htb) applies the network configuration\n\n#Enable ifb module and setup the ifb0 interface\nmodprobe ifb numifbs=1 &amp;amp;&amp;amp; ip link set dev ifb0 up\n#Create a device that redirects all the traffic\n#from eth0 to ifb0\ntc qdisc add dev eth0 handle ffff: ingress &amp;amp;&amp;amp; \\\ntc filter add dev eth0 parent ffff: protocol ip u32 \\\nmatch u32 0 0 action mirred egress redirect dev ifb0\n#Modify the token bucket configuration\ntc qdisc add dev ifb0 root handle 1: htb default 10 &amp;amp;&amp;amp; \\\ntc class add dev ifb0 parent 1: classid 1:1 htb rate 1mb &amp;amp;&amp;amp; \\\ntc class add dev ifb0 parent 1:1 classid 1:10 htb rate 1mb\n\nOpenVPN and Iptables\nIn order to be able to avoid the Geo-blocking of contents, the company provide us a commercial VPN. This consisted on a series of OpenVPN configuration scripts for multiple countries. Therefore, the app only needs to call openvpn:\n\nopenvpn ALBANIA-TCP.ovpn\n\nIt&#39;s really easy to block an IP on a Linux box, the app only needs to call the iptables, for example:\n\niptables -I FORWARD -s 192.168.150.0/24 -d 8.8.8.8  -j DROP\n\nThis snippet blocks the outgoing connections to 8.8.8.8 that goes out from the WiFi network\nNetworking stuff ...\nRegarding DNS Spoofing, it&#39;s a little bit trickier, however the dnsmasq service is very useful in that situation.\n[caption id=&quot;attachment_127&quot; align=&quot;alignnone&quot; width=&quot;578&quot;] DNS Masq schema[/caption]\nThe clients connected to the WiFi will resolve the DNS queries thanks to the dnsmasq client listening to incoming connections. This service is able to perform custom DNS resolutions based on a file that works like a /etc/hosts file.\n\n192.168.56.1   ubuntu.tecmint.lan\n192.168.56.10  centos.tecmint.lan\n\nRegarding the HTTPS Sniffer, the implemented solution implies having the SSL certificates from the company installed in a reverse proxy that terminates the SSL connection and forwards the HTTP traffic to an internal server that stores the decrypted requests on a cache.\n[caption id=&quot;attachment_129&quot; align=&quot;alignnone&quot; width=&quot;551&quot;] HTTPS Proxy Setup[/caption]\nThis cache can be queried from the GUI to inspect the requests. There&#39;s an additional implementation that allows to run pcap capture software directly on the network interface to inspect the packets from the UI.\nThis environment requires a little setup, that is the configuration of the proxy on the computers of the QA team.\nFinally, the QA team were able to save a lot of time setting up their scenarios. With this app it&#39;s only a matter of clicking buttons instead of executing weird scripts\nThe code ...\nDon&#39;t blame too much on the code quality: the whole project was implemented few years ago and as a side project on one or two weekends\nhttps://bitbucket.org/agaleraegea/rpitester\nHere&#39;s a video of me presenting that project ;)\n\n"
      } ,
   
      {
        "title"    : "Creating a timeseries database with DynamoDB",
        "category" : "",
        "tags"     : " aws, cloud, dynamodb, kinesis, python",
        "url"      : "/creating-timeseries-database-dynamodb/",
        "date"     : "September 21, 2018",
        "excerpt"  : "Working with sensor data, we need to store timestamped data in a way that was easily checked and show it in graphs. This post explains the process we follow to end up implementing a timeseries database on DynamoDB.\n\nRe...",
        "content"  : "Working with sensor data, we need to store timestamped data in a way that was easily checked and show it in graphs. This post explains the process we follow to end up implementing a timeseries database on DynamoDB.\n\nRequirements\nThe problem was pretty straightforward at the beginning: store temporal data that came from multiple sensors where one sensor could have more than one metric. For instance: one battery sensor might monitor the battery level as well as the battery voltage. The sensor data data flows continuously at an undetermined interval, it can be 1 minute, 5 minutes, ... Generally speaking the data can be represented as:\nMETRIC NAME - TIMESTAMP - VALUE\nIn order to support the defined usage scenarios we need to provide a tool able to:\n\n\n  Perform temporal queries: give me the metric between 2017/05/21 00:00:00 and 2017/08/21 00:30\n  Support multiple granularity: second, minute, hour, …\n  Support TTL for cold data: expire cold data\n  Perform in a cost-efficient manner\n\n\nAvailable technologies\nWhen one thinks about database at first the relational ones appear as the first option. The problem with relational databases such as MySQL, PostgreSQL, .. is that we the database becomes very unusable when the size of the tables grows. And in this case the data  will grow a lot with the usage.\nFurthermore, when the data is so big the indexes start to generate headaches making the queries take a lot of time.\nFinding these drawbacks in the traditional relational databases, we shift towards NoSQL databases.\nThe first one that came into our mind (because we had some previous experience with it) was whisper https://github.com/graphite-project/whisper. This database is a small component of the graphite project, basically is a wrapper to write the temporal data to a file performing multiple roll-up aggregations on the fly. This looked promising, however, when we heavy loaded it performed very poorly.\nSince the platform we were building it was AWS based, we decided to analyse what Amazon can provide us and finally found DynamoDB!\nDynamoDB at the rescue!\nDynamoDB is a key-value database that supports the configuration of item TTL and its costs are predictable because are in function of the required capacity.\nOne might ask: how can I store the presented model in a key-value database? The magic comes with the composite key feature: https://aws.amazon.com/es/blogs/database/choosing-the-right-dynamodb-partition-key/\n\nKnowing DynamoDB\nQuoting AWS documentation:\nThis type of key is composed of two attributes. The first attribute is the partition key, and the second attribute is the sort key.\nTherefore, we can use the metric name as the partition key and the timestamp and the sort key and the sensor value as the DynamoDB object:\n\n  \nDynamoDB Key Schema\n\nRollup aggregations?\nNow we can store the timestamped data in DynamoDB, however, what happens with the multiple granularity requirement? DynamoDB has a nice feature called DynamoDB Streams.\nDynamoDB Streams sends the events generated in the database (new records, edit, delete, ...) to a an AWS Lambda. Hence, we can perform the aggregations for the multiple granularities as soon as a new data value arrives. In order to perform the storage of the multiple aggregations, we can define one table for each aggregation.\nThe implementation: DynamoDB Timeseries Database\nFinally, in order to complete the setup we have used a serverless approach in order to allocate the cost of the project to the required capacity.\nThe final structure of the implemented solution looked like this:\n\n\nComponents of DynamoDB TimeseriesDB\n\n1) The service that uses the DynamoDB Timeseries database is a serverless application with an API Gateway calling an Lambda function. One of the steps of the business logic is communicate with the DynamoDB Insert Lambda.\n2) The insertion mechanism of the database is by invoking an AWS Lambda function. This function inserts the timestamped data into the lower granularity table.\n3) When the Insert function inserts the data into the lower granularity table, DynamoDB Streams invokes the AWS Lambda involved in performing the roll-up aggregations.\n4) The Aggregate function has the logic implemented on how to perform multiple aggregations (average, sum, count, ...).\n5) Each time serie can be configured to have different aggregations, TTL, timezone to perform temporal aggregation, etc... There are an additional lambda in order to configure the timeserie parameters.\n6) Once the aggregation is performed, the data is stored into the appropriate DynamoDB table according to the granularity: minute, hour, day, month, year, ...\nWith this solution we achieve a solution where we can fine tune the capacity of the database.\nProduction time!\nWhen we put this system into production, some issues arises with the capacity of the DynamoDB. When the incoming data flows faster than the reserved capacity in DynamoDB, the lambda functions became very slow and resulting in a high number of timeout errors.\nThe reasons for this is that when DynamoDB is running at capacity, it throttle requests, making the client adjust its speed to the reserved capacity. This works good for a traditional client, however it breaks with the serverless approach, because the Lambda functions were taking too much time.\nIn order to fix situation, we add another component to the system: an AWS Kinesis Stream https://aws.amazon.com/kinesis. Instead of writing directly to the DynamoDB table, the service Lambda function now writes the data into a Kinesis stream.\nIn the other side of the stream, we place a Kinesis consumer that is able to consume items from the stream in batches of items. Additionally, we are able to control the insertion speed of items in DynamoDB by sleeping some time between batch consumption. Since this consumer needs to be 24/7, it runs on a traditional EC2 instance.\nNow the scheme looks like this:\n\n\nAdding Kinesis into the TimeseriesDB\n\nThe additional point (7) shows the Kinesis stream where the items are being inserted by the Service Lambda function and consumed by the DynamoDB Timeseries DB Kinesis Consumer. The configured batch size (B) and sleep time (T) allows the consumer to buffer the insertion of data up to the reserved DynamoDB capacity.\nShow me the code\nAn open sourced version of the production code can be found here:\nhttps://github.com/adriangalera/dynamodb-timeseries\n"
      } 
   
   
   
]