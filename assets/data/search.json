[
   
      {
        "title"    : "Testing python BaseHttpServer",
        "category" : "",
        "tags"     : " python, testing, mocking",
        "url"      : "/testing-python-base-http-server/",
        "date"     : "February 18, 2021",
        "excerpt"  : "While the development of https://www.agalera.eu/standalone-app-raspberry-pi/ I needed to use python&#39;s BaseHttpServer and inject some dependencies into it.\n\nIt turns out, th...",
        "content"  : "While the development of https://www.agalera.eu/standalone-app-raspberry-pi/ I needed to use python&#39;s BaseHttpServer and inject some dependencies into it.\n\nIt turns out, there&#39;s no easy way of doing that. Moreover, I wanted to achieve 100% code coverage testing, so I should found a way of testing that code.\n\n\n\nHere’s the code I need to test:\n\nimport socketserver\nfrom http import server\n\n\nclass DogFeederServer(server.BaseHTTPRequestHandler):\n    def __init__(self, camera_output, call_dog, servo, *args, **kwargs):\n        self.camera_output = camera_output\n        self.call_dog = call_dog\n        self.servo = servo\n        # BaseHTTPRequestHandler calls do_GET **inside** __init__ !!!\n        # So we have to call super().__init__ after setting attributes.\n        super().__init__(*args, **kwargs)\n\n    def do_GET(self):\n        if self.path == &quot;/stream.mjpg&quot;:\n            self.send_response(200)\n            # do some magic with HTTP Streaming\n        else:\n            self.send_error(404)\n        self.end_headers()\n\n    def do_POST(self):\n        if self.path == &quot;/api/call&quot;:\n            if self.call_dog():\n                self.send_response(200)\n            else:\n                self.send_response(500)\n        elif self.path == &quot;/api/treat&quot;:\n            self.servo.open_and_close()\n            self.send_response(200)\n        else:\n            self.send_error(404)\n        self.end_headers()\n\n\nclass StreamingServer(socketserver.ThreadingMixIn, server.HTTPServer):\n    allow_reuse_address = True\n    daemon_threads = True\n\n\nAs you can see, the code is really simple.\n\nThe problem comes when you realise there are no easy way of calling the constructor of the server and pass the dependencies\n\nPassing dependencies on the constructor\n\nHopefully I discovered this StackOverflow post where someone has experience the same issue: https://stackoverflow.com/questions/21631799/how-can-i-pass-parameters-to-a-requesthandler\n\nI really like the approach of the “partial” application: we pass the arguments before and once the app is created with the arguments, is passed to the server:\n\naddress = (&quot;&quot;, 8000)\nhandler = partial(\n    DogFeederServer,\n    camera_output,\n    call_dog,\n    servo,\n)\nserver = StreamingServer(address, handler)\nserver.serve_forever()\n\n\nOnce we have the “partial” approach, we could easily provide mocks for the dependencies in the tests\n\nTest the server\n\nThe only way of testing the base HTTP server I found is to create some sort of “integration testing”: provide mocks to the server but actually start the HTTP server. To test the whole logic, we could use requests library to do the HTTP calls:\n\nimport socket\nfrom functools import partial\nfrom threading import Thread\nfrom unittest import TestCase\nfrom unittest.mock import MagicMock\n\nimport requests\n\nfrom dogfeeder.server import DogFeederServer, StreamingServer\n\n\nclass ServerTest(TestCase):\n    def setUp(self):\n        super(ServerTest, self).setUp()\n        self.get_free_port()\n        self.camera_output_mock = MagicMock()\n        self.call_dog_mock = MagicMock()\n        self.servo_mock = MagicMock()\n        address = (&quot;&quot;, self.mock_server_port)\n        handler = partial(\n            DogFeederServer,\n            self.camera_output_mock,\n            self.call_dog_mock,\n            self.servo_mock,\n        )\n        self.mock_server = StreamingServer(address, handler)\n\n        # Start running mock server in a separate thread.\n        # Daemon threads automatically shut down when the main process exits.\n        self.mock_server_thread = Thread(target=self.mock_server.serve_forever)\n        self.mock_server_thread.setDaemon(True)\n        self.mock_server_thread.start()\n\n    def test_servo_open_close(self):\n        url = f&quot;http://localhost:{self.mock_server_port}/api/treat&quot;\n        response = requests.post(url)\n        self.servo_mock.open_and_close.assert_called_once()\n        assert response.status_code == 200\n\n\n    def test_invalid_path(self):\n        url = f&quot;http://localhost:{self.mock_server_port}/unknown&quot;\n        response = requests.post(url)\n        assert response.status_code == 404\n        response = requests.get(url)\n        assert response.status_code == 404\n\n    def tearDown(self):\n        super(ServerTest, self).tearDown()\n\n    def get_free_port(self):\n        s = socket.socket(socket.AF_INET, type=socket.SOCK_STREAM)\n        s.bind((&quot;localhost&quot;, 0))\n        __, port = s.getsockname()\n        s.close()\n        self.mock_server_port = port\n\n\nThe key here is to start a daemon thread (that will die when the test ends) to start the HTTP server\n"
      } ,
   
      {
        "title"    : "Standalone application for Raspberry Pi",
        "category" : "",
        "tags"     : " linux, nodejs, raspberry-pi, devops, docker",
        "url"      : "/standalone-app-raspberry-pi/",
        "date"     : "February 18, 2021",
        "excerpt"  : "I&#39;m building a small application to give treats to my dog in a remote manner. \n\nI setup a Raspberry Pi with a very basic HTTP server connected to a servo motor that will open or close the deposit where the treats are stored. \n\nIn ...",
        "content"  : "I&#39;m building a small application to give treats to my dog in a remote manner. \n\nI setup a Raspberry Pi with a very basic HTTP server connected to a servo motor that will open or close the deposit where the treats are stored. \n\nIn this article I&#39;ll explain all the challenges I found to make this application standalone.\n\n\nRequirements\n\n\n  Accessible via web\n  To have a camera, a button to give treats and a button to play a sound\n  Easily installable in a Raspberry Pi: no need to install trillions of libraries\n  Production ready: even though this is a personal project, I want the app to be 100% test covered and to have a full CI/CD cycle\n\n\nSolutions\n\nFirst of all, I did some small investigations and tackle every requirement in a separate way. This way I manage to found scripts that:\n\n\n  Create a MJPEG stream out of the Raspberry Pi camera\n  Play a sound from the disk\n  Interact with a servo motor\n\n\nBackend\n\nOnce the parts are working independently, I made a python project with a very basic HTTP server based on BaseHTTPRequestHandler that receive request to the stream, to interact with the servo and to play a sound.\n\nThe interesting thing here was to be able to develop this project without using the Raspberry Pi. This is challenging because the required libraries are hardware specific to the Raspberry. But I manage to mock the camera and the servo libraries by using unittest python package\n\nfrom unittest.mock import MagicMock, patch\n\n\ndef mock_rpi_gpio():\n    MockRPi = MagicMock()\n    modules = {\n        &quot;RPi&quot;: MockRPi,\n        &quot;RPi.GPIO&quot;: MockRPi.GPIO,\n    }\n    patcher = patch.dict(&quot;sys.modules&quot;, modules)\n    patcher.start()\n\n\ndef mock_pi_camera():\n    picamera = MagicMock()\n    modules = {&quot;picamera&quot;: picamera, &quot;picamera.PiCamera&quot;: picamera.PiCamera}\n    patcher = patch.dict(&quot;sys.modules&quot;, modules)\n    patcher.start()\n\n\nmock_rpi_gpio()\nmock_pi_camera()\n\n\nunittest module allows you to define a conftest.py file that will be executed as a configuration step for you unit tests. Having done that, we can have tests that covers all the required functionality, even without installing the required libraries:\n\nfrom unittest.mock import call\n\nimport RPi.GPIO as mockGPIO\n\nfrom dogfeeder.servo import Servo\n\n\ndef test_initialize_closed_servo():\n    Servo()\n    mockGPIO.setmode.assert_called_once_with(mockGPIO.BCM)\n    mockGPIO.setup.assert_called_once_with(Servo.SERVO_PIN, mockGPIO.OUT)\n    mockGPIO.PWM.assert_called_once_with(Servo.SERVO_PIN, 50)\n    mock_pwm = mockGPIO.PWM()\n    mock_pwm.start.assert_called_once_with(Servo.CLOSED)\n\nFrontend\n\nThe implementation of the frontend is super simple. I used React to create 3 components:\n\n\n  CallButton: the button that plays an audio file\n  DispenseTreat: the button that interacts with the servo\n  WebcamContainer: the img that prints the MJPEG stream out of the Pi Camera.\n\n\nWhen any button is pressed and API call to backend is done in the background.\n\nNothing really fancy to see here.\n\nCI/CD\n\nWhen all the logic is done and the tests are passing, I decided that I wanted to go full professional and create a CI/CD pipeline for the project. In order to do that, I used gitlab.com\n\nThis has been the most challenging piece of the project. I wanted to create a standalone application so the installation process is keep to the minimum bar. In order to do so, I created a docker image with all the required dependencies to be used by Gitlab pipeline.\n\nDocker image\n\nFROM balenalib/raspberrypi3-python:3.7-buster\nRUN apt update &amp;amp;&amp;amp; apt upgrade\nRUN apt install build-essential binutils zlib1g-dev\nRUN apt install python3-picamera python3-rpi.gpio\nRUN pip3 install pyinstaller pytest pytest-cov flake8 requests\n\n\nIt’s based on balenalib/raspberrypi3-python Docker image, that simulates even the hardware and processor architecture of the Raspberry Pi 3. The docker image also contains all the libraries required to work (picamera, gpio, …) and tools for the CI/CD (pytest, flake8).\n\npyinstaller is installed in order to generate the executable file of the backend\n\nPipeline\n\nThe pipeline contains four stages:\n\n  test: unit tests of the backend and frontend\n  release: to generate semantic versioned tags of the project\n  build: to generate the standalone executable file of the backend and the web site for the frontend. Thanks to https://threedots.tech/post/automatic-semantic-versioning-in-gitlab-ci/\n  publish: I decided to store the generated artifacts within the Gitlab Package Registry\n\n\nstages:\n  - test\n  - release\n  - build\n  - publish\n\ntest-backend:\n  image: registry.gitlab.com/adrian.galera/dogfeeder/python-ci\n  stage: test      \n  script: \n    - cd dogfeeder-backend\n    - pytest --cov --cov-fail-under=100\n  only:\n    - master\n    - branches\n\ntest-frontend:\n  image: node:12.13-alpine\n  stage: test\n  script:\n    - cd dogfeeder-web\n    - npm ci --cache .npm --prefer-offline\n    - npm test\n  cache:\n    key: &quot;node-modules&quot;\n    paths:\n      - .npm/\n  only:\n    - master\n    - branches\n\nrelease:\n  image: python:3.7-stretch\n  stage: release\n  before_script:\n    # Allow gitlab runner push code to gitlab.com\n    # see: https://threedots.tech/post/automatic-semantic-versioning-in-gitlab-ci/\n    - mkdir -p ~/.ssh &amp;amp;&amp;amp; chmod 700 ~/.ssh\n    - ssh-keyscan gitlab.com &amp;gt;&amp;gt; ~/.ssh/known_hosts &amp;amp;&amp;amp; chmod 644 ~/.ssh/known_hosts\n    - eval $(ssh-agent -s)\n    - ssh-add &amp;lt;(echo &quot;$SSH_PRIVATE_KEY&quot;)\n    - pip install semver\n  script:\n    - python3 gen-semver\n  only:\n    - master\n  when: manual\n\nbuild-backend:\n  image: registry.gitlab.com/adrian.galera/dogfeeder/python-ci\n  stage: build\n  script: \n    - cd dogfeeder-backend\n    - pyinstaller dogfeeder/main.py -F --name dogfeeder-server\n  only:\n    - tags        \n  artifacts:\n    paths:\n     - &quot;dogfeeder-backend/dist/dogfeeder-server&quot;\n\nbuild-frontend:\n  image: node:12.13-alpine\n  stage: build\n  script:\n    - cd dogfeeder-web\n    - npm ci --cache .npm --prefer-offline\n    - npm run build\n    - npm run zip\n  cache:\n    key: &quot;node-modules&quot;\n    paths:\n      - .npm/\n  artifacts:\n    paths:\n     - &quot;dogfeeder-web/dogfeeder-web_.zip&quot;\n  only:\n    - tags  \n\npublish:\n  image: curlimages/curl:latest\n  stage: publish\n  script:\n   - VERSION=${CI_COMMIT_REF_NAME}\n   - &#39;curl --header &quot;JOB-TOKEN: $CI_JOB_TOKEN&quot; --upload-file dogfeeder-backend/dist/dogfeeder-server &quot;${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/dogfeeder/${VERSION}/dogfeeder-server&quot;&#39;\n   - &#39;curl --header &quot;JOB-TOKEN: $CI_JOB_TOKEN&quot; --upload-file dogfeeder-web/dogfeeder-web_.zip &quot;${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/dogfeeder/${VERSION}/dogfeeder-web.zip&quot;&#39;  \n  only:\n    - tags\n\n\nInstallation\n\nNow that the packages are stored in Gitlab, the installation is super simple. I created a script that downloads the artifacts from Gitlab and unzip the web into a running nginx and replace the executable file that will be picked up from a supervisorctl process:\n\nVERSION=$1\nTOKEN=${GITLAB_ACCESS_TOKEN}\n\nif [ -z &quot;$1&quot; ]; then\n    echo &quot;You need to provide PACKAGE_VERSION as argument: sudo ./install-dogfeeder.sh &amp;lt;PACKAGE_VERSION&amp;gt;&quot;\n    exit 1\nfi\n\nif [ -z &quot;$TOKEN&quot; ]; then\n    echo &quot;You need to set GITLAB_ACCESS_TOKEN environment variable&quot;\n    exit 1\nfi\n\nwget --header &quot;PRIVATE-TOKEN: ${TOKEN}&quot; &quot;https://gitlab.com/api/v4/projects/24187261/packages/generic/dogfeeder/${VERSION}/dogfeeder-server&quot; -O /tmp/dogfeeder-server-${VERSION}\nwget --header &quot;PRIVATE-TOKEN: ${TOKEN}&quot; &quot;https://gitlab.com/api/v4/projects/24187261/packages/generic/dogfeeder/${VERSION}/dogfeeder-web.zip&quot; -O /tmp/dogfeeder-web-${VERSION}.zip\n\nunzip -o /tmp/dogfeeder-web-${VERSION}.zip -d /var/www/html/.\n# Kill the process and supervisorctl will start it again:\nps -eaf | grep &quot;dogfeeder-server&quot; | grep -v grep | awk &#39;{ print $2 }&#39; | xargs kill -9 &amp;amp;&amp;amp; cp /tmp/dogfeeder-server-${VERSION} /home/pi/.local/bin/dogfeeder-server\nchmod +x /home/pi/.local/bin/dogfeeder-server\n\n"
      } ,
   
      {
        "title"    : "Push notifications with SNS and Firebase",
        "category" : "",
        "tags"     : " aws, sns, mobile, java, backend, firebase",
        "url"      : "/sns-firebase-android-ios/",
        "date"     : "November 20, 2020",
        "excerpt"  : "Currently we’re investigating usage of SNS to send Push notifications. We have some friends working already with Firebase and recommend us to check it out. We discovered that SNS supports sending messages to Firebase. However, not everything is...",
        "content"  : "Currently we’re investigating usage of SNS to send Push notifications. We have some friends working already with Firebase and recommend us to check it out. We discovered that SNS supports sending messages to Firebase. However, not everything is as easy as it looks like.\n\n\n\nHow push notifications work\n\nThere are quite a lot of moving parts in the scenario:\n\n\n  AWS SNS: Messaging system from AWS\n  Firebase Cloud Messaging (FCM): Message system to connect with the devices\n  Device: device that receives push notification\n\n\nThe communication works this way:\n\n\n  The mobile device is registered within FCM.\n  FCM answers with a token that identifies the user. For the sake of testing\nthis will be manually extracted, however when doing this in PRD,\nwe need to automate this process.\n  In AWS SNS, a platform app has been created connecting AWS SNS with\nFCM with the provided API credentials (see section below)\n  WIth the token obtained in step 2). A new app endpoint is created.\nThis endpoint identifies the app that registered the token\n  When the backend wants to send the push notification, it uses the\nregistered app endpoint for that token.\n  SNS forwards the message to FCM\n  FCM sends the message to the mobile device\n\n\n\n\nTesting FCM device configuration\n\nThe preliminary test are done using an android application because I’m more used to Android development. Once FCM is setup, we could obtain the token from the device and try to send a push notification from FCM console to the device. If we do that, we receive a message with an structure similar to:\n\n{  \n    &quot;notification&quot; : {\n        &quot;title&quot;: &quot;Test notification&quot;,\n        &quot;body&quot;: &quot;Test notification body&quot;\n    },\n    &quot;data&quot;: {\n\n    }\n}\n\n\nSo far, so good. We can do the same test from FCM to IOS and we will receive the same payload and the notification will pop up.\n\nTesting SNS FCM connection\n\nAfter following this guide, now we have connected SNS with FCM.\n\nWe can try sending a message to the mobile device by specifying the app endpoint. We can access through AWS Console to SNS UI and search for app plattform and the app endpoint that belongs to our device.\n\nIf we go the default way and send a message, our Android device will receive a message similar to:\n\n{  \n    &quot;data&quot;: {\n        &quot;default&quot;: &quot;Test message&quot;\n    }\n}\n\n\nComparing the received data with the previous one, we see a fundamental difference: the notification field is empty and the message is inserted in the data field inside a default field.\n\nIf we repeat the same operation for an iOS device, we will not receive the push notification.\n\nTo send in this default way from Java, we could use the following code:\n\npublic void publishDefaultMessage(String endpoint) {\n    PublishRequest publishRequest = PublishRequest.builder()\n            .message(getTextMessage())\n            .targetArn(endpoint)\n            .build();\n\n    PublishResponse result = snsClient.publish(publishRequest);\n    System.out.println(result.messageId() + &quot; Message sent. Status was &quot; + result.sdkHttpResponse().statusCode());\n}\n\n\nSending to Android and iOS\n\nIf we want to be able to send both to Android and iOS through FCM, we need to send a custom payload to SNS. If we send with default configuration, Android can receive the notification but not iOS.\n\nTo do so, we need to provide a JSON message as the payload:\n\n{&quot;GCM&quot;:&quot;{\\&quot;notification\\&quot;:{\\&quot;title\\&quot;:\\&quot;Title\\&quot;,\\&quot;body\\&quot;:\\&quot;Notification body sent with custom payload\\&quot;},\n\\&quot;data\\&quot;:{\\&quot;orderId\\&quot;:\\&quot;1234\\&quot;,\\&quot;customerId\\&quot;:\\&quot;1234\\&quot;}}&quot;}\n\n\nWe can send that test message from AWS Console, specifying to send different payload per protocol. Or we can do it through code:\n\npublic void publishCustomMessage(String endpoint) {\n    PublishRequest publishRequest = PublishRequest.builder()\n            .message(customFirebaseMessage())\n            .targetArn(endpoint)\n            .messageStructure(&quot;json&quot;) // Send custom payload per transport type\n            .build();\n\n    PublishResponse result = snsClient.publish(publishRequest);\n    System.out.println(result.messageId() + &quot; Message sent. Status was &quot; + result.sdkHttpResponse().statusCode());\n}\nprivate String customFirebaseMessage() {\n    Map&amp;lt;String, String&amp;gt; customMessage = new HashMap&amp;lt;&amp;gt;();\n    final String FIREBASE_PROTOCOL = &quot;GCM&quot;;\n    customMessage.put(FIREBASE_PROTOCOL, getFirebaseMessage());\n    return new Gson().toJson(customMessage);\n}\nprivate String getFirebaseMessage() {\n    FirebaseMessage message = new FirebaseMessage()\n            .withTitle(&quot;Title&quot;)\n            .withBody(&quot;Notification body sent with custom payload&quot;)\n            .withDataEntry(&quot;customerId&quot;, &quot;1234&quot;)\n            .withDataEntry(&quot;orderId&quot;, &quot;1234&quot;);\n    return message.toJson();\n}\n\n\nWhere FirebaseMessage is an object we have created:\n\npublic class FirebaseMessage {\n    private final Map&amp;lt;String, Object&amp;gt; notification = new HashMap&amp;lt;&amp;gt;();\n    private final Map&amp;lt;String, Object&amp;gt; data = new HashMap&amp;lt;&amp;gt;();\n\n    public FirebaseMessage withTitle(String title) {\n        this.notification.put(&quot;title&quot;, title);\n        return this;\n    }\n\n    public FirebaseMessage withBody(String body) {\n        this.notification.put(&quot;body&quot;, body);\n        return this;\n    }\n\n    public FirebaseMessage withDataEntry(String key, String value) {\n        this.data.put(key, value);\n        return this;\n    }\n\n    public String toJson() {\n        return new Gson().toJson(this);\n    }\n}\n\n\nIf we send the messages with this format, they will be received both in Android and iOS\n"
      } ,
   
      {
        "title"    : "Gitlab improvements: caches and Docker",
        "category" : "",
        "tags"     : " devops, ci/cd, gitlab, react, docker, frontend",
        "url"      : "/gitlab-docker-image/",
        "date"     : "May 22, 2020",
        "excerpt"  : "Gitlab or any other CI/CD system works really great to have an automated build system. However, you can waste lots of time if you don’t think about carefully. Every job needs to download the build tools and dependencies, so that’s a lot of time...",
        "content"  : "Gitlab or any other CI/CD system works really great to have an automated build system. However, you can waste lots of time if you don’t think about carefully. Every job needs to download the build tools and dependencies, so that’s a lot of time that could be reduced.\n\nIn this article I describe two techniques to avoid wasting that much time. First one is using a cache for the dependencies and the second is using a pre-built Docker image with the required build tools.\n\n\n\nScenario\n\nWe will be building a simple react application to be deployed into an S3 Bucket. This simple react application contains a bunch of npm dependencies to simulate a real application. You can find the code here: https://gitlab.com/adrian.galera/gitlab-docker-react.\n\nTo simulate a real work environment, let’s define a pipeline consisting in three steps:\n\n\n  test: runs the tests defined in the project\n  build: produce the artifact to be deployed\n  deploy: deploy the artifact to an AWS S3 Bucket\n\n\nBasic pipeline\n\nIn this basic pipeline, no cache nor Docker is configured. Each job needs to download everything:\n\ntest frontend:\n  image: node:12.13-alpine\n  stage: test\n  script:\n    - npm install\n    - npm test\n\nbuild frontend:\n  image: node:12.13-alpine\n  stage: build\n  script:\n    - npm install\n    - npm build\n  only:\n    - master\n\ndeploy frontend:\n  image: node:12.13-alpine\n  stage: deploy\n  script:\n    - apk add python curl build-base zip\n    - curl &quot;https://s3.amazonaws.com/aws-cli/awscli-bundle.zip&quot; -o &quot;awscli-bundle.zip&quot;\n    - unzip -o awscli-bundle.zip\n    - ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws\n    - aws --version\n    - aws s3 sync build s3://random-bucket\n  only:\n    - master\n\n\ntest and build jobs use a nodejs Docker image and install all the dependencies and run the scripts. deploy job uses the same image and downloads and install the awsclient in order to upload the built artifact to a S3 bucket.\n\nNext step is to measure the time:\n\n\n  \n    \n      Test\n      Build\n      Deploy\n      Total\n    \n    \n      1m 24s\n      1m 26s\n      57 s\n      3m 48 s\n    \n  \n\n\nThat’s a lot of time wasted downloading dependencies or installing tools. In the next section we will reduce it by using caches\n\nCaching node_modules\n\nWe can use Gitlab cache feature to keep the contents of the node_modules folder instead of downloading them every time. It’s really simple to set it up:\n\nstages:\n  - test\n  - build\n  - deploy\n\ntest frontend:\n  image: node:12.13-alpine\n  stage: test\n  script:\n    - npm ci --cache .npm --prefer-offline\n    - npm test\n  cache:\n    key: &quot;node-modules&quot;\n    paths:\n      - .npm/\n\nbuild frontend:\n  image: node:12.13-alpine\n  stage: build\n  script:\n    - npm ci --cache .npm --prefer-offline\n    - npm build\n  only:\n    - master\n  cache:\n    key: &quot;node-modules&quot;\n    paths:\n      - .npm/\n\ndeploy frontend:\n  image: node:12.13-alpine\n  stage: deploy\n  script:\n    - apk add python curl build-base zip\n    - curl &quot;https://s3.amazonaws.com/aws-cli/awscli-bundle.zip&quot; -o &quot;awscli-bundle.zip&quot;\n    - unzip -o awscli-bundle.zip\n    - ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws\n    - aws --version\n    - aws s3 sync build s3://random-bucket\n  only:\n    - master\n  cache:\n    key: &quot;node-modules&quot;\n    paths:\n      - .npm/\n\n\nWe only need to tell npm to work with the cache with this line:\n\nnpm ci --cache .npm --prefer-offline instead of the traditional npm install\n\nand configure the cache in each job:\n\ncache:\n  key: &quot;node-modules&quot;\n  paths:\n    - .npm/\n\n\nWe are telling gitlab to store the .npm folder in the cache named node-modules\n\nThe time improvement can observed in the following table:\n\n\n  \n    \n      Test\n      Build\n      Deploy\n      Total\n    \n    \n      1m 3s\n      57s\n      56s\n      2m 54s\n    \n  \n\n\nWe can observe a huge decrease in the test and build jobs. The time spent on deploy is pretty similar to the one before.\n\nUsing Docker image\n\nThe deploy job is downloading and installing the awsclient to upload the built artifact to S3 bucket. We could improve that time by using a Docker image which already contains\nthe awslcient. In order to do so, we can build our own image and store it in Gitlab Container Registry. The Container registry can be found in the side bar: Packages &amp;amp; Resgistries -&amp;gt; Container Registry\n\nWe will build an image with nodejs 12:13 and the aws client. In order to do so, we will use the following Dockerfile:\n\nFROM node:12.13-alpine\nRUN apk add python curl build-base zip\nRUN curl &quot;https://s3.amazonaws.com/aws-cli/awscli-bundle.zip&quot; -o &quot;awscli-bundle.zip&quot;\nRUN unzip -o awscli-bundle.zip\nRUN ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws\nRUN aws --version\n\n\nBasically, the Dockerfile is running the same commands as the pipeline, but only once. Once is the image is built, we push it to the registry and we start using it. No need to install anything!\n\nTo build and push the image, you only need to run the following commands:\n\ndocker login registry.gitlab.com\ndocker build -t registry.gitlab.com/&amp;lt;user&amp;gt;/&amp;lt;project&amp;gt;/node_python_aws .\ndocker push registry.gitlab.com/&amp;lt;user&amp;gt;/&amp;lt;project&amp;gt;/node_python_aws\n\n\nOnce is pushed we can use it in the pipeline in combination with the cache:\n\nstages:\n  - test\n  - build\n  - deploy\n\ntest frontend:\n  image: registry.gitlab.com/&amp;lt;user&amp;gt;/&amp;lt;project&amp;gt;/node_python_aws\n  stage: test\n  script:\n    - npm ci --cache .npm --prefer-offline\n    - npm test\n  cache:\n    key: &quot;node-modules&quot;\n    paths:\n      - .npm/\n\nbuild frontend:\n  image: registry.gitlab.com/&amp;lt;user&amp;gt;/&amp;lt;project&amp;gt;/node_python_aws\n  stage: build\n  script:\n    - npm ci --cache .npm --prefer-offline\n    - npm build\n  only:\n    - master\n  cache:\n    key: &quot;node-modules&quot;\n    paths:\n      - .npm/\n\ndeploy frontend:\n  image: registry.gitlab.com/&amp;lt;user&amp;gt;/&amp;lt;project&amp;gt;/node_python_aws\n  stage: deploy\n  script:\n    - aws s3 sync build s3://random-bucket\n  only:\n    - master\n  cache:\n    key: &quot;node-modules&quot;\n    paths:\n      - .npm/\n\n\nNote that in the deploy job, we’re only using the aws command directly.\n\nWe can notice and decrease in time on the deploy pipeline, as we were expecting:\n\n\n  \n    \n      Test\n      Build\n      Deploy\n      Total\n    \n    \n      1m 9s\n      1m 2s\n      35s\n      2m 46s\n    \n  \n\n\nThe decrease of time, might not look like it’s very big, but that’s because that’s a very simple example.\n\nFor bigger projects, the improvements can be huge, since there are lots of dependencies and build tools that could be cached or pre-installed\n\nWrap up\n\nWe have seen different techniques to speedup Gitlab pipeline: using caches for dependencies and Docker image to pre-install build tools.\n\nYou can find the comparisson tables of the different approaches hereunder:\n\n\n  \n    \n      Improvement\n      Test\n      Build\n      Deploy\n      Total\n    \n    \n      None\n      1m 24s\n      1m 26s\n      57s\n      3m 48s\n    \n    \n      Cache\n      1m 3s\n      57s\n      56s\n      2m 54s\n    \n    \n      Cache + Docker\n      1m 9s\n      1m 2s\n      35s\n      2m 46s\n    \n  \n\n\nTotal\n\n\n\n\n\nJobs\n\n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n\nIn the charts separated by job, we can see that the time improvement for test and build comes from using caches. Regarding deploy job, the big improvement comes we have pre-installed aws client provided by the Docker image.\n\n\n\n\n\n"
      } ,
   
      {
        "title"    : "Latitude,longitude bounds",
        "category" : "",
        "tags"     : " algorithms, typescript, latlng, maps",
        "url"      : "/bounds-lat-lng-array/",
        "date"     : "April 25, 2020",
        "excerpt"  : "I’m currently developing an application based on maps. In that application I want to represent a set of markers. In order to do so, the map library I’m using it has a fitBounds method. However, you need to...",
        "content"  : "I’m currently developing an application based on maps. In that application I want to represent a set of markers. In order to do so, the map library I’m using it has a fitBounds method. However, you need to compute the bounds of the map that allow all the markers to be visible. I describe in this article the implemented algorithm.\n\n\n\nAbstraction of latitude longitude\n\nFirst of all, we need to do a nasty approximation. We can represent the earth globe, as a 2-D cartesian axis. In order to do that, we can consider latitude as the y axis and longitude as the x axis.\n\nLongitude will be in range [-180,180] and latitude in the range [-90,90]. We can define the cardinal points in the chart:\n\n\n  North -&amp;gt; Point in (0,90)\n  East -&amp;gt; Point in (180,0)\n  South -&amp;gt; Point in (0,-90)\n  West -&amp;gt; Point in (-180,0)\n\n\nWe can define the bounds of a set of points using two points only: North-East point and South\n\n\n  North-East -&amp;gt; Point in (180,90)\n  South-West -&amp;gt; Point in (-180,-90)\n\n\nWe can see a visual representation of those points in the following chart:\n\n\n\n\n\nTaking this into account we can set up some algorithm to calculate the bounds.\n\nAlgorithm to compute the bounds\n\nThe first basic algorithm is to iterate over all points and compare the longitude (x) and latitude (y) and obtain the point with higher x and y and the point with lower x and y.\n\nconst SW: LatLngTuple = [-90, -180]\nconst NE: LatLngTuple = [90, 180]\nexport const ALL_WORLD_BOUNDS: LatLngBoundsExpression = [NE, SW]\nexport const getBoundsFromPoints = (points: Point[]): LatLngBoundsExpression =&amp;gt; {\n\n    if (points.length === 0)\n        return ALL_WORLD_BOUNDS\n\n    let nex = 0, swx = 0, ney = 0, swy = 0\n    points.forEach((point) =&amp;gt; {\n        if (nex === 0 &amp;amp;&amp;amp; swx === 0 &amp;amp;&amp;amp; ney === 0 &amp;amp;&amp;amp; swy === 0) {\n            nex = swx = point.longitude\n            ney = swy = point.latitude\n        } else {\n            if (point.longitude &amp;gt; nex) nex = point.longitude;\n            if (point.longitude &amp;lt; swx) swx = point.longitude;\n            if (point.latitude &amp;gt; ney) ney = point.latitude;\n            if (point.latitude &amp;lt; swy) swy = point.latitude;\n        }\n    })\n    return [[ney, nex], [swy, swx]]\n}\n\n\nBear in mind that the map library expects and array of [lat,lng], that’s why we are switching the natural order of x,y and we’re using y,x that corresponds to [lat,lng].\n\nUnit test\n\nThese are the unit tests that check this algorithm behaves correctly:\n\nimport {ALL_WORLD_BOUNDS, getBoundsFromPoints} from &quot;./BoundCalculator&quot;;\nimport {Point} from &quot;../../types/Point&quot;;\n\ntest(&quot;should compute bounds of an empty list&quot;, () =&amp;gt; {\n    const bounds = getBoundsFromPoints([])\n    expect(bounds).toEqual(ALL_WORLD_BOUNDS)\n})\n\ntest(&quot;should compute bounds of one point&quot;, () =&amp;gt; {\n    const lat = 1, lng = 1\n    const point = new Point(lat, lng)\n    const bounds = getBoundsFromPoints([point])\n    expect(bounds).toEqual([[lat, lng], [lat, lng]])\n})\n\ntest(&quot;should compute bounds a list of points, each point per quadrant&quot;, () =&amp;gt; {\n    const lat1 = 30, lng1 = 90\n    const lat2 = 30, lng2 = -90\n    const lat3 = -30, lng3 = -90\n    const lat4 = -30, lng4 = 90\n    const point1 = new Point(lat1, lng1)\n    const point2 = new Point(lat2, lng2)\n    const point3 = new Point(lat3, lng3)\n    const point4 = new Point(lat4, lng4)\n    const bounds = getBoundsFromPoints([point1, point2, point3, point4])\n    expect(bounds).toEqual([[lat1, lng1], [lat3, lng3]])\n})\n\n\n\n\n\n\n\n\n"
      } ,
   
      {
        "title"    : "Java big memory limit",
        "category" : "",
        "tags"     : " java, jvm, jvmOptions, profiling",
        "url"      : "/java-big-memory-limit/",
        "date"     : "February 3, 2020",
        "excerpt"  : "While at work, we were fine tuning some java application, to do that, we were setting up jvm options, such as -Xmx, -Xms and so on. That lead me to asking me the foll...",
        "content"  : "While at work, we were fine tuning some java application, to do that, we were setting up jvm options, such as -Xmx, -Xms and so on. That lead me to asking me the following question:\n\n\n  What would happen if you start the jvm with the minimum memory to be higher than the computer memory?\n\n\n\n\nOur initial assumption is that -Xms will try to reserve the specified amount of memory. If that is bigger than computer memory, the JVM will stop with a OutOfMemoryError and/or the SO will start swapping and of course cause really poor performance.\n\nSetting up\n\nTo answer that question, I prepared a little experiment. A very silly gradle to project that prints something:\n\nhttps://github.com/adriangalera/jvm-big-xms\n\nBasically it runs this dummy code:\n\npublic class Boom {\n  \n    public static void main(String[] args) throws InterruptedException {\n        while (true) {\n            System.out.println(&quot;I&#39;m here&quot;);\n            Thread.sleep(1000);\n        }\n    }\n}\n\nAnd then, generate the jar file:\n./gradlew build\n\n\nFirst experiment\n\nFirst thing to do is run in a controlled environment, let’s say with 2 GB of initial memory:\n\njava -Xms2G -jar build/libs/boom-1.0-SNAPSHOT.jar\n\n\nThe program is responding, the memory profile seems fine; everything works as expected:\n\n\n\nLet’s go wild!\n\nNow, I’ll try with more memory than the available for the computer:\n\njava -Xms100G -jar build/libs/boom-1.0-SNAPSHOT.jar\n\n\nProgram is responding, and checking the memory profile I got a surprise: it is resporting 100GB!\n\n\n\n\n\nAfter some discussion with some collegues, we agreed that this did not explode at the begining because the JVM is using virtual memory. In Linux setup this allocated in 64 bit for userland processes.\n\nThe initial assumption also was wrong about swap, we check swap usage and was completely normal.\n\nTherefore, the JVM does not really reserve the initial memory as it seems initially.\n\nLet’s break it\n\nIf we want to actually pre reserve the memory, there’s a jvm option for that:\n\njava -XX:+AlwaysPreTouch -Xms100g -jar build/libs/boom-1.0-SNAPSHOT.jar \n\n\nExecuting this, the computer will become unusuable because the JVM is taking the whole memory and the SO is taking the rest of the swap disk.\n"
      } ,
   
      {
        "title"    : "Lombok tricks",
        "category" : "",
        "tags"     : " java, lombok",
        "url"      : "/lombok-tricks/",
        "date"     : "December 31, 2019",
        "excerpt"  : "\nLombok is this very beatiful tool to reduce the burden of writing Java code, but sometimes it could \nbe hard to tame. In this article I write down some issues and solutions I found while using lombok.\n\n\n\nInmutable obj...",
        "content"  : "\nLombok is this very beatiful tool to reduce the burden of writing Java code, but sometimes it could \nbe hard to tame. In this article I write down some issues and solutions I found while using lombok.\n\n\n\nInmutable objects and Jackson\n\nLet’s say we want to have an inmutable object (@Value) such as:\n\n@Value\n@Builder\npublic class Foo {\n\n    private String id;\n    private String description;\n}\n\n\nIf that’s the structure returned by some API, one could do the following to consume it:\n\nRestTemplate restTemplate = new RestTemplate();\nHttpEntity&amp;lt;String&amp;gt; entity = new HttpEntity&amp;lt;&amp;gt;();\ntry {\n    ResponseEntity&amp;lt;Foo&amp;gt; response = restTemplate.exchange(url HttpMethod.GET, entity,Foo.class);\n    return Optional.ofNullable(response.getBody());\n} catch (Exception ex) {\n    log.error(&quot;Error requesting to API: {}&quot;, ex);\n}\n\n\nHowever, in this case, some exception like the following will be thrown by Jackson JSON deserialization library:\n\nCaused by: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: \nCannot construct instance of `com.gal.Foo` (no Creators, like \ndefault construct, exist): cannot deserialize from Object value \n(no delegate- or property-based Creator)\n\n\nThe solution for this is pretty simple, we need to configure lombok with a private no-arg constructor and a constructor with all arguments:\n\n@Value\n@NoArgsConstructor(force = true, access = AccessLevel.PRIVATE)\n@AllArgsConstructor\n@Builder\npublic class Foo {\n\n    private String id;\n    private String description;\n}\n\n\nThis way, Jackson can deserialize the object with minimal lombok configuration\n"
      } ,
   
      {
        "title"    : "Unix useful commands",
        "category" : "",
        "tags"     : " unix, scripting, linux, macosx",
        "url"      : "/unix-useful-commands/",
        "date"     : "November 17, 2019",
        "excerpt"  : "This article is a compilation of Unix useful commands to solve multiple issues found in my career.\n\nEnvironment variables by variable\n\nImage we need to access the contents of an environment variable, but its na...",
        "content"  : "This article is a compilation of Unix useful commands to solve multiple issues found in my career.\n\nEnvironment variables by variable\n\nImage we need to access the contents of an environment variable, but its name is stored in another variable.\n\nFor example:\nDEV_AWS_ACCESS_KEY_ID=&quot;1234-dev&quot;\nRD_AWS_ACCESS_KEY_ID=&quot;abcd-prd&quot;\n\nThis could happen for instance while configuring multiple AWS in a CI system.\n\nLet’s continue with the example, the CI system provide a variable called “stage”, which can be dev or prd; then we want to prepend the content of this variable to get the credentials to the proper account:\n\nCUR_ENV=`echo ${stage} | tr a-z A-Z`\nENV_ACCESS_KEY=&quot;${CUR_ENV}_AWS_ACCESS_KEY_ID&quot;\n\n\nNow the magic comes, if this the shell is based in bash, we could the technique called as “variable indirection”, like this:\n\nACTUAL_KEY=echo ${!ENV_ACCESS_KEY}\n\n\nHowever, this will not work on all the shells, a more general solution could be:\n\neval ACTUAL_KEY=\\$$ENV_ACCESS_KEY\n\n\nHowever, there might be security implications by using eval\n\nCheck if there are git changes in script\n\nIn a CI pipeline, you might want to check if there are changes to create an automatic commit, etc.\n\nYou can do that by running the following snippet:\n\ngit diff-index --quiet HEAD\nANY_CHANGE=$?\n[ $ANY_CHANGE -ne 0 ] &amp;amp;&amp;amp; echo &quot;Do something with the change&quot;\n\n"
      } ,
   
      {
        "title"    : "Discovering terraform",
        "category" : "",
        "tags"     : " devops, aws, terraform, cloud",
        "url"      : "/discovering-terraform/",
        "date"     : "October 14, 2019",
        "excerpt"  : "Recently I have been playing with Terraform tool and I wrote some basic use cases for the sake of sharing the knowledge\n\n\nWhat is terraform?\n\nImagine you start a new cloud project in AWS....",
        "content"  : "Recently I have been playing with Terraform tool and I wrote some basic use cases for the sake of sharing the knowledge\n\n\nWhat is terraform?\n\nImagine you start a new cloud project in AWS. At the very begining, you will write some scripts to generate your AWS resources:\n\n\n  DynamoDB tables\n  S3 buckets\n  RDS databases\n  …\n\n\nHowever, lots of configuration are required to make it work properly, i.e. IAM roles, ARN of resources, etc… So, managing this burden manually or with some scripts rapidly could become an issue.\n\nEven worst, imagine you have resources in both Google Cloud an AWS. The amount of scripts will be doubled as well as the issues.\n\nTerraform was created to help with those issues. It is a tool written in Go language that enables managing the state of cloud infrastructure from configuration files with its own Domain Specific Language (DSL).\n\nFurthermore, it is a handy tool to define the infrastructure as code. In this approach the changes to the infrastructure can be pushed to a git repository for instance; this way the changes can be reviewed, etc.\n\nAnother fancy feature is that it keeps the state of the resources running in Cloud. This way terraform allows to make incremental changes (if possible).\n\nBasic usage\n\nThere are three main commands to use:\n\n\n  terraform plan: process the configuration templates and present in stdout the actions that will be applied\n  terraform apply: process the configuration templates and apply the required actions\n  terraform destroy: process the configuration templates and delete the resources\n\n\nScenarios\n\nIn this section, I prepared 4 scenarions to show some cool features that terraform can provide.\n\nBasic\n\nThis scenario create a DynamoDB table and an S3 bucket. There’s nothing fancy here, only basic usage of terraform:\n\nvariable &quot;app-prefix&quot; {\n  type = string\n  default = &quot;comms-ks-01&quot;\n}\nresource &quot;aws_dynamodb_table&quot; &quot;configuration&quot; {\n  name           = &quot;${var.app-prefix}_timeserie_configuration&quot;\n  billing_mode   = &quot;PAY_PER_REQUEST&quot;\n  hash_key       = &quot;timeserie1&quot;\n\n  attribute {\n    name = &quot;timeserie1&quot;\n    type = &quot;S&quot;\n  }\n\n}\nresource &quot;aws_s3_bucket&quot; &quot;s3-bucket-rnd-name&quot; {\n    bucket = &quot;${var.app-prefix}-timeserie-configuration&quot;\n} \noutput &quot;bucket-arn&quot; {\n  value = &quot;${aws_s3_bucket.s3-bucket-rnd-name.arn}&quot;\n}\n\nThe template defines a variable (“app-prefix), a DynamoDB table (“configuration”) and a S3 bucket (“s3-bucket-rnd-name”). The last line defines to output to print the ARN of the created bucket\n\nFor each\n\nIn this scenario, the requirement is to create three tables with similar names. In order to reuse the code, we can use terraform’s for-each statement:\n\nresource &quot;aws_dynamodb_table&quot; &quot;configuration&quot; {\n\n  for_each = {\n    test1: &quot;${var.app-prefix}_configuration_test_1&quot;,\n    test2: &quot;${var.app-prefix}_configuration_test_2&quot;,\n    test3: &quot;${var.app-prefix}_configuration_test_3&quot;,\n  }\n\n  name           = each.value\n  billing_mode   = &quot;PAY_PER_REQUEST&quot;\n  hash_key       = &quot;timeserie&quot;\n\n  attribute {\n    name = &quot;timeserie&quot;\n    type = &quot;S&quot;\n  }\n}\n\nThis way with only one resource definition, we can create multiple resources.\n\nLinking resources\n\nUp to here, we haven’t done anything fancy, just creating resources. However, we can create more interesting environments, such as a serverless pipeline to process a file:\n\n\n\n# Lambda definition\nresource &quot;aws_lambda_function&quot; &quot;download-s3-lambda&quot; {\n  filename      = &quot;download-s3-file-lambda.zip&quot;\n  function_name = &quot;${var.app-prefix}-download-files-lambda&quot;\n  role          = &quot;${aws_iam_role.iam_for_lambda.arn}&quot;\n  handler       = &quot;receive-file-s3.handler&quot;\n  runtime       = &quot;python3.7&quot;\n  depends_on    = [&quot;aws_iam_role_policy_attachment.lambda_logs&quot;, &quot;aws_cloudwatch_log_group.example&quot;]\n}\n\n# Notify lambda when a file is created in the S3 bucket\nresource &quot;aws_s3_bucket_notification&quot; &quot;bucket_notification&quot; {\n  bucket = &quot;${aws_s3_bucket.s3-files-bucket.id}&quot;\n\n  lambda_function {\n    lambda_function_arn = &quot;${aws_lambda_function.download-s3-lambda.arn}&quot;\n    events              = [&quot;s3:ObjectCreated:*&quot;]\n  }\n}\n# S3 bucket to place files\nresource &quot;aws_s3_bucket&quot; &quot;s3-files-bucket&quot; {\n    bucket = &quot;${var.app-prefix}-files-bucket&quot;\n    force_destroy = &quot;true&quot;\n}\n\nThe template in this environment is more complex, because there’s a lot of IAM permissions in place. For simplicity, not all the resources are included in this article. For more info: https://github.com/adriangalera/terraform-knowledge-sharing.\n\nIn the upper code snippet can be observed how the AWS lambda and the AWS S3 bucket are created. Additionally, the notification from S3 to Lambda is created. When an object is created  in the S3 bucket, the AWS lambda will be invoked.\n\nTemplates\nTerraform also supports templates to enable code reuse. In this environment, we will create two docker container to be ran on ECS. Those two docker containers will execute the same image and echo the value of an environment variable.\n\nIf we did not have templates, we would need to define the resources twice. Thanks to the templates, we don’t need to define the container definition twice, we only need to pass the variables.\n\ndata &quot;template_file&quot; &quot;container_backend&quot; {\n  template = &quot;${file(&quot;container_definition.tpl&quot;)}&quot;\n  vars = {\n    container_name = &quot;${var.app-prefix}_backend_container&quot;\n    log_group = &quot;${var.app-prefix}_backend_container&quot;\n    service_type = &quot;backend&quot;\n  }\n}\n  family = &quot;${var.app-prefix}_backend_task_definition&quot;\n  requires_compatibilities = [ &quot;FARGATE&quot; ]\n  network_mode =  &quot;awsvpc&quot;\n  execution_role_arn = &quot;${aws_iam_role.ecs_container_iam_role.arn}&quot;\n  cpu = 256\n  memory = 512\n  container_definitions = &quot;${data.template_file.container_backend.rendered}&quot;\n}\n\nIn order to get the rendered values of the template, we need to get the rendered field.\n\nIn this case, the template contains lots of configuration and IAM definitions. For more info, check the whole repository: https://github.com/adriangalera/terraform-knowledge-sharing\n"
      } ,
   
      {
        "title"    : "Mockito ArgumentCaptor with inheritance",
        "category" : "",
        "tags"     : " testing, java, mockito",
        "url"      : "/mockito-argumentcaptor-inheritance/",
        "date"     : "August 8, 2019",
        "excerpt"  : "Working with Mockito’s ArgumentCaptor I discover it has a awful issue with inheritance.\n\nLet’s suppose we have a parent class named Animal and two child classes &lt;code class=&quot;highli...",
        "content"  : "Working with Mockito’s ArgumentCaptor I discover it has a awful issue with inheritance.\n\nLet’s suppose we have a parent class named Animal and two child classes Dog and Cat:\n\npublic class Animal {\n\n    private String species;\n\n    public Animal(String species) {\n        this.species = species;\n    }\n\n    public String getSpecies() {\n        return species;\n    }\n}\npublic class Cat extends Animal {\n\n    private final String colorEyes;\n\n    public Cat(String colorEyes) {\n        super(&quot;Cat&quot;);\n        this.colorEyes = colorEyes;\n    }\n}\npublic class Dog extends Animal {\n\n    private String name;\n\n    public Dog(String name) {\n        super(&quot;Dog&quot;);\n        this.name = name;\n    }\n\n    public String getName() {\n        return name;\n    }\n}\n\n\nAnd a class that accept an argument of type Animal:\npublic class AnimalProcessor {\n\n    public void processAnimal(Animal animal) {\n        System.out.println(animal.getSpecies());\n    }\n}\n\n\nOne might think on writing the unit test of AnimalProcessor similar to this snippet:\n\npublic class ArgumentCaptorInheritanceTest {\n\n    @Mock\n    private AnimalProcessor animalProcessor;\n\n    @Before\n    public void init() {\n        MockitoAnnotations.initMocks(this);\n    }\n\n    @Test\n    public void shouldProcessDog() {\n        Dog dog = new Dog(&quot;Rex&quot;);\n        Cat cat = new Cat(&quot;blue&quot;);\n\n        ArgumentCaptor&amp;lt;Dog&amp;gt; dogArgumentCaptor = ArgumentCaptor.forClass(Dog.class);\n\n        animalProcessor.processAnimal(dog);\n        animalProcessor.processAnimal(cat);\n\n        Mockito.verify(animalProcessor).processAnimal(dogArgumentCaptor.capture());\n\n        Assert.assertEquals(&quot;Rex&quot;, dogArgumentCaptor.getValue().getName());\n    }\n\n\nWell, this fails … ArgumentCaptor does not work well in this case. It makes the verify fail because the method has been called twice.\n\nThe expected behaviour is that only verify analyses the calls when the Dog instance is passed.\n\nIn order to execute the test in this way, some ugly workaround needs to be done:\n\n    @Test\n    public void shouldProcessDog() {\n        Dog dog = new Dog(&quot;Rex&quot;);\n        Cat cat = new Cat(&quot;blue&quot;);\n\n        ArgumentCaptor&amp;lt;Animal&amp;gt; animalCaptor = ArgumentCaptor.forClass(Animal.class);\n\n        animalProcessor.processAnimal(dog);\n        animalProcessor.processAnimal(cat);\n\n        Mockito.verify(animalProcessor, Mockito.times(2)).processAnimal(animalCaptor.capture());\n\n        List&amp;lt;Animal&amp;gt; processedAnimals = animalCaptor.getAllValues();\n        Optional&amp;lt;Animal&amp;gt; dogOptional = processedAnimals.stream()\n                                        .filter(a -&amp;gt; a instanceof Dog)\n                                        .findFirst();\n        Assert.assertTrue(dogOptional.isPresent());\n        Assert.assertEquals(&quot;Rex&quot;, ((Dog) dogOptional.get()).getName());\n    }\n\n\nInstead of capturing the arguments for Dog, you can do it for Animal.\n\nThen the verify will successfully capture the two calls and then all the captured values can be analysed. You can filter the captured values for the objects that are of the interested instance.\n\nThis example comes from:\n\nhttps://github.com/adriangalera/java-sandbox/tree/master/src/test/java/mockito/argcaptor\n\nThis is a known issue (already reported as an issue in their repo):\n\nhttps://github.com/mockito/mockito/issues/565\n\nAs of the day of writing the article, the issues is still there and it has been opened from 2016 …\n"
      } ,
   
      {
        "title"    : "Publishing versions in Gradle BOM",
        "category" : "",
        "tags"     : " gradle, groovy",
        "url"      : "/gradle-bom-publish-dependency-version/",
        "date"     : "July 18, 2019",
        "excerpt"  : "We are planning to use the concept of Bill Of Materials (BOM) to define the version of our dependencies. However this externalization do not allow us to have the version variable in the projects that import the bom. Here you will find out how w...",
        "content"  : "We are planning to use the concept of Bill Of Materials (BOM) to define the version of our dependencies. However this externalization do not allow us to have the version variable in the projects that import the bom. Here you will find out how we managed to get the version in our components.\n\nBill of Materials\nEveryone knows dependencies are hell, specially dealing with versions. That’s why we want to centralize all the version definition in a Bill Of Materials file. This approach is used for instance by Spring framework.\n\nHowever in our projects we need to provide the current version of libraries for some external component configuration.\n\nIn this sense, the two requirements collide because the versions cannot be extracted from the bom file.\n\nPublishing the versions\n\nHere’s our bom gradle script:\n\nplugins {\n    id &#39;java&#39;\n    id &#39;maven-publish&#39;\n    id &#39;io.spring.dependency-management&#39; version &#39;1.0.6.RELEASE&#39;\n}\n\ngroup &#39;com.example&#39;\n\ndef versions = [\n        library1: &#39;5.14.13&#39;,\n        library2: &#39;1.0.6.RELEASE&#39;,  \n]\n\n\ndependencyManagement {\n    dependencies {\n        dependency group: &#39;com.example&#39;, name: &#39;library1&#39;, version: versions.library1\n        dependency group: &#39;com.example&#39;, name: &#39;library2&#39;, version: versions.library2    \n    }\n}\n\npublishing {\n    repositories {\n        maven {\n            url &#39;https://repo.com/maven-releases&#39;\n            credentials credentials\n        }\n    }\n\n    publications {\n        mavenBom(MavenPublication) {\n            artifacts = []\n        }\n    }\n}\n\n\nWhen we execute the publish it correctly generates the bom file. However the versions are not accesible from the projects that import the bom.\n\nIn order to extract the version, we can add it as properties to the bom file:\n\n publications {\n        mavenBom(MavenPublication) {\n            artifacts = []\n            pom.withXml {\n                def propertiesNode = new Node(null, &quot;properties&quot;)\n                versions.each { entry -&amp;gt; propertiesNode.appendNode(entry.key, entry.value) }\n                asNode().append(propertiesNode)\n            }\n        }\n    }\n\n\nIt took like 3 hours to get these 3 lines working because Groovy sucks … After that we get the versions in the pom.xml:\n\n&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;\n&amp;lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&amp;gt;\n  &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;\n  ...\n  &amp;lt;properties&amp;gt;\n    &amp;lt;library1&amp;gt;5.14.13&amp;lt;/library1&amp;gt;\n    &amp;lt;library2&amp;gt;1.0.6.RELEASE&amp;lt;/library2&amp;gt;\n  &amp;lt;/properties&amp;gt;\n&amp;lt;/project&amp;gt;\n\n\n\nFrom the project that want to use the bom we can access these versions like this:\n\ndependencyManagement.importedProperties[&#39;library1&#39;]\n\n"
      } ,
   
      {
        "title"    : "Java Unit Test to check UTF-8 chars",
        "category" : "",
        "tags"     : " java, unit-test, utf8, i18n",
        "url"      : "/java-test-utf8/",
        "date"     : "June 19, 2019",
        "excerpt"  : "During a migration to a new plattform, we have detected an issue with the character encoding. Some of the messages contained the UTF-8 replacement character (�)\n\n\n\nFortunately, we have been able to fix the configuration issue ...",
        "content"  : "During a migration to a new plattform, we have detected an issue with the character encoding. Some of the messages contained the UTF-8 replacement character (�)\n\n\n\nFortunately, we have been able to fix the configuration issue and to make sure it does not happen again we have put in place a variation of the following unit test:\n\nimport org.junit.Assert;\nimport org.junit.Test;\n\npublic class CheckUtf8ReplacementChar {\n\n\n    private boolean containsUtf8ReplacementCharacter(String target) {\n        final int REPLACEMENT_CHARACTER_VALUE = 65533;\n        for (int i = 0; i &amp;lt; target.length(); i++) {\n            if ((int) target.charAt(i) == REPLACEMENT_CHARACTER_VALUE) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n    @Test\n    public void shouldDetectUtf8ReplacementChar() {\n        final String wrongString = &quot;Wrong characters ������������������&amp;lt;br&amp;gt;&quot;;\n        final String okString = &quot;OK characters&quot;;\n        Assert.assertTrue(containsUtf8ReplacementCharacter(wrongString));\n        Assert.assertFalse(containsUtf8ReplacementCharacter(okString));\n    }\n}\n\n\nEven though this can be improved, we didn’t have much time to think about it. That’s the first way we could develop.\n"
      } ,
   
      {
        "title"    : "Grafana, nginx reverse-proxy and Docker",
        "category" : "",
        "tags"     : " grafana, docker, devops, nginx",
        "url"      : "/grafana-nginx-reverse-proxy-docker/",
        "date"     : "June 15, 2019",
        "excerpt"  : "We want to start monitoring our AWS resources using Cloudwatch. The API is awesome, however its visualisation tool sucks, that&#39;s why we choose Grafana to present the data.\n\nCurrently, we&#39;re using ECS to deploy our servi...",
        "content"  : "We want to start monitoring our AWS resources using Cloudwatch. The API is awesome, however its visualisation tool sucks, that&#39;s why we choose Grafana to present the data.\n\nCurrently, we&#39;re using ECS to deploy our services and Grafana is available as a Docker image therefore this tool is a natural fit! We checked the documentation and everything looks fine: https://grafana.com/docs/installation/docker\nThe problem comes when we tried to deploy Grafana docker image using our governance tool. This tool expects a /health-check endpoint to detect the status of our applications. Here we have two approaches:\n\nTry to modify the Grafana code to add this logic inside\nAdd something to the Docker container to answer this endpoint.\n\nFor obvious reasons we chose the second version. Initially we were thinking on some kind of script. However we realised that the requests to Grafana need to be proxied. This is even mentioned in their documentation! https://grafana.com/docs/installation/behind_proxy/ .\nThe solution\nNow that we have discarded the scripting, we chose nginx to implement the reverse proxy and we delegate the health-check endpoint to a static content under webroot of nginx.\nHereunder is the Dockerfile, which is self-explanatory:\n\nFROM grafana/grafana\nEXPOSE 8080 8080\nCOPY health-check /health-check\nCOPY start-nginx-grafana.sh /start-nginx-grafana.sh\nUSER root\nRUN apt-get update &amp;amp;amp;&amp;amp;amp; apt-get install -y nginx\nRUN chown -R grafana:grafana /etc/nginx/nginx.conf /var/log/nginx /var/lib/nginx /start-nginx-grafana.sh\nRUN chmod +x /start-nginx-grafana.sh\nUSER grafana\nRUN cp /health-check/nginx.conf /etc/nginx/nginx.conf\nENTRYPOINT [ &quot;/start-nginx-grafana.sh&quot; ]\n\n\nThe tricky part we found was that installing nginx required sudo permissions, however this could be easily achieved changing to the user root in the Dockerfile. Grafana service runs as grafana user, so, some permissions of files and folders of the nginx services need to be changed to the grafana user.\nThe following snippet shows the nginx.conf file:\n\nworker_processes  1;\npid /var/lib/nginx/nginx.pid;\n\nevents {\n    worker_connections  1024;\n}\n\nhttp {\n    include       mime.types;\n    default_type  application/octet-stream;\n    sendfile        on;\n    keepalive_timeout  65;\n    server {\n        listen       8080;\n        server_name  localhost;\n\n        location /health-check {\n            default_type  &quot;application/json&quot;;\n            root   /health-check;\n            index  health-check.json;\n        }\n\n        location / {\n            proxy_pass http://localhost:3000/;\n        }\n\n        #location /api {\n        #    return 403;\n        #}\n    }\n}\n\n\nThis configuration enables the health-check endpoint to be compatible with our governance tool. Precisely nginx returns the file health-check.json in response to this endpoint. nginx proxies any other request to the Grafana instance running inside the container. The presence of the nginx reverse proxy enables the user to implement more features, like blocking the Grafana API, etc...\nYou can check the code to provide Grafana,nginx and Docker here: https://github.com/adriangalera/nginx-grafana-docker\n"
      } ,
   
      {
        "title"    : "Parse huge local json file",
        "category" : "",
        "tags"     : " frontend, react, json, oboejs",
        "url"      : "/parse-huge-local-json-file/",
        "date"     : "March 13, 2019",
        "excerpt"  : "I need to provide a UI to analyse the results of a long-running process that generates huge JSON file whose size is in GB order.\nIn the article I will talk about the solution implemented using the HTML5 FileReader API and the mighty &lt;a h...",
        "content"  : "I need to provide a UI to analyse the results of a long-running process that generates huge JSON file whose size is in GB order.\nIn the article I will talk about the solution implemented using the HTML5 FileReader API and the mighty oboe.js library\n\nTL;DR: The code\nThanks to HTML5 FileReader API , files can be read locally in the browser without any need for servers. Even better, files can be read in chunks in order to keep the memory footprint as low as desired.\nIf you search in Google about how to parse huge JSON files, eventually the streaming techniques will appear. In the XML world there are two different techniques for parsing files:\n\nSAX: Read the XML as events, keeping a little memory footprint\nDOM: Read the whole XML in memory allowing easy manipulation\n\nWorking with JSON the DOM technique is the most used. For instance &quot;JSON.parse&quot; loads the whole string in memory before parsing the JSON. What will happen if the string is really big? The browser will explode.\nWe need to apply the SAX loading technique to read the big JSON file. In order to achieve that we can use Oboejs library:\nOboe.js is an open source Javascript library for loading JSON using streaming, combining the convenience of DOM with the speed and fluidity of SAX.\nUsing oboe.js\nReading the documentation it is not clear if one can use the FileReader API with oboe-js. It clearly says you can pass an URL or a NodeJs stream to its initializer method:\n\noboe( String url )\n\noboe({\n    url: String,\n    method: String,\n    headers: Object,\n    body: String|Object,\n    cached: Boolean,\n    withCredentials: Boolean\n})\n\noboe(stream)\n\n\nSearching over the internet I have found this Github issue where it&#39;s author is asking for some solution to not using an URL nor NodeJs stream.\nSo, finally there&#39;s a way to combine the power of the FileReader API and the streaming capabilities of oboejs\nThe code\nSince the UI we are building is built in React, I have made this project as a plug-and-play React component:\nhttps://github.com/adriangalera/parse-huge-json\nP.S: The plug-and-play worked like a charm!\n&amp;nbsp;\n"
      } ,
   
      {
        "title"    : "Relational model in DynamoDB",
        "category" : "",
        "tags"     : " aws, cloud, dynamodb, relational, persistence, ecs, python",
        "url"      : "/relational-model-dynamodb/",
        "date"     : "January 28, 2019",
        "excerpt"  : "For a new project that I am starting, I have the requirement to implement a highly relational model in AWS. The most important condition is to keep costs as low as possible, since this is a personal project and I do not want to get poor.\n&lt;p...",
        "content"  : "For a new project that I am starting, I have the requirement to implement a highly relational model in AWS. The most important condition is to keep costs as low as possible, since this is a personal project and I do not want to get poor.\nTherefore I will persist the model in DynamoDB configured to use the minimum resources as possible.\n\nThe application consist on three entities: User,Map and Points.\nUsers can create multiple maps that contain several points. The following UML schema explain the relationships:\n\n  \nRelational model UML\n\nDynamoDB is a key-value store with support for range key. Thanks to that I am able to implement the following queries:\n\n\n  CRUD User,Map,Point\n  Add a map for one user\n  Add a point in a map\n  Get points from a map\n  Remove map from user\n  Remove point from user\n  Get maps for a user\n\n\nThe DynamoDB model\nUsers table\nThe user table is straightforward, the only key is a unique identifier for the user.\n\n{\n    &quot;TableName&quot;: USER_TABLE,\n    &quot;KeySchema&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;user_id&quot;,\n            &quot;KeyType&quot;: &quot;HASH&quot;\n        }\n    ],\n    &quot;AttributeDefinitions&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;user_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        }\n    ]\n}\n\n\nThere are additional attributes that keep track of the number of points and maps stored for that user:\n\nrecord = {\n    &#39;user_id&#39;: {\n        &#39;S&#39;: str(obj.user_id)\n    },\n    &#39;num_points&#39;: {\n        &#39;N&#39;: str(obj.num_points)\n    },\n    &#39;num_maps&#39;: {\n        &#39;N&#39;: str(obj.num_maps)\n    }\n}\n\n\nMaps table\nThe map table is a little bit more complex, because it has to keep relations between users and maps. Therefore, I use the range key to save the unique identifier of the map:\n\n{\n    &quot;TableName&quot;: MAPS_TABLE,\n    &quot;KeySchema&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;user_id&quot;,\n            &quot;KeyType&quot;: &quot;HASH&quot;\n        },\n{\n            &quot;AttributeName&quot;: &quot;map_id&quot;,\n            &quot;KeyType&quot;: &quot;RANGE&quot;\n        },\n    ],\n    &quot;AttributeDefinitions&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;map_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        },\n{\n            &quot;AttributeName&quot;: &quot;user_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        }\n    ]\n}\n\n\nThere are additional attributes associated to the map (self-explanatory):\n\n{\n    &#39;user_id&#39;: {\n        &#39;S&#39;: str(obj.user_id)\n    },\n    &#39;map_id&#39;: {\n        &#39;S&#39;: str(obj.map_id)\n    },\n    &#39;name&#39;: {\n        &#39;S&#39;: str(obj.name)\n    },\n    &#39;description&#39;: {\n        &#39;S&#39;: str(obj.description)\n    },\n    &#39;num_points&#39;: {\n        &#39;N&#39;: str(obj.num_points)\n    }\n}\n\n\nPoints table\nThis is most complex table. The keys are similar to the maps, the range key is used to store the unique identifier of the map:\n\n{\n    &quot;TableName&quot;: POINTS_TABLE,\n    &quot;KeySchema&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;map_id&quot;,\n            &quot;KeyType&quot;: &quot;HASH&quot;\n        },\n        {\n            &quot;AttributeName&quot;: &quot;point_id&quot;,\n            &quot;KeyType&quot;: &quot;RANGE&quot;\n        },\n    ],\n    &quot;AttributeDefinitions&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;map_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        },\n        {\n            &quot;AttributeName&quot;: &quot;point_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        },\n    ]\n}\n\nAnd the additional parameters:\n{\n    &#39;point_id&#39;: {\n        &#39;S&#39;: obj.point_id\n    },\n    &#39;map_id&#39;: {\n        &#39;S&#39;: str(obj.map_id)\n    },\n    &#39;lat&#39;: {\n        &#39;S&#39;: str(obj.lat)\n    },\n    &#39;lon&#39;: {\n        &#39;S&#39;: str(obj.lon)\n    },\n    &#39;date&#39;: {\n        &#39;N&#39;: str(obj.epoch)\n    },\n    &#39;name&#39;: {\n        &#39;S&#39;: str(obj.name)\n    },\n}\n\nThe challenge with this model is to be able to delete a map with a large number of points. It is counter-intuitive, because one might think that removing only the points with the primary key of the map will make the work but ...\nTHIS WILL NOT WORK!\nDue to the way DynamoDB is implemented, this is not possible (https://stackoverflow.com/questions/34259358/dynamodb-delete-all-items-having-same-hash-key). In that kind of tables, you need to provide the primary key and the range key in order to delete an item.\nSince the number of items can be large, it could take a lot of capacity to delete a the points. I do not want to consume that capacity, so I will let DynamoDB throttle the deletes to adapt to the capacity.\nThe project is serverless (Lambda) based and trying to delete a large number of points will result in timeouts when DynamoDB throttle the deletes. There are two possible solutions here: increase the write capacity of the table (increase cost) or increase the Lambda timeout (increase cost).\nAfter thinking a little bit, the valid solution I choose is to launch an ECS Task with the logic to delete the large number of maps:\n\nclient.run_task(\n    cluster=&quot;arn:aws:ecs:eu-west-1:***:cluster/remove-points-cluster&quot;,\n    taskDefinition=&quot;arn:aws:ecs:eu-west-1:***:task-definition/remove-points&quot;,\n    overrides={\n        &quot;containerOverrides&quot;: [\n            {\n                &quot;name&quot;: &quot;remove-points&quot;,\n                &quot;environment&quot;: [\n                    {\n                        &quot;name&quot;: &quot;MAP_ID&quot;,\n                        &quot;value&quot;: map_id\n                    }\n                ]\n            }\n        ]\n    },\n    launchType=&quot;FARGATE&quot;,\n    networkConfiguration={\n        &quot;awsvpcConfiguration&quot;: {\n            &quot;subnets&quot;: [&quot;1&quot;, &quot;2&quot;],\n            &quot;assignPublicIp&quot;: &quot;ENABLED&quot;\n        }\n    }\n)\n\nThe best part of this ECS Task is that only took 5 minutes to use the same code base and Dockerize the logic of removing the points!\nNow the long-running task of delete a large number of points is done in ECS, where the pricing model is pay per use. Since this is a feature that is not going to happen a lot, it&#39;s perfectly fine.\n"
      } ,
   
      {
        "title"    : "Mocking external API with wiremock",
        "category" : "",
        "tags"     : " java, testing, e2e, docker, wiremock",
        "url"      : "/mocking-external-apis-wiremock/",
        "date"     : "November 27, 2018",
        "excerpt"  : "What happens when you are developing a component that heavily rely on an external API you do not control? Or even worst, that still does not exist. How could you test your component without connecting the external dependency? When we...",
        "content"  : "What happens when you are developing a component that heavily rely on an external API you do not control? Or even worst, that still does not exist. How could you test your component without connecting the external dependency? When we don&#39;t have control over the API that we need to integrate, we need a tool like a &quot;mock server&quot;.\nThis article will discover and provide a bootstrap project for wiremock. More info: wiremock.org\n\nQuoting from their website:\nWireMock is a simulator for HTTP-based APIs. Some might consider it a service virtualization tool or a mock server.\nAt its core is a Java software that receives HTTP requests with some mapped requests to responses\nTL;DR: https://github.com/adriangalera/docker-compose-wiremock/\nConfiguring wiremock\nConfiguring wiremock only consists on defining the requests to be mocked and the response that should be answered on the presence of the mocked request.\nDocker\nOne nice way of integrate wiremock with your current testing environment is using it inside docker. There&#39;s this project https://github.com/rodolpheche/wiremock-docker that provides the wiremock service to docker.\nIn order to configure it, you must create the following folder structure:\n.\n├── Dockerfile\n└── stubs\n    ├── __files\n    │   └── response.json\n    └── mappings\n        └── request.json\n\n\nThe mappings folder contains all the mocked requests definitions and __files contains the response JSON for the mocked requests as shown before.\nExample\nLet&#39;s say we have an external API developed by another team in the company under the host externalapi.com and is not yet finished. The call that our service needs to perform is externalapi.com/v1/resource/resource1 and will respond:\n\n{\n    &quot;hello&quot;:&quot;world&quot;\n}\n\nLet&#39;s configure wiremock, so we can start working on our service in parallel with the other team.\n\n\n  Configure the request mapping\n\n\n{\n    &quot;request&quot;:{\n        &quot;method&quot;:&quot;GET&quot;,\n        &quot;urlPathPattern&quot;:&quot;/v1/resource/([a-zA-Z0-9-\\\\_]*)&quot;\n    },\n    &quot;response&quot;:{\n        &quot;status&quot;:200,\n        &quot;bodyFileName&quot;:&quot;response.json&quot;,\n        &quot;headers&quot;:{\n            &quot;Content-Type&quot;:&quot;application/json&quot;\n        }\n    }\n}\n\n\n  Configure the response\n\n\n{\n    &quot;hello&quot;:&quot;world&quot;\n}\n\n\n\n  Test it\n\n\ndocker-compose up --build -d\ncurl http://localhost:7070/v1/resource/resource1\n{\n&quot;hello&quot; : &quot;world&quot;\n}\n\n\nYay! It worked!\n\nThe only missing point is configure the actual component to point to the mocked server. For example with ribbon:\n\nexternalservice.ribbon.listOfServers=http://localhost:7070\n\n"
      } ,
   
      {
        "title"    : "Timeseries database with DynamoDB",
        "category" : "",
        "tags"     : " aws, cloud, dynamodb, kinesis, python",
        "url"      : "/timeseries-db-dynamodb/",
        "date"     : "September 21, 2018",
        "excerpt"  : "Working with sensor data, we need to store timestamped data in a way that was easily checked and show it in graphs. This post explains the process we follow to end up implementing a timeseries database on DynamoDB.\n\nRe...",
        "content"  : "Working with sensor data, we need to store timestamped data in a way that was easily checked and show it in graphs. This post explains the process we follow to end up implementing a timeseries database on DynamoDB.\n\nRequirements\nThe problem was pretty straightforward at the beginning: store temporal data that came from multiple sensors where one sensor could have more than one metric. For instance: one battery sensor might monitor the battery level as well as the battery voltage. The sensor data data flows continuously at an undetermined interval, it can be 1 minute, 5 minutes, ... Generally speaking the data can be represented as:\nMETRIC NAME - TIMESTAMP - VALUE\nIn order to support the defined usage scenarios we need to provide a tool able to:\n\n\n  Perform temporal queries: give me the metric between 2017/05/21 00:00:00 and 2017/08/21 00:30\n  Support multiple granularity: second, minute, hour, …\n  Support TTL for cold data: expire cold data\n  Perform in a cost-efficient manner\n\n\nAvailable technologies\nWhen one thinks about database at first the relational ones appear as the first option. The problem with relational databases such as MySQL, PostgreSQL, .. is that we the database becomes very unusable when the size of the tables grows. And in this case the data  will grow a lot with the usage.\nFurthermore, when the data is so big the indexes start to generate headaches making the queries take a lot of time.\nFinding these drawbacks in the traditional relational databases, we shift towards NoSQL databases.\nThe first one that came into our mind (because we had some previous experience with it) was whisper https://github.com/graphite-project/whisper. This database is a small component of the graphite project, basically is a wrapper to write the temporal data to a file performing multiple roll-up aggregations on the fly. This looked promising, however, when we heavy loaded it performed very poorly.\nSince the platform we were building it was AWS based, we decided to analyse what Amazon can provide us and finally found DynamoDB!\nDynamoDB at the rescue!\nDynamoDB is a key-value database that supports the configuration of item TTL and its costs are predictable because are in function of the required capacity.\nOne might ask: how can I store the presented model in a key-value database? The magic comes with the composite key feature: https://aws.amazon.com/es/blogs/database/choosing-the-right-dynamodb-partition-key/\n\nKnowing DynamoDB\nQuoting AWS documentation:\nThis type of key is composed of two attributes. The first attribute is the partition key, and the second attribute is the sort key.\nTherefore, we can use the metric name as the partition key and the timestamp and the sort key and the sensor value as the DynamoDB object:\n\n  \nDynamoDB Key Schema\n\nRollup aggregations?\nNow we can store the timestamped data in DynamoDB, however, what happens with the multiple granularity requirement? DynamoDB has a nice feature called DynamoDB Streams.\nDynamoDB Streams sends the events generated in the database (new records, edit, delete, ...) to a an AWS Lambda. Hence, we can perform the aggregations for the multiple granularities as soon as a new data value arrives. In order to perform the storage of the multiple aggregations, we can define one table for each aggregation.\nThe implementation: DynamoDB Timeseries Database\nFinally, in order to complete the setup we have used a serverless approach in order to allocate the cost of the project to the required capacity.\nThe final structure of the implemented solution looked like this:\n\n\nComponents of DynamoDB TimeseriesDB\n\n1) The service that uses the DynamoDB Timeseries database is a serverless application with an API Gateway calling an Lambda function. One of the steps of the business logic is communicate with the DynamoDB Insert Lambda.\n2) The insertion mechanism of the database is by invoking an AWS Lambda function. This function inserts the timestamped data into the lower granularity table.\n3) When the Insert function inserts the data into the lower granularity table, DynamoDB Streams invokes the AWS Lambda involved in performing the roll-up aggregations.\n4) The Aggregate function has the logic implemented on how to perform multiple aggregations (average, sum, count, ...).\n5) Each time serie can be configured to have different aggregations, TTL, timezone to perform temporal aggregation, etc... There are an additional lambda in order to configure the timeserie parameters.\n6) Once the aggregation is performed, the data is stored into the appropriate DynamoDB table according to the granularity: minute, hour, day, month, year, ...\nWith this solution we achieve a solution where we can fine tune the capacity of the database.\nProduction time!\nWhen we put this system into production, some issues arises with the capacity of the DynamoDB. When the incoming data flows faster than the reserved capacity in DynamoDB, the lambda functions became very slow and resulting in a high number of timeout errors.\nThe reasons for this is that when DynamoDB is running at capacity, it throttle requests, making the client adjust its speed to the reserved capacity. This works good for a traditional client, however it breaks with the serverless approach, because the Lambda functions were taking too much time.\nIn order to fix situation, we add another component to the system: an AWS Kinesis Stream https://aws.amazon.com/kinesis. Instead of writing directly to the DynamoDB table, the service Lambda function now writes the data into a Kinesis stream.\nIn the other side of the stream, we place a Kinesis consumer that is able to consume items from the stream in batches of items. Additionally, we are able to control the insertion speed of items in DynamoDB by sleeping some time between batch consumption. Since this consumer needs to be 24/7, it runs on a traditional EC2 instance.\nNow the scheme looks like this:\n\n\nAdding Kinesis into the TimeseriesDB\n\nThe additional point (7) shows the Kinesis stream where the items are being inserted by the Service Lambda function and consumed by the DynamoDB Timeseries DB Kinesis Consumer. The configured batch size (B) and sleep time (T) allows the consumer to buffer the insertion of data up to the reserved DynamoDB capacity.\nShow me the code\nAn open sourced version of the production code can be found here:\nhttps://github.com/adriangalera/dynamodb-timeseries\n"
      } ,
   
      {
        "title"    : "Raspberry Pi for QA",
        "category" : "",
        "tags"     : " linux, nodejs, raspberry-pi, qa, testing",
        "url"      : "/raspberry-pi-qa/",
        "date"     : "September 21, 2018",
        "excerpt"  : "On a previous company the QA team needed to perform some scenarios that were difficult to reproduce, for instance force a buffering event. Their setup require deep technical understanding of Linux. In order to ease its world, I decided to creat...",
        "content"  : "On a previous company the QA team needed to perform some scenarios that were difficult to reproduce, for instance force a buffering event. Their setup require deep technical understanding of Linux. In order to ease its world, I decided to create a neat service running on a Raspberry Pi.\n\nThe problems:\nConcretely, the scenarios that the QA needed to cover were:\nRestrict the network performance\nThe test that the QA team were performing, included provoking buffering issues in media players. They found issues on how this could be achieved: in Chrome it could be achieved through Developer Tools, but what about mobile devices, or even worst: some weird embedded media players?\nAccess Geo-blocked content\nSince the company had customers world-wide and some of them used Geo-blocked content, they need to access those contents through the use of VPNs. This required spending some time configuration the VPN clients on their side\nIP Blocking\nThere were some scenarios (I cannot remember right now) that required to block the connection to some IP. This is really easy to do on a UNIX machine with iptables, but good luck doing that on Windows.\nModify DNS records\nIt were some scenario where they needed to change the DNS records. For instance: www.google.com -&amp;gt; 192.168.1.100. I don&#39;t remember the rationale to this requirement :(\nAnalyse HTTPS traffic\nIn the majority of the environments the connections with the company server&#39;s were made with HTTPS. This caused a little bit of headache while analysing the HTTP traffic.\nThe solutions ...\nSince most of the scenarios require some networking tweaks, the obvious decision was Linux, even better: Raspberry Pi. The project consist in a NodeJS Express application that executed scripts on the RaspberryPi.\nThe raspberry PI have two network interfaces: the LAN and the WiFi. The configuration is setup in the interfaces file:\n\nauto lo\niface lo inet loopback\nauto eth0\nauto wlan0\n#static ethernet conf\niface eth0 inet static\naddress 192.168.1.100\nnetmask 255.255.255.0\ngateway\t192.168.1.1\ndns-nameservers 8.8.4.4\niface wlan0 inet static\naddress 192.168.150.1\nnetmask 255.255.255.0\n\n\nIn the WiFi interface hostapd service is configured in order to serve a WiFi connection. This will configure the traffic outgoing traffic from wlan0 to eth0.\nPlaying with tc and ifb\nThe network performance restrictions can be applied using the tc command and the ifb module on Linux. This module redirects the traffic from one real network interface to a virtual one. When the traffic passes through the ifb0 interface the token bucket (htb) applies the network configuration\n\n#Enable ifb module and setup the ifb0 interface\nmodprobe ifb numifbs=1 &amp;amp;amp;&amp;amp;amp; ip link set dev ifb0 up\n#Create a device that redirects all the traffic\n#from eth0 to ifb0\ntc qdisc add dev eth0 handle ffff: ingress &amp;amp;amp;&amp;amp;amp; \\\ntc filter add dev eth0 parent ffff: protocol ip u32 \\\nmatch u32 0 0 action mirred egress redirect dev ifb0\n#Modify the token bucket configuration\ntc qdisc add dev ifb0 root handle 1: htb default 10 &amp;amp;amp;&amp;amp;amp; \\\ntc class add dev ifb0 parent 1: classid 1:1 htb rate 1mb &amp;amp;amp;&amp;amp;amp; \\\ntc class add dev ifb0 parent 1:1 classid 1:10 htb rate 1mb\n\n\nOpenVPN and Iptables\nIn order to be able to avoid the Geo-blocking of contents, the company provide us a commercial VPN. This consisted on a series of OpenVPN configuration scripts for multiple countries. Therefore, the app only needs to call openvpn:\n\nopenvpn ALBANIA-TCP.ovpn\n\n\nIt&#39;s really easy to block an IP on a Linux box, the app only needs to call the iptables, for example:\n\niptables -I FORWARD -s 192.168.150.0/24 -d 8.8.8.8  -j DROP\n\n\nThis snippet blocks the outgoing connections to 8.8.8.8 that goes out from the WiFi network\nNetworking stuff ...\nRegarding DNS Spoofing, it&#39;s a little bit trickier, however the dnsmasq service is very useful in that situation.\n\n\ndnsmasq schema\n\nThe clients connected to the WiFi will resolve the DNS queries thanks to the dnsmasq client listening to incoming connections. This service is able to perform custom DNS resolutions based on a file that works like a /etc/hosts file.\n\n192.168.56.1   ubuntu.tecmint.lan\n192.168.56.10  centos.tecmint.lan\n\n\nRegarding the HTTPS Sniffer, the implemented solution implies having the SSL certificates from the company installed in a reverse proxy that terminates the SSL connection and forwards the HTTP traffic to an internal server that stores the decrypted requests on a cache.\n\n\ndnsmasq schema\n\nThis cache can be queried from the GUI to inspect the requests. There&#39;s an additional implementation that allows to run pcap capture software directly on the network interface to inspect the packets from the UI.\nThis environment requires a little setup, that is the configuration of the proxy on the computers of the QA team.\nFinally, the QA team were able to save a lot of time setting up their scenarios. With this app it&#39;s only a matter of clicking buttons instead of executing weird scripts\nThe code ...\nDon&#39;t blame too much on the code quality: the whole project was implemented few years ago and as a side project on one or two weekends\nhttps://github.com/adriangalera/rpitester\nHere&#39;s a video of me presenting that project ;)\n\n"
      } 
   
   
   
]