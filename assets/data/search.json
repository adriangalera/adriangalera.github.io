[
   
      {
        "title"    : "Discovering terraform",
        "category" : "",
        "tags"     : " devops, aws, terraform, cloud",
        "url"      : "/discovering-terraform/",
        "date"     : "October 14, 2019",
        "excerpt"  : "Recently I have been playing with Terraform tool and I wrote some basic use cases for the sake of sharing the knowledge\n\n\nWhat is terraform?\n\nImagine you start a new cloud project in AWS....",
        "content"  : "Recently I have been playing with Terraform tool and I wrote some basic use cases for the sake of sharing the knowledge\n\n\nWhat is terraform?\n\nImagine you start a new cloud project in AWS. At the very begining, you will write some scripts to generate your AWS resources:\n\n\n  DynamoDB tables\n  S3 buckets\n  RDS databases\n  …\n\n\nHowever, lots of configuration are required to make it work properly, i.e. IAM roles, ARN of resources, etc… So, managing this burden manually or with some scripts rapidly could become an issue.\n\nEven worst, imagine you have resources in both Google Cloud an AWS. The amount of scripts will be doubled as well as the issues.\n\nTerraform was created to help with those issues. It is a tool written in Go language that enables managing the state of cloud infrastructure from configuration files with its own Domain Specific Language (DSL).\n\nFurthermore, it is a handy tool to define the infrastructure as code. In this approach the changes to the infrastructure can be pushed to a git repository for instance; this way the changes can be reviewed, etc.\n\nAnother fancy feature is that it keeps the state of the resources running in Cloud. This way terraform allows to make incremental changes (if possible).\n\nBasic usage\n\nThere are three main commands to use:\n\n\n  terraform plan: process the configuration templates and present in stdout the actions that will be applied\n  terraform apply: process the configuration templates and apply the required actions\n  terraform destroy: process the configuration templates and delete the resources\n\n\nScenarios\n\nIn this section, I prepared 4 scenarions to show some cool features that terraform can provide.\n\nBasic\n\nThis scenario create a DynamoDB table and an S3 bucket. There’s nothing fancy here, only basic usage of terraform:\n\nvariable &quot;app-prefix&quot; {\n  type = string\n  default = &quot;comms-ks-01&quot;\n}\nresource &quot;aws_dynamodb_table&quot; &quot;configuration&quot; {\n  name           = &quot;${var.app-prefix}_timeserie_configuration&quot;\n  billing_mode   = &quot;PAY_PER_REQUEST&quot;\n  hash_key       = &quot;timeserie1&quot;\n\n  attribute {\n    name = &quot;timeserie1&quot;\n    type = &quot;S&quot;\n  }\n\n}\nresource &quot;aws_s3_bucket&quot; &quot;s3-bucket-rnd-name&quot; {\n    bucket = &quot;${var.app-prefix}-timeserie-configuration&quot;\n} \noutput &quot;bucket-arn&quot; {\n  value = &quot;${aws_s3_bucket.s3-bucket-rnd-name.arn}&quot;\n}\n\nThe template defines a variable (“app-prefix), a DynamoDB table (“configuration”) and a S3 bucket (“s3-bucket-rnd-name”). The last line defines to output to print the ARN of the created bucket\n\nFor each\n\nIn this scenario, the requirement is to create three tables with similar names. In order to reuse the code, we can use terraform’s for-each statement:\n\nresource &quot;aws_dynamodb_table&quot; &quot;configuration&quot; {\n\n  for_each = {\n    test1: &quot;${var.app-prefix}_configuration_test_1&quot;,\n    test2: &quot;${var.app-prefix}_configuration_test_2&quot;,\n    test3: &quot;${var.app-prefix}_configuration_test_3&quot;,\n  }\n\n  name           = each.value\n  billing_mode   = &quot;PAY_PER_REQUEST&quot;\n  hash_key       = &quot;timeserie&quot;\n\n  attribute {\n    name = &quot;timeserie&quot;\n    type = &quot;S&quot;\n  }\n}\n\nThis way with only one resource definition, we can create multiple resources.\n\nLinking resources\n\nUp to here, we haven’t done anything fancy, just creating resources. However, we can create more interesting environments, such as a serverless pipeline to process a file:\n\n\n\n# Lambda definition\nresource &quot;aws_lambda_function&quot; &quot;download-s3-lambda&quot; {\n  filename      = &quot;download-s3-file-lambda.zip&quot;\n  function_name = &quot;${var.app-prefix}-download-files-lambda&quot;\n  role          = &quot;${aws_iam_role.iam_for_lambda.arn}&quot;\n  handler       = &quot;receive-file-s3.handler&quot;\n  runtime       = &quot;python3.7&quot;\n  depends_on    = [&quot;aws_iam_role_policy_attachment.lambda_logs&quot;, &quot;aws_cloudwatch_log_group.example&quot;]\n}\n\n# Notify lambda when a file is created in the S3 bucket\nresource &quot;aws_s3_bucket_notification&quot; &quot;bucket_notification&quot; {\n  bucket = &quot;${aws_s3_bucket.s3-files-bucket.id}&quot;\n\n  lambda_function {\n    lambda_function_arn = &quot;${aws_lambda_function.download-s3-lambda.arn}&quot;\n    events              = [&quot;s3:ObjectCreated:*&quot;]\n  }\n}\n# S3 bucket to place files\nresource &quot;aws_s3_bucket&quot; &quot;s3-files-bucket&quot; {\n    bucket = &quot;${var.app-prefix}-files-bucket&quot;\n    force_destroy = &quot;true&quot;\n}\n\nThe template in this environment is more complex, because there’s a lot of IAM permissions in place. For simplicity, not all the resources are included in this article. For more info: https://github.com/adriangalera/terraform-knowledge-sharing.\n\nIn the upper code snippet can be observed how the AWS lambda and the AWS S3 bucket are created. Additionally, the notification from S3 to Lambda is created. When an object is created  in the S3 bucket, the AWS lambda will be invoked.\n\nTemplates\nTerraform also supports templates to enable code reuse. In this environment, we will create two docker container to be ran on ECS. Those two docker containers will execute the same image and echo the value of an environment variable.\n\nIf we did not have templates, we would need to define the resources twice. Thanks to the templates, we don’t need to define the container definition twice, we only need to pass the variables.\n\ndata &quot;template_file&quot; &quot;container_backend&quot; {\n  template = &quot;${file(&quot;container_definition.tpl&quot;)}&quot;\n  vars = {\n    container_name = &quot;${var.app-prefix}_backend_container&quot;\n    log_group = &quot;${var.app-prefix}_backend_container&quot;\n    service_type = &quot;backend&quot;\n  }\n}\n  family = &quot;${var.app-prefix}_backend_task_definition&quot;\n  requires_compatibilities = [ &quot;FARGATE&quot; ]\n  network_mode =  &quot;awsvpc&quot;\n  execution_role_arn = &quot;${aws_iam_role.ecs_container_iam_role.arn}&quot;\n  cpu = 256\n  memory = 512\n  container_definitions = &quot;${data.template_file.container_backend.rendered}&quot;\n}\n\nIn order to get the rendered values of the template, we need to get the rendered field.\n\nIn this case, the template contains lots of configuration and IAM definitions. For more info, check the whole repository: https://github.com/adriangalera/terraform-knowledge-sharing\n"
      } ,
   
      {
        "title"    : "Mockito ArgumentCaptor with inheritance",
        "category" : "",
        "tags"     : " testing, java, mockito",
        "url"      : "/mockito-argumentcaptor-inheritance/",
        "date"     : "August 8, 2019",
        "excerpt"  : "Working with Mockito’s ArgumentCaptor I discover it has a awful issue with inheritance.\n\nLet’s suppose we have a parent class named Animal and two child classes &lt;code class=&quot;highli...",
        "content"  : "Working with Mockito’s ArgumentCaptor I discover it has a awful issue with inheritance.\n\nLet’s suppose we have a parent class named Animal and two child classes Dog and Cat:\n\npublic class Animal {\n\n    private String species;\n\n    public Animal(String species) {\n        this.species = species;\n    }\n\n    public String getSpecies() {\n        return species;\n    }\n}\npublic class Cat extends Animal {\n\n    private final String colorEyes;\n\n    public Cat(String colorEyes) {\n        super(&quot;Cat&quot;);\n        this.colorEyes = colorEyes;\n    }\n}\npublic class Dog extends Animal {\n\n    private String name;\n\n    public Dog(String name) {\n        super(&quot;Dog&quot;);\n        this.name = name;\n    }\n\n    public String getName() {\n        return name;\n    }\n}\n\n\nAnd a class that accept an argument of type Animal:\npublic class AnimalProcessor {\n\n    public void processAnimal(Animal animal) {\n        System.out.println(animal.getSpecies());\n    }\n}\n\n\nOne might think on writing the unit test of AnimalProcessor similar to this snippet:\n\npublic class ArgumentCaptorInheritanceTest {\n\n    @Mock\n    private AnimalProcessor animalProcessor;\n\n    @Before\n    public void init() {\n        MockitoAnnotations.initMocks(this);\n    }\n\n    @Test\n    public void shouldProcessDog() {\n        Dog dog = new Dog(&quot;Rex&quot;);\n        Cat cat = new Cat(&quot;blue&quot;);\n\n        ArgumentCaptor&amp;lt;Dog&amp;gt; dogArgumentCaptor = ArgumentCaptor.forClass(Dog.class);\n\n        animalProcessor.processAnimal(dog);\n        animalProcessor.processAnimal(cat);\n\n        Mockito.verify(animalProcessor).processAnimal(dogArgumentCaptor.capture());\n\n        Assert.assertEquals(&quot;Rex&quot;, dogArgumentCaptor.getValue().getName());\n    }\n\n\nWell, this fails … ArgumentCaptor does not work well in this case. It makes the verify fail because the method has been called twice.\n\nThe expected behaviour is that only verify analyses the calls when the Dog instance is passed.\n\nIn order to execute the test in this way, some ugly workaround needs to be done:\n\n    @Test\n    public void shouldProcessDog() {\n        Dog dog = new Dog(&quot;Rex&quot;);\n        Cat cat = new Cat(&quot;blue&quot;);\n\n        ArgumentCaptor&amp;lt;Animal&amp;gt; animalCaptor = ArgumentCaptor.forClass(Animal.class);\n\n        animalProcessor.processAnimal(dog);\n        animalProcessor.processAnimal(cat);\n\n        Mockito.verify(animalProcessor, Mockito.times(2)).processAnimal(animalCaptor.capture());\n\n        List&amp;lt;Animal&amp;gt; processedAnimals = animalCaptor.getAllValues();\n        Optional&amp;lt;Animal&amp;gt; dogOptional = processedAnimals.stream()\n                                        .filter(a -&amp;gt; a instanceof Dog)\n                                        .findFirst();\n        Assert.assertTrue(dogOptional.isPresent());\n        Assert.assertEquals(&quot;Rex&quot;, ((Dog) dogOptional.get()).getName());\n    }\n\n\nInstead of capturing the arguments for Dog, you can do it for Animal.\n\nThen the verify will successfully capture the two calls and then all the captured values can be analysed. You can filter the captured values for the objects that are of the interested instance.\n\nThis example comes from:\n\nhttps://github.com/adriangalera/java-sandbox/tree/master/src/test/java/mockito/argcaptor\n\nThis is a known issue (already reported as an issue in their repo):\n\nhttps://github.com/mockito/mockito/issues/565\n\nAs of the day of writing the article, the issues is still there and it has been opened from 2016 …\n"
      } ,
   
      {
        "title"    : "Publishing versions in Gradle BOM",
        "category" : "",
        "tags"     : " gradle, groovy",
        "url"      : "/gradle-bom-publish-dependency-version/",
        "date"     : "July 18, 2019",
        "excerpt"  : "We are planning to use the concept of Bill Of Materials (BOM) to define the version of our dependencies. However this externalization do not allow us to have the version variable in the projects that import the bom. Here you will find out how w...",
        "content"  : "We are planning to use the concept of Bill Of Materials (BOM) to define the version of our dependencies. However this externalization do not allow us to have the version variable in the projects that import the bom. Here you will find out how we managed to get the version in our components.\n\nBill of Materials\nEveryone knows dependencies are hell, specially dealing with versions. That’s why we want to centralize all the version definition in a Bill Of Materials file. This approach is used for instance by Spring framework.\n\nHowever in our projects we need to provide the current version of libraries for some external component configuration.\n\nIn this sense, the two requirements collide because the versions cannot be extracted from the bom file.\n\nPublishing the versions\n\nHere’s our bom gradle script:\n\nplugins {\n    id &#39;java&#39;\n    id &#39;maven-publish&#39;\n    id &#39;io.spring.dependency-management&#39; version &#39;1.0.6.RELEASE&#39;\n}\n\ngroup &#39;com.example&#39;\n\ndef versions = [\n        library1: &#39;5.14.13&#39;,\n        library2: &#39;1.0.6.RELEASE&#39;,  \n]\n\n\ndependencyManagement {\n    dependencies {\n        dependency group: &#39;com.example&#39;, name: &#39;library1&#39;, version: versions.library1\n        dependency group: &#39;com.example&#39;, name: &#39;library2&#39;, version: versions.library2    \n    }\n}\n\npublishing {\n    repositories {\n        maven {\n            url &#39;https://repo.com/maven-releases&#39;\n            credentials credentials\n        }\n    }\n\n    publications {\n        mavenBom(MavenPublication) {\n            artifacts = []\n        }\n    }\n}\n\n\nWhen we execute the publish it correctly generates the bom file. However the versions are not accesible from the projects that import the bom.\n\nIn order to extract the version, we can add it as properties to the bom file:\n\n publications {\n        mavenBom(MavenPublication) {\n            artifacts = []\n            pom.withXml {\n                def propertiesNode = new Node(null, &quot;properties&quot;)\n                versions.each { entry -&amp;gt; propertiesNode.appendNode(entry.key, entry.value) }\n                asNode().append(propertiesNode)\n            }\n        }\n    }\n\n\nIt took like 3 hours to get these 3 lines working because Groovy sucks … After that we get the versions in the pom.xml:\n\n&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;\n&amp;lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&amp;gt;\n  &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;\n  ...\n  &amp;lt;properties&amp;gt;\n    &amp;lt;library1&amp;gt;5.14.13&amp;lt;/library1&amp;gt;\n    &amp;lt;library2&amp;gt;1.0.6.RELEASE&amp;lt;/library2&amp;gt;\n  &amp;lt;/properties&amp;gt;\n&amp;lt;/project&amp;gt;\n\n\n\nFrom the project that want to use the bom we can access these versions like this:\n\ndependencyManagement.importedProperties[&#39;library1&#39;]\n\n"
      } ,
   
      {
        "title"    : "Java Unit Test to check UTF-8 chars",
        "category" : "",
        "tags"     : " java, unit-test, utf8, i18n",
        "url"      : "/java-test-utf8/",
        "date"     : "June 19, 2019",
        "excerpt"  : "During a migration to a new plattform, we have detected an issue with the character encoding. Some of the messages contained the UTF-8 replacement character (�)\n\n\n\nFortunately, we have been able to fix the configuration issue ...",
        "content"  : "During a migration to a new plattform, we have detected an issue with the character encoding. Some of the messages contained the UTF-8 replacement character (�)\n\n\n\nFortunately, we have been able to fix the configuration issue and to make sure it does not happen again we have put in place a variation of the following unit test:\n\nimport org.junit.Assert;\nimport org.junit.Test;\n\npublic class CheckUtf8ReplacementChar {\n\n\n    private boolean containsUtf8ReplacementCharacter(String target) {\n        final int REPLACEMENT_CHARACTER_VALUE = 65533;\n        for (int i = 0; i &amp;lt; target.length(); i++) {\n            if ((int) target.charAt(i) == REPLACEMENT_CHARACTER_VALUE) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n    @Test\n    public void shouldDetectUtf8ReplacementChar() {\n        final String wrongString = &quot;Wrong characters ������������������&amp;lt;br&amp;gt;&quot;;\n        final String okString = &quot;OK characters&quot;;\n        Assert.assertTrue(containsUtf8ReplacementCharacter(wrongString));\n        Assert.assertFalse(containsUtf8ReplacementCharacter(okString));\n    }\n}\n\n\nEven though this can be improved, we didn’t have much time to think about it. That’s the first way we could develop.\n"
      } ,
   
      {
        "title"    : "Grafana, nginx reverse-proxy and Docker",
        "category" : "",
        "tags"     : " grafana, docker, devops, nginx",
        "url"      : "/grafana-nginx-reverse-proxy-docker/",
        "date"     : "June 15, 2019",
        "excerpt"  : "We want to start monitoring our AWS resources using Cloudwatch. The API is awesome, however its visualisation tool sucks, that&#39;s why we choose Grafana to present the data.\n\nCurrently, we&#39;re using ECS to deploy our servi...",
        "content"  : "We want to start monitoring our AWS resources using Cloudwatch. The API is awesome, however its visualisation tool sucks, that&#39;s why we choose Grafana to present the data.\n\nCurrently, we&#39;re using ECS to deploy our services and Grafana is available as a Docker image therefore this tool is a natural fit! We checked the documentation and everything looks fine: https://grafana.com/docs/installation/docker\nThe problem comes when we tried to deploy Grafana docker image using our governance tool. This tool expects a /health-check endpoint to detect the status of our applications. Here we have two approaches:\n\nTry to modify the Grafana code to add this logic inside\nAdd something to the Docker container to answer this endpoint.\n\nFor obvious reasons we chose the second version. Initially we were thinking on some kind of script. However we realised that the requests to Grafana need to be proxied. This is even mentioned in their documentation! https://grafana.com/docs/installation/behind_proxy/ .\nThe solution\nNow that we have discarded the scripting, we chose nginx to implement the reverse proxy and we delegate the health-check endpoint to a static content under webroot of nginx.\nHereunder is the Dockerfile, which is self-explanatory:\n\nFROM grafana/grafana\nEXPOSE 8080 8080\nCOPY health-check /health-check\nCOPY start-nginx-grafana.sh /start-nginx-grafana.sh\nUSER root\nRUN apt-get update &amp;amp;amp;&amp;amp;amp; apt-get install -y nginx\nRUN chown -R grafana:grafana /etc/nginx/nginx.conf /var/log/nginx /var/lib/nginx /start-nginx-grafana.sh\nRUN chmod +x /start-nginx-grafana.sh\nUSER grafana\nRUN cp /health-check/nginx.conf /etc/nginx/nginx.conf\nENTRYPOINT [ &quot;/start-nginx-grafana.sh&quot; ]\n\n\nThe tricky part we found was that installing nginx required sudo permissions, however this could be easily achieved changing to the user root in the Dockerfile. Grafana service runs as grafana user, so, some permissions of files and folders of the nginx services need to be changed to the grafana user.\nThe following snippet shows the nginx.conf file:\n\nworker_processes  1;\npid /var/lib/nginx/nginx.pid;\n\nevents {\n    worker_connections  1024;\n}\n\nhttp {\n    include       mime.types;\n    default_type  application/octet-stream;\n    sendfile        on;\n    keepalive_timeout  65;\n    server {\n        listen       8080;\n        server_name  localhost;\n\n        location /health-check {\n            default_type  &quot;application/json&quot;;\n            root   /health-check;\n            index  health-check.json;\n        }\n\n        location / {\n            proxy_pass http://localhost:3000/;\n        }\n\n        #location /api {\n        #    return 403;\n        #}\n    }\n}\n\n\nThis configuration enables the health-check endpoint to be compatible with our governance tool. Precisely nginx returns the file health-check.json in response to this endpoint. nginx proxies any other request to the Grafana instance running inside the container. The presence of the nginx reverse proxy enables the user to implement more features, like blocking the Grafana API, etc...\nYou can check the code to provide Grafana,nginx and Docker here: https://github.com/adriangalera/nginx-grafana-docker\n"
      } ,
   
      {
        "title"    : "Parse huge local json file",
        "category" : "",
        "tags"     : " frontend, react, json, oboejs",
        "url"      : "/parse-huge-local-json-file/",
        "date"     : "March 13, 2019",
        "excerpt"  : "I need to provide a UI to analyse the results of a long-running process that generates huge JSON file whose size is in GB order.\nIn the article I will talk about the solution implemented using the HTML5 FileReader API and the mighty &lt;a h...",
        "content"  : "I need to provide a UI to analyse the results of a long-running process that generates huge JSON file whose size is in GB order.\nIn the article I will talk about the solution implemented using the HTML5 FileReader API and the mighty oboe.js library\n\nTL;DR: The code\nThanks to HTML5 FileReader API , files can be read locally in the browser without any need for servers. Even better, files can be read in chunks in order to keep the memory footprint as low as desired.\nIf you search in Google about how to parse huge JSON files, eventually the streaming techniques will appear. In the XML world there are two different techniques for parsing files:\n\nSAX: Read the XML as events, keeping a little memory footprint\nDOM: Read the whole XML in memory allowing easy manipulation\n\nWorking with JSON the DOM technique is the most used. For instance &quot;JSON.parse&quot; loads the whole string in memory before parsing the JSON. What will happen if the string is really big? The browser will explode.\nWe need to apply the SAX loading technique to read the big JSON file. In order to achieve that we can use Oboejs library:\nOboe.js is an open source Javascript library for loading JSON using streaming, combining the convenience of DOM with the speed and fluidity of SAX.\nUsing oboe.js\nReading the documentation it is not clear if one can use the FileReader API with oboe-js. It clearly says you can pass an URL or a NodeJs stream to its initializer method:\n\noboe( String url )\n\noboe({\n    url: String,\n    method: String,\n    headers: Object,\n    body: String|Object,\n    cached: Boolean,\n    withCredentials: Boolean\n})\n\noboe(stream)\n\n\nSearching over the internet I have found this Github issue where it&#39;s author is asking for some solution to not using an URL nor NodeJs stream.\nSo, finally there&#39;s a way to combine the power of the FileReader API and the streaming capabilities of oboejs\nThe code\nSince the UI we are building is built in React, I have made this project as a plug-and-play React component:\nhttps://github.com/adriangalera/parse-huge-json\nP.S: The plug-and-play worked like a charm!\n&amp;nbsp;\n"
      } ,
   
      {
        "title"    : "Relational model in DynamoDB",
        "category" : "",
        "tags"     : " aws, cloud, dynamodb, relational, persistence, ecs, python",
        "url"      : "/relational-model-dynamodb/",
        "date"     : "January 28, 2019",
        "excerpt"  : "For a new project that I am starting, I have the requirement to implement a highly relational model in AWS. The most important condition is to keep costs as low as possible, since this is a personal project and I do not want to get poor.\n&lt;p...",
        "content"  : "For a new project that I am starting, I have the requirement to implement a highly relational model in AWS. The most important condition is to keep costs as low as possible, since this is a personal project and I do not want to get poor.\nTherefore I will persist the model in DynamoDB configured to use the minimum resources as possible.\n\nThe application consist on three entities: User,Map and Points.\nUsers can create multiple maps that contain several points. The following UML schema explain the relationships:\n\n  \nRelational model UML\n\nDynamoDB is a key-value store with support for range key. Thanks to that I am able to implement the following queries:\n\n\n  CRUD User,Map,Point\n  Add a map for one user\n  Add a point in a map\n  Get points from a map\n  Remove map from user\n  Remove point from user\n  Get maps for a user\n\n\nThe DynamoDB model\nUsers table\nThe user table is straightforward, the only key is a unique identifier for the user.\n\n{\n    &quot;TableName&quot;: USER_TABLE,\n    &quot;KeySchema&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;user_id&quot;,\n            &quot;KeyType&quot;: &quot;HASH&quot;\n        }\n    ],\n    &quot;AttributeDefinitions&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;user_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        }\n    ]\n}\n\n\nThere are additional attributes that keep track of the number of points and maps stored for that user:\n\nrecord = {\n    &#39;user_id&#39;: {\n        &#39;S&#39;: str(obj.user_id)\n    },\n    &#39;num_points&#39;: {\n        &#39;N&#39;: str(obj.num_points)\n    },\n    &#39;num_maps&#39;: {\n        &#39;N&#39;: str(obj.num_maps)\n    }\n}\n\n\nMaps table\nThe map table is a little bit more complex, because it has to keep relations between users and maps. Therefore, I use the range key to save the unique identifier of the map:\n\n{\n    &quot;TableName&quot;: MAPS_TABLE,\n    &quot;KeySchema&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;user_id&quot;,\n            &quot;KeyType&quot;: &quot;HASH&quot;\n        },\n{\n            &quot;AttributeName&quot;: &quot;map_id&quot;,\n            &quot;KeyType&quot;: &quot;RANGE&quot;\n        },\n    ],\n    &quot;AttributeDefinitions&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;map_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        },\n{\n            &quot;AttributeName&quot;: &quot;user_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        }\n    ]\n}\n\n\nThere are additional attributes associated to the map (self-explanatory):\n\n{\n    &#39;user_id&#39;: {\n        &#39;S&#39;: str(obj.user_id)\n    },\n    &#39;map_id&#39;: {\n        &#39;S&#39;: str(obj.map_id)\n    },\n    &#39;name&#39;: {\n        &#39;S&#39;: str(obj.name)\n    },\n    &#39;description&#39;: {\n        &#39;S&#39;: str(obj.description)\n    },\n    &#39;num_points&#39;: {\n        &#39;N&#39;: str(obj.num_points)\n    }\n}\n\n\nPoints table\nThis is most complex table. The keys are similar to the maps, the range key is used to store the unique identifier of the map:\n\n{\n    &quot;TableName&quot;: POINTS_TABLE,\n    &quot;KeySchema&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;map_id&quot;,\n            &quot;KeyType&quot;: &quot;HASH&quot;\n        },\n        {\n            &quot;AttributeName&quot;: &quot;point_id&quot;,\n            &quot;KeyType&quot;: &quot;RANGE&quot;\n        },\n    ],\n    &quot;AttributeDefinitions&quot;: [\n        {\n            &quot;AttributeName&quot;: &quot;map_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        },\n        {\n            &quot;AttributeName&quot;: &quot;point_id&quot;,\n            &quot;AttributeType&quot;: &quot;S&quot;\n        },\n    ]\n}\n\nAnd the additional parameters:\n{\n    &#39;point_id&#39;: {\n        &#39;S&#39;: obj.point_id\n    },\n    &#39;map_id&#39;: {\n        &#39;S&#39;: str(obj.map_id)\n    },\n    &#39;lat&#39;: {\n        &#39;S&#39;: str(obj.lat)\n    },\n    &#39;lon&#39;: {\n        &#39;S&#39;: str(obj.lon)\n    },\n    &#39;date&#39;: {\n        &#39;N&#39;: str(obj.epoch)\n    },\n    &#39;name&#39;: {\n        &#39;S&#39;: str(obj.name)\n    },\n}\n\nThe challenge with this model is to be able to delete a map with a large number of points. It is counter-intuitive, because one might think that removing only the points with the primary key of the map will make the work but ...\nTHIS WILL NOT WORK!\nDue to the way DynamoDB is implemented, this is not possible (https://stackoverflow.com/questions/34259358/dynamodb-delete-all-items-having-same-hash-key). In that kind of tables, you need to provide the primary key and the range key in order to delete an item.\nSince the number of items can be large, it could take a lot of capacity to delete a the points. I do not want to consume that capacity, so I will let DynamoDB throttle the deletes to adapt to the capacity.\nThe project is serverless (Lambda) based and trying to delete a large number of points will result in timeouts when DynamoDB throttle the deletes. There are two possible solutions here: increase the write capacity of the table (increase cost) or increase the Lambda timeout (increase cost).\nAfter thinking a little bit, the valid solution I choose is to launch an ECS Task with the logic to delete the large number of maps:\n\nclient.run_task(\n    cluster=&quot;arn:aws:ecs:eu-west-1:***:cluster/remove-points-cluster&quot;,\n    taskDefinition=&quot;arn:aws:ecs:eu-west-1:***:task-definition/remove-points&quot;,\n    overrides={\n        &quot;containerOverrides&quot;: [\n            {\n                &quot;name&quot;: &quot;remove-points&quot;,\n                &quot;environment&quot;: [\n                    {\n                        &quot;name&quot;: &quot;MAP_ID&quot;,\n                        &quot;value&quot;: map_id\n                    }\n                ]\n            }\n        ]\n    },\n    launchType=&quot;FARGATE&quot;,\n    networkConfiguration={\n        &quot;awsvpcConfiguration&quot;: {\n            &quot;subnets&quot;: [&quot;1&quot;, &quot;2&quot;],\n            &quot;assignPublicIp&quot;: &quot;ENABLED&quot;\n        }\n    }\n)\n\nThe best part of this ECS Task is that only took 5 minutes to use the same code base and Dockerize the logic of removing the points!\nNow the long-running task of delete a large number of points is done in ECS, where the pricing model is pay per use. Since this is a feature that is not going to happen a lot, it&#39;s perfectly fine.\n"
      } ,
   
      {
        "title"    : "Mocking external API with wiremock",
        "category" : "",
        "tags"     : " java, testing, e2e, docker, wiremock",
        "url"      : "/mocking-external-apis-wiremock/",
        "date"     : "November 27, 2018",
        "excerpt"  : "What happens when you are developing a component that heavily rely on an external API you do not control? Or even worst, that still does not exist. How could you test your component without connecting the external dependency? When we...",
        "content"  : "What happens when you are developing a component that heavily rely on an external API you do not control? Or even worst, that still does not exist. How could you test your component without connecting the external dependency? When we don&#39;t have control over the API that we need to integrate, we need a tool like a &quot;mock server&quot;.\nThis article will discover and provide a bootstrap project for wiremock. More info: wiremock.org\n\nQuoting from their website:\nWireMock is a simulator for HTTP-based APIs. Some might consider it a service virtualization tool or a mock server.\nAt its core is a Java software that receives HTTP requests with some mapped requests to responses\nTL;DR: https://github.com/adriangalera/docker-compose-wiremock/\nConfiguring wiremock\nConfiguring wiremock only consists on defining the requests to be mocked and the response that should be answered on the presence of the mocked request.\nDocker\nOne nice way of integrate wiremock with your current testing environment is using it inside docker. There&#39;s this project https://github.com/rodolpheche/wiremock-docker that provides the wiremock service to docker.\nIn order to configure it, you must create the following folder structure:\n.\n├── Dockerfile\n└── stubs\n    ├── __files\n    │   └── response.json\n    └── mappings\n        └── request.json\n\n\nThe mappings folder contains all the mocked requests definitions and __files contains the response JSON for the mocked requests as shown before.\nExample\nLet&#39;s say we have an external API developed by another team in the company under the host externalapi.com and is not yet finished. The call that our service needs to perform is externalapi.com/v1/resource/resource1 and will respond:\n\n{\n    &quot;hello&quot;:&quot;world&quot;\n}\n\nLet&#39;s configure wiremock, so we can start working on our service in parallel with the other team.\n\n\n  Configure the request mapping\n\n\n{\n    &quot;request&quot;:{\n        &quot;method&quot;:&quot;GET&quot;,\n        &quot;urlPathPattern&quot;:&quot;/v1/resource/([a-zA-Z0-9-\\\\_]*)&quot;\n    },\n    &quot;response&quot;:{\n        &quot;status&quot;:200,\n        &quot;bodyFileName&quot;:&quot;response.json&quot;,\n        &quot;headers&quot;:{\n            &quot;Content-Type&quot;:&quot;application/json&quot;\n        }\n    }\n}\n\n\n  Configure the response\n\n\n{\n    &quot;hello&quot;:&quot;world&quot;\n}\n\n\n\n  Test it\n\n\ndocker-compose up --build -d\ncurl http://localhost:7070/v1/resource/resource1\n{\n&quot;hello&quot; : &quot;world&quot;\n}\n\n\nYay! It worked!\n\nThe only missing point is configure the actual component to point to the mocked server. For example with ribbon:\n\nexternalservice.ribbon.listOfServers=http://localhost:7070\n\n"
      } ,
   
      {
        "title"    : "Timeseries database with DynamoDB",
        "category" : "",
        "tags"     : " aws, cloud, dynamodb, kinesis, python",
        "url"      : "/timeseries-db-dynamodb/",
        "date"     : "September 21, 2018",
        "excerpt"  : "Working with sensor data, we need to store timestamped data in a way that was easily checked and show it in graphs. This post explains the process we follow to end up implementing a timeseries database on DynamoDB.\n\nRe...",
        "content"  : "Working with sensor data, we need to store timestamped data in a way that was easily checked and show it in graphs. This post explains the process we follow to end up implementing a timeseries database on DynamoDB.\n\nRequirements\nThe problem was pretty straightforward at the beginning: store temporal data that came from multiple sensors where one sensor could have more than one metric. For instance: one battery sensor might monitor the battery level as well as the battery voltage. The sensor data data flows continuously at an undetermined interval, it can be 1 minute, 5 minutes, ... Generally speaking the data can be represented as:\nMETRIC NAME - TIMESTAMP - VALUE\nIn order to support the defined usage scenarios we need to provide a tool able to:\n\n\n  Perform temporal queries: give me the metric between 2017/05/21 00:00:00 and 2017/08/21 00:30\n  Support multiple granularity: second, minute, hour, …\n  Support TTL for cold data: expire cold data\n  Perform in a cost-efficient manner\n\n\nAvailable technologies\nWhen one thinks about database at first the relational ones appear as the first option. The problem with relational databases such as MySQL, PostgreSQL, .. is that we the database becomes very unusable when the size of the tables grows. And in this case the data  will grow a lot with the usage.\nFurthermore, when the data is so big the indexes start to generate headaches making the queries take a lot of time.\nFinding these drawbacks in the traditional relational databases, we shift towards NoSQL databases.\nThe first one that came into our mind (because we had some previous experience with it) was whisper https://github.com/graphite-project/whisper. This database is a small component of the graphite project, basically is a wrapper to write the temporal data to a file performing multiple roll-up aggregations on the fly. This looked promising, however, when we heavy loaded it performed very poorly.\nSince the platform we were building it was AWS based, we decided to analyse what Amazon can provide us and finally found DynamoDB!\nDynamoDB at the rescue!\nDynamoDB is a key-value database that supports the configuration of item TTL and its costs are predictable because are in function of the required capacity.\nOne might ask: how can I store the presented model in a key-value database? The magic comes with the composite key feature: https://aws.amazon.com/es/blogs/database/choosing-the-right-dynamodb-partition-key/\n\nKnowing DynamoDB\nQuoting AWS documentation:\nThis type of key is composed of two attributes. The first attribute is the partition key, and the second attribute is the sort key.\nTherefore, we can use the metric name as the partition key and the timestamp and the sort key and the sensor value as the DynamoDB object:\n\n  \nDynamoDB Key Schema\n\nRollup aggregations?\nNow we can store the timestamped data in DynamoDB, however, what happens with the multiple granularity requirement? DynamoDB has a nice feature called DynamoDB Streams.\nDynamoDB Streams sends the events generated in the database (new records, edit, delete, ...) to a an AWS Lambda. Hence, we can perform the aggregations for the multiple granularities as soon as a new data value arrives. In order to perform the storage of the multiple aggregations, we can define one table for each aggregation.\nThe implementation: DynamoDB Timeseries Database\nFinally, in order to complete the setup we have used a serverless approach in order to allocate the cost of the project to the required capacity.\nThe final structure of the implemented solution looked like this:\n\n\nComponents of DynamoDB TimeseriesDB\n\n1) The service that uses the DynamoDB Timeseries database is a serverless application with an API Gateway calling an Lambda function. One of the steps of the business logic is communicate with the DynamoDB Insert Lambda.\n2) The insertion mechanism of the database is by invoking an AWS Lambda function. This function inserts the timestamped data into the lower granularity table.\n3) When the Insert function inserts the data into the lower granularity table, DynamoDB Streams invokes the AWS Lambda involved in performing the roll-up aggregations.\n4) The Aggregate function has the logic implemented on how to perform multiple aggregations (average, sum, count, ...).\n5) Each time serie can be configured to have different aggregations, TTL, timezone to perform temporal aggregation, etc... There are an additional lambda in order to configure the timeserie parameters.\n6) Once the aggregation is performed, the data is stored into the appropriate DynamoDB table according to the granularity: minute, hour, day, month, year, ...\nWith this solution we achieve a solution where we can fine tune the capacity of the database.\nProduction time!\nWhen we put this system into production, some issues arises with the capacity of the DynamoDB. When the incoming data flows faster than the reserved capacity in DynamoDB, the lambda functions became very slow and resulting in a high number of timeout errors.\nThe reasons for this is that when DynamoDB is running at capacity, it throttle requests, making the client adjust its speed to the reserved capacity. This works good for a traditional client, however it breaks with the serverless approach, because the Lambda functions were taking too much time.\nIn order to fix situation, we add another component to the system: an AWS Kinesis Stream https://aws.amazon.com/kinesis. Instead of writing directly to the DynamoDB table, the service Lambda function now writes the data into a Kinesis stream.\nIn the other side of the stream, we place a Kinesis consumer that is able to consume items from the stream in batches of items. Additionally, we are able to control the insertion speed of items in DynamoDB by sleeping some time between batch consumption. Since this consumer needs to be 24/7, it runs on a traditional EC2 instance.\nNow the scheme looks like this:\n\n\nAdding Kinesis into the TimeseriesDB\n\nThe additional point (7) shows the Kinesis stream where the items are being inserted by the Service Lambda function and consumed by the DynamoDB Timeseries DB Kinesis Consumer. The configured batch size (B) and sleep time (T) allows the consumer to buffer the insertion of data up to the reserved DynamoDB capacity.\nShow me the code\nAn open sourced version of the production code can be found here:\nhttps://github.com/adriangalera/dynamodb-timeseries\n"
      } ,
   
      {
        "title"    : "Raspberry Pi for QA",
        "category" : "",
        "tags"     : " linux, nodejs, raspberry-pi, qa, testing",
        "url"      : "/raspberry-pi-qa/",
        "date"     : "September 21, 2018",
        "excerpt"  : "On a previous company the QA team needed to perform some scenarios that were difficult to reproduce, for instance force a buffering event. Their setup require deep technical understanding of Linux. In order to ease its world, I decided to creat...",
        "content"  : "On a previous company the QA team needed to perform some scenarios that were difficult to reproduce, for instance force a buffering event. Their setup require deep technical understanding of Linux. In order to ease its world, I decided to create a neat service running on a Raspberry Pi.\n\nThe problems:\nConcretely, the scenarios that the QA needed to cover were:\nRestrict the network performance\nThe test that the QA team were performing, included provoking buffering issues in media players. They found issues on how this could be achieved: in Chrome it could be achieved through Developer Tools, but what about mobile devices, or even worst: some weird embedded media players?\nAccess Geo-blocked content\nSince the company had customers world-wide and some of them used Geo-blocked content, they need to access those contents through the use of VPNs. This required spending some time configuration the VPN clients on their side\nIP Blocking\nThere were some scenarios (I cannot remember right now) that required to block the connection to some IP. This is really easy to do on a UNIX machine with iptables, but good luck doing that on Windows.\nModify DNS records\nIt were some scenario where they needed to change the DNS records. For instance: www.google.com -&amp;gt; 192.168.1.100. I don&#39;t remember the rationale to this requirement :(\nAnalyse HTTPS traffic\nIn the majority of the environments the connections with the company server&#39;s were made with HTTPS. This caused a little bit of headache while analysing the HTTP traffic.\nThe solutions ...\nSince most of the scenarios require some networking tweaks, the obvious decision was Linux, even better: Raspberry Pi. The project consist in a NodeJS Express application that executed scripts on the RaspberryPi.\nThe raspberry PI have two network interfaces: the LAN and the WiFi. The configuration is setup in the interfaces file:\n\nauto lo\niface lo inet loopback\nauto eth0\nauto wlan0\n#static ethernet conf\niface eth0 inet static\naddress 192.168.1.100\nnetmask 255.255.255.0\ngateway\t192.168.1.1\ndns-nameservers 8.8.4.4\niface wlan0 inet static\naddress 192.168.150.1\nnetmask 255.255.255.0\n\n\nIn the WiFi interface hostapd service is configured in order to serve a WiFi connection. This will configure the traffic outgoing traffic from wlan0 to eth0.\nPlaying with tc and ifb\nThe network performance restrictions can be applied using the tc command and the ifb module on Linux. This module redirects the traffic from one real network interface to a virtual one. When the traffic passes through the ifb0 interface the token bucket (htb) applies the network configuration\n\n#Enable ifb module and setup the ifb0 interface\nmodprobe ifb numifbs=1 &amp;amp;amp;&amp;amp;amp; ip link set dev ifb0 up\n#Create a device that redirects all the traffic\n#from eth0 to ifb0\ntc qdisc add dev eth0 handle ffff: ingress &amp;amp;amp;&amp;amp;amp; \\\ntc filter add dev eth0 parent ffff: protocol ip u32 \\\nmatch u32 0 0 action mirred egress redirect dev ifb0\n#Modify the token bucket configuration\ntc qdisc add dev ifb0 root handle 1: htb default 10 &amp;amp;amp;&amp;amp;amp; \\\ntc class add dev ifb0 parent 1: classid 1:1 htb rate 1mb &amp;amp;amp;&amp;amp;amp; \\\ntc class add dev ifb0 parent 1:1 classid 1:10 htb rate 1mb\n\n\nOpenVPN and Iptables\nIn order to be able to avoid the Geo-blocking of contents, the company provide us a commercial VPN. This consisted on a series of OpenVPN configuration scripts for multiple countries. Therefore, the app only needs to call openvpn:\n\nopenvpn ALBANIA-TCP.ovpn\n\n\nIt&#39;s really easy to block an IP on a Linux box, the app only needs to call the iptables, for example:\n\niptables -I FORWARD -s 192.168.150.0/24 -d 8.8.8.8  -j DROP\n\n\nThis snippet blocks the outgoing connections to 8.8.8.8 that goes out from the WiFi network\nNetworking stuff ...\nRegarding DNS Spoofing, it&#39;s a little bit trickier, however the dnsmasq service is very useful in that situation.\n\n\ndnsmasq schema\n\nThe clients connected to the WiFi will resolve the DNS queries thanks to the dnsmasq client listening to incoming connections. This service is able to perform custom DNS resolutions based on a file that works like a /etc/hosts file.\n\n192.168.56.1   ubuntu.tecmint.lan\n192.168.56.10  centos.tecmint.lan\n\n\nRegarding the HTTPS Sniffer, the implemented solution implies having the SSL certificates from the company installed in a reverse proxy that terminates the SSL connection and forwards the HTTP traffic to an internal server that stores the decrypted requests on a cache.\n\n\ndnsmasq schema\n\nThis cache can be queried from the GUI to inspect the requests. There&#39;s an additional implementation that allows to run pcap capture software directly on the network interface to inspect the packets from the UI.\nThis environment requires a little setup, that is the configuration of the proxy on the computers of the QA team.\nFinally, the QA team were able to save a lot of time setting up their scenarios. With this app it&#39;s only a matter of clicking buttons instead of executing weird scripts\nThe code ...\nDon&#39;t blame too much on the code quality: the whole project was implemented few years ago and as a side project on one or two weekends\nhttps://github.com/adriangalera/rpitester\nHere&#39;s a video of me presenting that project ;)\n\n"
      } 
   
   
   
]